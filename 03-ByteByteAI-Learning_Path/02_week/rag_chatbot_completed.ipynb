{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "568f5f07",
   "metadata": {},
   "source": [
    "# Project 2: Customer‚ÄëSupport Chatbot for an E-Commerce Store\n",
    "\n",
    "## üéØ What is RAG (Retrieval-Augmented Generation)?\n",
    "\n",
    "**RAG** is a technique that combines:\n",
    "1. **Retrieval** - Finding relevant documents from a knowledge base\n",
    "2. **Augmented** - Adding retrieved context to the prompt\n",
    "3. **Generation** - LLM generates answer based on context\n",
    "\n",
    "```\n",
    "User Question ‚Üí Retriever ‚Üí Relevant Docs ‚Üí LLM + Context ‚Üí Answer\n",
    "```\n",
    "\n",
    "**Why RAG?**\n",
    "- LLMs have knowledge cutoff dates (can't know recent info)\n",
    "- LLMs may hallucinate (make up facts)\n",
    "- RAG grounds responses in YOUR actual documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfe2a04",
   "metadata": {},
   "source": [
    "## 1 - Environment Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b49fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPORT LIBRARIES - Each library serves a specific purpose in our RAG pipeline\n",
    "# ============================================================================\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# STANDARD PYTHON LIBRARIES\n",
    "# -----------------------------------------------------------------------------\n",
    "# os: Interact with operating system (file paths, environment variables)\n",
    "# Example: os.getenv('API_KEY') gets environment variable\n",
    "import os\n",
    "\n",
    "# pathlib: Modern way to handle file paths (cross-platform compatible)\n",
    "# Example: Path('/home/user') / 'documents' / 'file.txt' = '/home/user/documents/file.txt'\n",
    "import pathlib\n",
    "\n",
    "# textwrap: Format text for display (wrap long lines, dedent)\n",
    "# Example: textwrap.wrap('long text...', width=50) breaks into lines of 50 chars\n",
    "import textwrap\n",
    "\n",
    "# glob: Find files matching a pattern (like shell wildcards)\n",
    "# Example: glob.glob('*.pdf') finds all PDF files in current directory\n",
    "import glob\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# DOCUMENT LOADERS - Load different types of documents\n",
    "# -----------------------------------------------------------------------------\n",
    "# These loaders convert raw files into LangChain Document objects\n",
    "# Each Document has: page_content (text) + metadata (source, page number, etc.)\n",
    "\n",
    "# UnstructuredURLLoader: Fetches and parses web pages into text\n",
    "# Example: loader = UnstructuredURLLoader(['https://example.com'])\n",
    "from langchain_community.document_loaders import UnstructuredURLLoader\n",
    "\n",
    "# TextLoader: Loads plain text files (.txt)\n",
    "# Example: loader = TextLoader('readme.txt')\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# PyPDFLoader: Extracts text from PDF files (page by page)\n",
    "# Example: loader = PyPDFLoader('manual.pdf')\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# TEXT SPLITTER - Break documents into smaller chunks\n",
    "# -----------------------------------------------------------------------------\n",
    "# Why split? LLMs have context limits, and smaller chunks = better retrieval\n",
    "# RecursiveCharacterTextSplitter tries to split at natural boundaries:\n",
    "# First by paragraphs (\\n\\n), then sentences (\\n), then words\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# VECTOR STORE - Store and search embeddings efficiently\n",
    "# -----------------------------------------------------------------------------\n",
    "# FAISS (Facebook AI Similarity Search) is a library for similarity search\n",
    "# It stores vectors and finds nearest neighbors very fast\n",
    "# Example: Given query vector, find 5 most similar document vectors\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# EMBEDDINGS - Convert text to numerical vectors\n",
    "# -----------------------------------------------------------------------------\n",
    "# Embeddings capture semantic meaning: similar meanings = similar vectors\n",
    "# \"king\" - \"man\" + \"woman\" ‚âà \"queen\" (famous example of semantic arithmetic)\n",
    "\n",
    "# OpenAIEmbeddings: Uses OpenAI's API (requires API key, costs money)\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# HuggingFaceEmbeddings: Uses local HuggingFace models (free, runs locally)\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# SentenceTransformerEmbeddings: Another wrapper for sentence-transformers\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# LLM (Large Language Model) - The \"brain\" that generates responses\n",
    "# -----------------------------------------------------------------------------\n",
    "# Ollama: Runs open-source LLMs locally on your machine\n",
    "# No API key needed, privacy-friendly, works offline\n",
    "from langchain.llms import Ollama\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# CHAINS - Connect components together into a pipeline\n",
    "# -----------------------------------------------------------------------------\n",
    "# ConversationalRetrievalChain: Combines retriever + LLM + memory\n",
    "# It handles: query ‚Üí retrieve docs ‚Üí format prompt ‚Üí generate answer\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# PROMPTS - Templates that structure how we talk to the LLM\n",
    "# -----------------------------------------------------------------------------\n",
    "# PromptTemplate: Create reusable prompts with variables\n",
    "# Example: PromptTemplate(\"Hello {name}!\") ‚Üí \"Hello Alice!\" when name=\"Alice\"\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "print(\"‚úÖ Libraries imported! You're good to go!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaacfdc",
   "metadata": {},
   "source": [
    "## 2 - Data Preparation\n",
    "\n",
    "### Theory: The Document Pipeline\n",
    "```\n",
    "Raw Files (PDF/HTML/TXT)\n",
    "         ‚Üì\n",
    "    [LOAD] ‚Üí Document objects with text + metadata\n",
    "         ‚Üì\n",
    "    [CHUNK] ‚Üí Smaller pieces (300-500 tokens each)\n",
    "         ‚Üì\n",
    "Ready for embedding!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87150689",
   "metadata": {},
   "source": [
    "### 2.1 - Ingest source documents (Load PDFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff055a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 2.1: LOAD PDF DOCUMENTS\n",
    "# ============================================================================\n",
    "# Goal: Read all PDF files and extract their text content\n",
    "#\n",
    "# How glob works:\n",
    "# - glob.glob(\"pattern\") returns list of matching file paths\n",
    "# - \"data/Everstorm_*.pdf\" matches any PDF starting with \"Everstorm_\"\n",
    "# - Example matches: data/Everstorm_Shipping.pdf, data/Everstorm_Returns.pdf\n",
    "# ============================================================================\n",
    "\n",
    "# Find all PDF files matching our pattern\n",
    "# The * is a wildcard that matches any characters\n",
    "pdf_paths = glob.glob(\"data/Everstorm_*.pdf\")\n",
    "\n",
    "# Initialize empty list to store all document pages\n",
    "# Each page becomes a separate Document object\n",
    "raw_docs = []\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# SOLUTION: Loop through each PDF and load its pages\n",
    "# -----------------------------------------------------------------------------\n",
    "# PyPDFLoader.load() returns a list of Document objects (one per page)\n",
    "# Each Document has:\n",
    "#   - page_content: The actual text from that page\n",
    "#   - metadata: Dict with 'source' (file path) and 'page' (page number)\n",
    "#\n",
    "# Example Document:\n",
    "# Document(\n",
    "#     page_content=\"Welcome to our store...\",\n",
    "#     metadata={'source': 'data/Everstorm_Shipping.pdf', 'page': 0}\n",
    "# )\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "for pdf_path in pdf_paths:\n",
    "    # Create a loader for this specific PDF file\n",
    "    # PyPDFLoader uses the PyPDF library under the hood\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    \n",
    "    # Load all pages from the PDF and add to our collection\n",
    "    # .load() reads the file, extracts text from each page\n",
    "    # .extend() adds all items from the list (vs .append() which adds the list itself)\n",
    "    raw_docs.extend(loader.load())\n",
    "\n",
    "# Verify loading worked\n",
    "print(f\"Loaded {len(raw_docs)} PDF pages from {len(pdf_paths)} files.\")\n",
    "\n",
    "# Let's peek at the first document to understand the structure\n",
    "if raw_docs:\n",
    "    print(f\"\\nüìÑ Sample document metadata: {raw_docs[0].metadata}\")\n",
    "    print(f\"üìù First 200 chars: {raw_docs[0].page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69f135e",
   "metadata": {},
   "source": [
    "### (Optional) 2.1b - Load web pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65abec32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OPTIONAL: LOAD WEB PAGES\n",
    "# ============================================================================\n",
    "# UnstructuredURLLoader fetches HTML pages and extracts readable text\n",
    "# It removes HTML tags, scripts, styles, leaving just the content\n",
    "# \n",
    "# Note: Web scraping can fail due to:\n",
    "#   - Network issues\n",
    "#   - Blocked requests (rate limiting, bot detection)\n",
    "#   - Page structure changes\n",
    "# Always have fallback logic!\n",
    "# ============================================================================\n",
    "\n",
    "URLS = [\n",
    "    # BigCommerce documentation about shipping\n",
    "    \"https://developer.bigcommerce.com/docs/store-operations/shipping\",\n",
    "    # BigCommerce documentation about refunds\n",
    "    \"https://developer.bigcommerce.com/docs/store-operations/orders/refunds\",\n",
    "]\n",
    "\n",
    "try:\n",
    "    # -----------------------------------------------------------------------------\n",
    "    # SOLUTION: Load web pages using UnstructuredURLLoader\n",
    "    # -----------------------------------------------------------------------------\n",
    "    # Create loader with list of URLs to fetch\n",
    "    url_loader = UnstructuredURLLoader(urls=URLS)\n",
    "    \n",
    "    # Fetch and parse all URLs (may take a few seconds)\n",
    "    web_docs = url_loader.load()\n",
    "    \n",
    "    # Add web documents to our raw_docs collection\n",
    "    raw_docs.extend(web_docs)\n",
    "    \n",
    "    print(f\"Fetched {len(web_docs)} documents from the web.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    # If web fetch fails, fall back to local PDFs only\n",
    "    print(\"‚ö†Ô∏è  Web fetch failed, using offline copies:\", e)\n",
    "    \n",
    "    # -----------------------------------------------------------------------------\n",
    "    # FALLBACK: Just use the PDFs we already loaded\n",
    "    # -----------------------------------------------------------------------------\n",
    "    # In production, you might load cached HTML files here\n",
    "    print(f\"Continuing with {len(raw_docs)} offline documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ac4490",
   "metadata": {},
   "source": [
    "### 2.2 - Chunk the text\n",
    "\n",
    "### Theory: Why Chunking Matters\n",
    "\n",
    "**Problem**: Documents can be thousands of tokens, but:\n",
    "1. LLMs have context limits (e.g., 4K, 8K, 128K tokens)\n",
    "2. Retrieval works better with focused chunks\n",
    "3. Embedding quality degrades for very long texts\n",
    "\n",
    "**Solution**: Split into 300-500 token chunks with overlap\n",
    "\n",
    "```\n",
    "Original: [========================================]\n",
    "                          ‚Üì split with overlap\n",
    "Chunks:   [=====]     ‚Üê Chunk 1\n",
    "             [=====]  ‚Üê Chunk 2 (overlaps with 1)\n",
    "                [=====] ‚Üê Chunk 3 (overlaps with 2)\n",
    "```\n",
    "\n",
    "**Why overlap?** Prevents cutting sentences in the middle!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450d48a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 2.2: SPLIT DOCUMENTS INTO CHUNKS\n",
    "# ============================================================================\n",
    "# RecursiveCharacterTextSplitter tries to split at natural boundaries:\n",
    "#   1. First tries to split at \"\\n\\n\" (paragraph breaks)\n",
    "#   2. If still too long, splits at \"\\n\" (line breaks)\n",
    "#   3. If still too long, splits at \" \" (spaces between words)\n",
    "#   4. Last resort: splits at character level\n",
    "#\n",
    "# Parameters:\n",
    "#   - chunk_size: Maximum characters per chunk (300 = ~75 tokens)\n",
    "#   - chunk_overlap: Characters shared between chunks (30 = ~7 tokens)\n",
    "#\n",
    "# Rule of thumb: 1 token ‚âà 4 characters in English\n",
    "# ============================================================================\n",
    "\n",
    "# Initialize empty list for our text chunks\n",
    "chunks = []\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# SOLUTION: Create splitter and split documents\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Create the text splitter with our chosen parameters\n",
    "# chunk_size=300: Each chunk will be at most 300 characters\n",
    "# chunk_overlap=30: Adjacent chunks share 30 characters (prevents lost context)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,      # Max characters per chunk\n",
    "    chunk_overlap=30,    # Overlap between chunks\n",
    "    length_function=len, # How to measure length (character count)\n",
    ")\n",
    "\n",
    "# Split all documents into chunks\n",
    "# .split_documents() preserves metadata from original documents\n",
    "# Each chunk knows which source file it came from!\n",
    "chunks = text_splitter.split_documents(raw_docs)\n",
    "\n",
    "print(f\"‚úÖ {len(chunks)} chunks ready for embedding\")\n",
    "\n",
    "# Let's see how chunking changed our data\n",
    "print(f\"\\nüìä Before: {len(raw_docs)} documents\")\n",
    "print(f\"üìä After: {len(chunks)} chunks\")\n",
    "\n",
    "# Peek at a sample chunk\n",
    "if chunks:\n",
    "    print(f\"\\nüìÑ Sample chunk (first 200 chars):\")\n",
    "    print(f\"   {chunks[0].page_content[:200]}...\")\n",
    "    print(f\"   Source: {chunks[0].metadata.get('source', 'Unknown')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89e6213",
   "metadata": {},
   "source": [
    "## 3 - Build a Retriever\n",
    "\n",
    "### Theory: How Semantic Search Works\n",
    "\n",
    "```\n",
    "Text: \"What is your return policy?\"\n",
    "         ‚Üì Embedding Model\n",
    "Vector: [0.23, -0.45, 0.12, ..., 0.78]  (384 dimensions)\n",
    "```\n",
    "\n",
    "**Key Insight**: Similar meanings ‚Üí Similar vectors!\n",
    "- \"return policy\" and \"refund policy\" have similar vectors\n",
    "- \"return policy\" and \"weather forecast\" have different vectors\n",
    "\n",
    "**Cosine Similarity**: Measures angle between vectors\n",
    "- 1.0 = identical direction (same meaning)\n",
    "- 0.0 = perpendicular (unrelated)\n",
    "- -1.0 = opposite (opposite meaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90db2d2",
   "metadata": {},
   "source": [
    "### 3.1 - Load embedding model and test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a222122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 3.1: CREATE EMBEDDING MODEL\n",
    "# ============================================================================\n",
    "# We use 'thenlper/gte-small' - a small but effective embedding model\n",
    "#   - 33 million parameters (very lightweight)\n",
    "#   - 384-dimensional embeddings\n",
    "#   - Free to use, runs locally\n",
    "#   - Good quality for its size\n",
    "#\n",
    "# Alternative models:\n",
    "#   - 'all-MiniLM-L6-v2': Popular, 384 dims, very fast\n",
    "#   - 'all-mpnet-base-v2': Better quality, 768 dims\n",
    "#   - OpenAI 'text-embedding-3-small': Best quality, requires API key\n",
    "# ============================================================================\n",
    "\n",
    "# Initialize the embedding model\n",
    "# First time running this downloads the model (~50MB)\n",
    "# SentenceTransformerEmbeddings wraps the sentence-transformers library\n",
    "embedder = SentenceTransformerEmbeddings(\n",
    "    model_name=\"thenlper/gte-small\"  # Small, fast, effective model\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# SOLUTION: Test the embedding model\n",
    "# -----------------------------------------------------------------------------\n",
    "# embed_query() converts a single text string into a vector\n",
    "# The vector is a list of floats representing semantic meaning\n",
    "\n",
    "# Create embedding for a test sentence\n",
    "test_text = \"Hello world!\"\n",
    "embedding_vector = embedder.embed_query(test_text)\n",
    "\n",
    "# Check the embedding dimensions\n",
    "print(f\"‚úÖ Embedding model loaded!\")\n",
    "print(f\"üìä Text: '{test_text}'\")\n",
    "print(f\"üìä Embedding dimension: {len(embedding_vector)}\")\n",
    "print(f\"üìä First 5 values: {embedding_vector[:5]}\")\n",
    "\n",
    "# Bonus: Show how similar texts get similar embeddings\n",
    "text_a = \"What is your return policy?\"\n",
    "text_b = \"How can I return an item?\"\n",
    "text_c = \"What's the weather today?\"\n",
    "\n",
    "vec_a = embedder.embed_query(text_a)\n",
    "vec_b = embedder.embed_query(text_b)\n",
    "vec_c = embedder.embed_query(text_c)\n",
    "\n",
    "# Simple cosine similarity calculation\n",
    "import numpy as np\n",
    "def cosine_similarity(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "print(f\"\\nüîç Similarity Demo:\")\n",
    "print(f\"   '{text_a}' vs '{text_b}': {cosine_similarity(vec_a, vec_b):.3f}\")\n",
    "print(f\"   '{text_a}' vs '{text_c}': {cosine_similarity(vec_a, vec_c):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebdae3d",
   "metadata": {},
   "source": [
    "### 3.2 - Build FAISS vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611eda64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 3.2: BUILD FAISS VECTOR INDEX\n",
    "# ============================================================================\n",
    "# FAISS (Facebook AI Similarity Search) efficiently stores and searches vectors\n",
    "#\n",
    "# How it works:\n",
    "#   1. We give FAISS all our chunk embeddings\n",
    "#   2. FAISS builds an optimized index structure\n",
    "#   3. At query time, FAISS quickly finds the k nearest neighbors\n",
    "#\n",
    "# Why FAISS?\n",
    "#   - Handles millions of vectors\n",
    "#   - Very fast similarity search (milliseconds)\n",
    "#   - Supports GPU acceleration\n",
    "#   - Easy to save/load from disk\n",
    "# ============================================================================\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# SOLUTION: Create vector store from chunks\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Step 1: Build the FAISS index from documents\n",
    "# FAISS.from_documents() does three things:\n",
    "#   a) Embeds all chunk texts using our embedding model\n",
    "#   b) Stores the vectors in a FAISS index\n",
    "#   c) Links each vector to its original Document (text + metadata)\n",
    "vectordb = FAISS.from_documents(\n",
    "    documents=chunks,      # Our text chunks (list of Document objects)\n",
    "    embedding=embedder     # The embedding model to use\n",
    ")\n",
    "\n",
    "# Step 2: Create a retriever from the vector store\n",
    "# The retriever wraps the vector store with a simple interface\n",
    "# search_kwargs={'k': 8} means return the 8 most similar chunks\n",
    "retriever = vectordb.as_retriever(\n",
    "    search_kwargs={'k': 8}  # Return top 8 most similar chunks\n",
    ")\n",
    "\n",
    "# Step 3: Save the index for later use (optional but recommended)\n",
    "# This saves both the vectors and the document metadata\n",
    "# Next time, you can load with: FAISS.load_local('faiss_index', embedder)\n",
    "vectordb.save_local(\"faiss_index\")\n",
    "\n",
    "# Step 4: Verify the index was created correctly\n",
    "print(\"‚úÖ Vector store with\", vectordb.index.ntotal, \"embeddings\")\n",
    "print(f\"üìÅ Index saved to 'faiss_index/' directory\")\n",
    "\n",
    "# Test the retriever with a sample query\n",
    "test_query = \"What is the return policy?\"\n",
    "test_results = retriever.invoke(test_query)\n",
    "print(f\"\\nüîç Test query: '{test_query}'\")\n",
    "print(f\"üìä Retrieved {len(test_results)} chunks\")\n",
    "if test_results:\n",
    "    print(f\"üìÑ Top result preview: {test_results[0].page_content[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c456f820",
   "metadata": {},
   "source": [
    "## 4 - Build the Generation Engine\n",
    "\n",
    "### Theory: Ollama Local LLM Server\n",
    "\n",
    "Ollama runs LLMs locally on your machine:\n",
    "- **No API key needed** - completely free\n",
    "- **Privacy** - data never leaves your computer\n",
    "- **Offline capable** - works without internet\n",
    "- **Many models** - Gemma, Llama, Mistral, etc.\n",
    "\n",
    "```bash\n",
    "# Install (one-time)\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "# Start server (keep running in background)\n",
    "ollama serve\n",
    "\n",
    "# Download model (one-time per model)\n",
    "ollama pull gemma3:1b\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba7e203",
   "metadata": {},
   "source": [
    "### 4.1 - Test LLM with Ollama (Sanity Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d34a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 4: LOAD AND TEST THE LLM\n",
    "# ============================================================================\n",
    "# Make sure Ollama is running before executing this cell!\n",
    "# In terminal: ollama serve\n",
    "# In another terminal: ollama pull gemma3:1b\n",
    "#\n",
    "# Temperature controls randomness:\n",
    "#   - 0.0 = deterministic (always same output)\n",
    "#   - 0.1 = mostly consistent (good for factual Q&A)\n",
    "#   - 0.7 = balanced (good for creative tasks)\n",
    "#   - 1.0 = very random (highly creative/unpredictable)\n",
    "# ============================================================================\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# SOLUTION: Initialize and test the LLM\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Initialize Ollama LLM client\n",
    "# model: which model to use (must be downloaded first)\n",
    "# temperature: controls randomness (low = more consistent)\n",
    "llm = Ollama(\n",
    "    model=\"gemma3:1b\",   # Gemma 3 1B parameter model\n",
    "    temperature=0.1      # Low temp for factual responses\n",
    ")\n",
    "\n",
    "# Test with a simple prompt\n",
    "# .invoke() sends the prompt to Ollama and returns the response\n",
    "test_prompt = \"What is 2 + 2? Answer in one word.\"\n",
    "response = llm.invoke(test_prompt)\n",
    "\n",
    "print(\"‚úÖ LLM is working!\")\n",
    "print(f\"‚ùì Prompt: {test_prompt}\")\n",
    "print(f\"ü§ñ Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6556263d",
   "metadata": {},
   "source": [
    "## 5 - Build the RAG Chain\n",
    "\n",
    "### Theory: The Complete RAG Pipeline\n",
    "\n",
    "```\n",
    "User Question: \"What is your return policy?\"\n",
    "         ‚Üì\n",
    "    [EMBED] ‚Üí Query vector\n",
    "         ‚Üì\n",
    "    [RETRIEVE] ‚Üí Top k similar chunks from FAISS\n",
    "         ‚Üì\n",
    "    [FORMAT PROMPT] ‚Üí System prompt + Context + Question\n",
    "         ‚Üì\n",
    "    [GENERATE] ‚Üí LLM creates answer from context\n",
    "         ‚Üì\n",
    "Answer: \"You can return items within 30 days...\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbfec15",
   "metadata": {},
   "source": [
    "### 5.1 - Define a system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcecb2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 5.1: DEFINE THE SYSTEM PROMPT\n",
    "# ============================================================================\n",
    "# The system prompt is CRITICAL for RAG quality!\n",
    "# It tells the LLM:\n",
    "#   1. What role to play (Customer Support Chatbot)\n",
    "#   2. What information to use (only the provided context)\n",
    "#   3. What to do when unsure (admit it doesn't know)\n",
    "#   4. How to format the response (concise, cite sources)\n",
    "#\n",
    "# A good RAG prompt prevents hallucination by:\n",
    "#   - Explicitly limiting the LLM to the provided context\n",
    "#   - Giving a clear fallback for unknown questions\n",
    "# ============================================================================\n",
    "\n",
    "# Define the system prompt template\n",
    "# {context} will be replaced with retrieved document chunks\n",
    "# {question} will be replaced with the user's question\n",
    "SYSTEM_TEMPLATE = \"\"\"\n",
    "You are a **Customer Support Chatbot** for Everstorm Outfitters.\n",
    "\n",
    "IMPORTANT RULES:\n",
    "1. Use ONLY the information in <context> to answer.\n",
    "2. If the answer is NOT in the context, say: \"I don't know based on the retrieved documents.\"\n",
    "3. Be concise and accurate. Quote key phrases from the context when helpful.\n",
    "4. When possible, cite the source document.\n",
    "5. Do NOT make up information or use outside knowledge.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "USER QUESTION:\n",
    "{question}\n",
    "\n",
    "ASSISTANT:\n",
    "\"\"\"\n",
    "\n",
    "print(\"‚úÖ System prompt defined!\")\n",
    "print(f\"üìù Template has {len(SYSTEM_TEMPLATE)} characters\")\n",
    "print(\"\\nüìã Template preview:\")\n",
    "print(SYSTEM_TEMPLATE[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de27cc9a",
   "metadata": {},
   "source": [
    "### 5.2 - Create the RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b138b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 5.2: BUILD THE RAG CHAIN\n",
    "# ============================================================================\n",
    "# ConversationalRetrievalChain connects all components:\n",
    "#   - Retriever: finds relevant documents\n",
    "#   - LLM: generates the answer\n",
    "#   - Prompt: structures how we ask the LLM\n",
    "#   - Memory: tracks conversation history (optional)\n",
    "#\n",
    "# Flow: question ‚Üí retrieve docs ‚Üí format prompt ‚Üí LLM ‚Üí answer\n",
    "# ============================================================================\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# SOLUTION: Create the complete RAG chain\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Step 1: Create a PromptTemplate from our system template\n",
    "# input_variables tells LangChain which parts to fill in\n",
    "rag_prompt = PromptTemplate(\n",
    "    template=SYSTEM_TEMPLATE,\n",
    "    input_variables=[\"context\", \"question\"]  # Variables to fill in\n",
    ")\n",
    "\n",
    "# Step 2: Ensure LLM is initialized (re-initialize for clarity)\n",
    "llm = Ollama(\n",
    "    model=\"gemma3:1b\",\n",
    "    temperature=0.1  # Low temperature for consistent, factual answers\n",
    ")\n",
    "\n",
    "# Step 3: Build the ConversationalRetrievalChain\n",
    "# This chains together: retriever ‚Üí prompt ‚Üí LLM\n",
    "chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,                              # The language model\n",
    "    retriever=retriever,                  # Our FAISS retriever\n",
    "    return_source_documents=True,         # Return the docs used (for debugging)\n",
    "    combine_docs_chain_kwargs={\n",
    "        \"prompt\": rag_prompt              # Our custom prompt template\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"‚úÖ RAG chain created!\")\n",
    "print(\"üîó Components connected: Retriever ‚Üí Prompt ‚Üí LLM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1eb44d",
   "metadata": {},
   "source": [
    "### 5.3 - Validate the RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6d7a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 5.3: TEST THE RAG CHAIN WITH SAMPLE QUESTIONS\n",
    "# ============================================================================\n",
    "# chat_history tracks previous Q&A pairs for context\n",
    "# Format: list of (question, answer) tuples\n",
    "# This allows follow-up questions like \"Can you tell me more about that?\"\n",
    "# ============================================================================\n",
    "\n",
    "# Sample questions to test our chatbot\n",
    "test_questions = [\n",
    "    \"If I'm not happy with my purchase, what is your refund policy and how do I start a return?\",\n",
    "    \"How long will delivery take for a standard order, and where can I track my package once it ships?\",\n",
    "    \"What's the quickest way to contact your support team, and what are your operating hours?\",\n",
    "]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# SOLUTION: Test the RAG chain\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Initialize empty chat history\n",
    "# As we ask questions, we'll add (question, answer) tuples\n",
    "chat_history = []\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ü§ñ RAG CHATBOT TEST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Loop through each test question\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\n‚ùì Question {i}: {question}\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    # Invoke the chain with the question and chat history\n",
    "    # The chain returns a dict with 'answer' and 'source_documents'\n",
    "    result = chain.invoke({\n",
    "        \"question\": question,\n",
    "        \"chat_history\": chat_history\n",
    "    })\n",
    "    \n",
    "    # Extract the answer\n",
    "    answer = result[\"answer\"]\n",
    "    \n",
    "    # Print the answer\n",
    "    print(f\"ü§ñ Answer: {answer}\")\n",
    "    \n",
    "    # Show which sources were used (helpful for debugging)\n",
    "    if \"source_documents\" in result:\n",
    "        sources = set([doc.metadata.get('source', 'Unknown') for doc in result[\"source_documents\"]])\n",
    "        print(f\"üìö Sources: {', '.join([s.split('/')[-1] for s in sources])}\")\n",
    "    \n",
    "    # Update chat history for context in follow-up questions\n",
    "    chat_history.append((question, answer))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ RAG chatbot test complete!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f8c6b4",
   "metadata": {},
   "source": [
    "## 6 - Build Streamlit UI (Optional)\n",
    "\n",
    "This creates a web interface for your chatbot. Run with: `streamlit run app.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c182fb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 6: CREATE STREAMLIT WEB APP\n",
    "# ============================================================================\n",
    "# This cell writes a complete Streamlit app to app.py\n",
    "# Run it with: streamlit run app.py\n",
    "# ============================================================================\n",
    "\n",
    "streamlit_code = '''\n",
    "# ===========================================================================\n",
    "# STREAMLIT RAG CHATBOT APP\n",
    "# ===========================================================================\n",
    "# A simple web interface for our customer support chatbot\n",
    "# Run with: streamlit run app.py\n",
    "# ===========================================================================\n",
    "\n",
    "import streamlit as st\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.llms import Ollama\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# PAGE CONFIGURATION\n",
    "# ---------------------------------------------------------------------------\n",
    "st.set_page_config(\n",
    "    page_title=\"Customer Support Chatbot\",\n",
    "    page_icon=\"üõçÔ∏è\",\n",
    "    layout=\"centered\"\n",
    ")\n",
    "\n",
    "st.title(\"üõçÔ∏è Everstorm Outfitters Support\")\n",
    "st.caption(\"Ask me about shipping, returns, payments, and more!\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# LOAD RAG COMPONENTS (cached for performance)\n",
    "# ---------------------------------------------------------------------------\n",
    "@st.cache_resource\n",
    "def load_chain():\n",
    "    \"\"\"Load and cache the RAG chain components.\"\"\"\n",
    "    # Load embeddings model\n",
    "    embedder = SentenceTransformerEmbeddings(model_name=\"thenlper/gte-small\")\n",
    "    \n",
    "    # Load saved FAISS index\n",
    "    vectordb = FAISS.load_local(\n",
    "        \"faiss_index\", \n",
    "        embedder,\n",
    "        allow_dangerous_deserialization=True  # Required for loading pickle files\n",
    "    )\n",
    "    retriever = vectordb.as_retriever(search_kwargs={\"k\": 8})\n",
    "    \n",
    "    # Initialize LLM\n",
    "    llm = Ollama(model=\"gemma3:1b\", temperature=0.1)\n",
    "    \n",
    "    # System prompt\n",
    "    SYSTEM_TEMPLATE = \"\"\"\n",
    "    You are a helpful Customer Support Chatbot for Everstorm Outfitters.\n",
    "    \n",
    "    Rules:\n",
    "    1. Use ONLY the provided context to answer.\n",
    "    2. If unsure, say \"I don\\'t know based on the documents.\"\n",
    "    3. Be concise and helpful.\n",
    "    \n",
    "    Context: {context}\n",
    "    \n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        template=SYSTEM_TEMPLATE,\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "    \n",
    "    # Build chain\n",
    "    chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "        combine_docs_chain_kwargs={\"prompt\": prompt}\n",
    "    )\n",
    "    return chain\n",
    "\n",
    "# Load the chain\n",
    "chain = load_chain()\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# CHAT INTERFACE\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# Initialize session state for chat history\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "if \"chat_history\" not in st.session_state:\n",
    "    st.session_state.chat_history = []\n",
    "\n",
    "# Display chat messages\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])\n",
    "\n",
    "# Chat input\n",
    "if prompt := st.chat_input(\"Ask a question about our policies...\"):\n",
    "    # Add user message to chat\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(prompt)\n",
    "    \n",
    "    # Generate response\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        with st.spinner(\"Thinking...\"):\n",
    "            result = chain.invoke({\n",
    "                \"question\": prompt,\n",
    "                \"chat_history\": st.session_state.chat_history\n",
    "            })\n",
    "            response = result[\"answer\"]\n",
    "            st.markdown(response)\n",
    "    \n",
    "    # Update history\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "    st.session_state.chat_history.append((prompt, response))\n",
    "\n",
    "# Sidebar with info\n",
    "with st.sidebar:\n",
    "    st.header(\"About\")\n",
    "    st.write(\"This chatbot answers questions using RAG.\")\n",
    "    st.write(\"**Powered by:**\")\n",
    "    st.write(\"- ü¶ú LangChain\")\n",
    "    st.write(\"- üìä FAISS\")\n",
    "    st.write(\"- ü§ñ Gemma 3 (via Ollama)\")\n",
    "    \n",
    "    if st.button(\"Clear Chat\"):\n",
    "        st.session_state.messages = []\n",
    "        st.session_state.chat_history = []\n",
    "        st.rerun()\n",
    "'''\n",
    "\n",
    "# Write the Streamlit app to a file\n",
    "with open(\"app.py\", \"w\") as f:\n",
    "    f.write(streamlit_code)\n",
    "\n",
    "print(\"‚úÖ Streamlit app saved to app.py\")\n",
    "print(\"\\nüöÄ To run the chatbot UI:\")\n",
    "print(\"   streamlit run app.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7328a5",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "You've built a complete **RAG-based customer support chatbot**!\n",
    "\n",
    "### What You Learned:\n",
    "\n",
    "| Concept | What It Does | Tool Used |\n",
    "|---------|--------------|----------|\n",
    "| **Document Loading** | Extracts text from PDFs/URLs | PyPDFLoader, UnstructuredURLLoader |\n",
    "| **Chunking** | Splits text into searchable pieces | RecursiveCharacterTextSplitter |\n",
    "| **Embeddings** | Converts text to semantic vectors | SentenceTransformerEmbeddings |\n",
    "| **Vector Store** | Stores & searches embeddings | FAISS |\n",
    "| **LLM** | Generates natural language answers | Ollama (Gemma 3) |\n",
    "| **RAG Chain** | Connects retrieval to generation | ConversationalRetrievalChain |\n",
    "\n",
    "### Next Steps:\n",
    "1. Try different embedding models\n",
    "2. Experiment with chunk sizes\n",
    "3. Test different LLMs (Llama, Mistral)\n",
    "4. Add more documents to your knowledge base"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
