{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f7fe8ac",
   "metadata": {},
   "source": [
    "# Project 3: **Ask‑the‑Web Agent**\n",
    "\n",
    "Welcome to Project 3! In this project, you will learn how to use tool‑calling LLMs, extend them with custom tools, and build a simplified *Perplexity‑style* agent that answers questions by searching the web."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4311a6",
   "metadata": {},
   "source": [
    "## Learning Objectives  \n",
    "* Understand why tool calling is useful and how LLMs can invoke external tools.\n",
    "* Implement a minimal loop that parses the LLM's output and executes a Python function.\n",
    "* See how *function schemas* (docstrings and type hints) let us scale to many tools.\n",
    "* Use **LangChain** to get function‑calling capability\n",
    "* Combine LLM with a web‑search tool to build a simple ask‑the‑web agent.\n",
    "* Connect to external tools using **MCP (Model Context Protocol)**, a universal standard for LLM‑tool integration.\n",
    "* Optionally build a UI using Chainlit to test your agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f16864",
   "metadata": {},
   "source": [
    "## Roadmap\n",
    "0. Environment setup\n",
    "1. Write simple tools and connect them to an LLM\n",
    "2. Standardize tool calling with JSON schemas\n",
    "3. Use LangGraph for tool calling\n",
    "4. Build a Perplexity-style web-search agent\n",
    "5. (Optional) MCP: connect to external tool servers\n",
    "6. (Optional) A minimal UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeae51d0",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "# 0- Environment setup\n",
    "\n",
    "### Step 1: Create your environment and install dependencies \n",
    "Before we start coding, you need a reproducible setup. Open a terminal in the same directory as this notebook, and use Conda or uv to install the project dependencies.\n",
    "\n",
    "#### Option 1: Conda\n",
    "\n",
    "\n",
    "```bash\n",
    "# Create and activate the conda environment\n",
    "conda env create -f environment.yml && conda activate web_agent\n",
    "\n",
    "```\n",
    "\n",
    "#### Option 2: UV (faster)\n",
    "\n",
    "If you prefer [uv](https://docs.astral.sh/uv/) over Conda:\n",
    "\n",
    "```bash\n",
    "# Install uv (skip if already installed)\n",
    "curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "\n",
    "# Create venv and install dependencies\n",
    "uv venv .venv-web-agent-uv && source .venv-web-agent-uv/bin/activate\n",
    "uv pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "### Step 2: Register this environment as a Jupyter kernel\n",
    "```bash\n",
    "python -m ipykernel install --user --name=web_agent --display-name \"web_agent\"\n",
    "```\n",
    "Now open your notebook and switch to the `web_agent` kernel (Kernel → Change Kernel).\n",
    "\n",
    "### Step 3: Set up Ollama\n",
    "\n",
    "In this project, we use **Ollama** to load and use open-weight LLMs. We start with smaller models like `gemma3:1b` and then switch to larger models like `llama3.2:3b`.\n",
    "\n",
    "Start the **Ollama** server in a terminal. This launches a local API endpoint that listens for LLM requests.\n",
    "\n",
    "```bash\n",
    "ollama serve\n",
    "```\n",
    "\n",
    "Downloads the model so you can run them locally without API calls. \n",
    "```bash\n",
    "ollama pull gemma3:1b\n",
    "ollama pull llama3.2:3b\n",
    "```\n",
    "\n",
    "You can explore other available models [here](https://ollama.com/library) and pull them to experiment with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bejkbbzrdy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is running. Installed models: ['gemma3:1b', 'gemma3:12b', 'deepseek-r1:8b', 'llama3.2:3b', 'phi4-mini:latest', 'qwen2.5:3b-instruct', 'llama3.2:1b', 'deepseek-r1:latest', 'gemma3:270m', 'llama2:7b', 'llama3:latest', 'gemma3:latest']\n"
     ]
    }
   ],
   "source": [
    "# Quick check: is Ollama running?\n",
    "# If this fails, open a terminal and run: ollama serve\n",
    "\n",
    "import httpx\n",
    "\n",
    "response = httpx.get(\"http://localhost:11434/api/tags\", timeout=5)\n",
    "models = [m[\"name\"] for m in response.json().get(\"models\", [])]\n",
    "print(f\"Ollama is running. Installed models: {models}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c27158f",
   "metadata": {},
   "source": [
    "## 1- Tool Calling\n",
    "\n",
    "LLMs are strong at answering questions, but they cannot directly access external data such as live web results, APIs, or computations. In real applications, agents rarely rely only on their internal knowledge. They need to query APIs, retrieve data, or perform calculations to stay accurate and useful. Tool calling bridges this gap by allowing the LLM to request actions from the outside world.\n",
    "\n",
    "<img src=\"assets/tools.png\" width=\"700\">\n",
    "\n",
    "As show below, We first implement a tool, then describe the tool as part of the model's prompt. When the model decides that a tool is needed, it emits a structured output. A parser will detect this output, execute the corresponding function, and feed the result back to the LLM so the conversation continues.\n",
    "\n",
    "<img src=\"assets/tool_flow.png\" width=\"700\">\n",
    "\n",
    "In this section, you will implement a simple `get_current_weather` function and teach the `gemma3:1b` model to use it when required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a536f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 1: Implement the tool\n",
    "# ---------------------------------------------------------\n",
    "# You can either:\n",
    "#   (a) Call a real weather API (for example, OpenWeatherMap), or\n",
    "#   (b) Create a dummy function that returns a fixed response (e.g., \"It is 23°C and sunny in San Francisco.\")\n",
    "#\n",
    "# Output:\n",
    "#   • Return a short, human-readable sentence describing the weather.\n",
    "#\n",
    "# Example expected behavior:\n",
    "#   get_current_weather(\"San Francisco\") → \"It is 23°C and sunny in San Francisco.\"\n",
    "#\n",
    "\n",
    "def get_current_weather(city, unit = \"celsius\") -> str:\n",
    "    return f\"It is 23°{unit[0].upper()} and sunny in {city}.\" # no real API call to stay offline-friendly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a43c3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Step 2: Create a prompt to teach the LLM when and how to use your tool\n",
    "# ----------------------------------------------------------------------\n",
    "# What to include:\n",
    "#   • A SYSTEM_PROMPT that tells the model about the tool use and describes the tool\n",
    "#   • A USER_QUESTION with a user query that should trigger the tool.\n",
    "#       Example: \"What is the weather in San Diego today?\"\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are an assistant that can call tools. \"\n",
    "    \"When the user asks something requiring fresh data, respond **only** with a JSON like: \"\n",
    "    'TOOL_CALL:{\"name\": <tool_name>, \"args\": { ... }}.'\n",
    ")\n",
    "\n",
    "TOOLS_SPEC = \"\"\"\n",
    "You can call exactly one tool:\n",
    "- name: get_current_weather\n",
    "  description: Return the current weather for a city.\n",
    "  arguments:\n",
    "    city: string\n",
    "    unit: \"celsius\" | \"fahrenheit\"  (optional, default \"celsius\")\n",
    "\"\"\"\n",
    "\n",
    "# USER_QUESTION = \"What is your name?\"\n",
    "USER_QUESTION = \"What is the weather in San Diego today?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eebb062",
   "metadata": {},
   "source": [
    "Now that you have defined a tool and shown the model how to use it, the next step is to call the LLM using your prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027cb75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Model output:\n",
      " TOOL_CALL:{\"name\": \"get_current_weather\", \"args\": { \"city\": \"San Diego\", \"unit\": \"celsius\" }}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 3: Call the LLM with your prompt\n",
    "# ---------------------------------------------------------\n",
    "# Task:\n",
    "#   Send SYSTEM_PROMPT + USER_QUESTION to the model.\n",
    "#\n",
    "# Steps:\n",
    "#   1. Create an Ollama client\n",
    "#   2. Use chat.completions.create to send your prompt to gemma3:1b\n",
    "#   3. Print the response.\n",
    "#\n",
    "# Expected:\n",
    "#   The model should return something like:\n",
    "#   TOOL_CALL: {\"name\": \"get_current_weather\", \"args\": {\"city\": \"San Diego\"}}\n",
    "# ---------------------------------------------------------\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key = \"ollama\", base_url = \"http://localhost:11434/v1\")\n",
    "\n",
    "MODEL = \"gemma3:1b\"\n",
    "# MODEL = \"llama3.2:3b\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT + \"\\n\\n\" + TOOLS_SPEC},\n",
    "        {\"role\": \"user\", \"content\": USER_QUESTION},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "output = response.choices[0].message.content\n",
    "print(\"\\n Model output:\\n\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94aeb4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calling tool `get_current_weather` with args {'city': 'San Diego', 'unit': 'celsius'}\n",
      "Result: It is 23°C and sunny in San Diego.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 4: Manually parse the LLM output and call the tool\n",
    "# ---------------------------------------------------------\n",
    "# Task:\n",
    "#   Detect when the model requests a tool, extract its name and arguments,\n",
    "#   and execute the corresponding function.\n",
    "#\n",
    "# Steps:\n",
    "#   1. Search for the text pattern \"TOOL_CALL:{...}\" in the model output.\n",
    "#   2. Parse the JSON inside it to get the tool name and args.\n",
    "#   3. Call the matching function (e.g., get_current_weather).\n",
    "#\n",
    "# Expected:\n",
    "#   You should see a line like:\n",
    "#       Calling tool `get_current_weather` with args {'city': 'San Diego'}\n",
    "#       Result: It is 23°C and sunny in San Diego.\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "import re, json\n",
    "\n",
    "pattern = r'TOOL_CALL:\\s*({.*})'\n",
    "match = re.search(pattern, output, flags=re.DOTALL)\n",
    "if match:\n",
    "    call_json = json.loads(match.group(1))\n",
    "    fn_name   = call_json[\"name\"]\n",
    "    args      = call_json.get(\"args\", {})\n",
    "    print(f\"\\nCalling tool `{fn_name}` with args {args}\")\n",
    "    result = globals()[fn_name](**args)\n",
    "    print(\"Result:\", result)\n",
    "else:\n",
    "    print(\"No tool call detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be26661",
   "metadata": {},
   "source": [
    "# 2- Standardize tool calling\n",
    "\n",
    "So far, we handled tool calling manually by writing a function, manually teaching the LLM about it, and write a regex to parse the output. This approach does not scale if we want to add more tools. Adding more tools would mean more `if/else` blocks and manual edits to the prompt.\n",
    "\n",
    "To make the system flexible, we can standardize tool definitions by automatically reading each function's signature, converting it to a JSON schema, and passing that schema to the LLM. This way, the LLM can dynamically understand which tools exist and how to call them without requiring manual updates to prompts or conditional logic.\n",
    "\n",
    "Next, you will implement a small helper that extracts metadata from functions and builds a schema for each tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce911b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'description': 'Return the current weather as a human-readable sentence.',\n",
      " 'name': 'get_current_weather',\n",
      " 'parameters': {'properties': {'city': {'description': 'Argument city',\n",
      "                                        'type': 'string'},\n",
      "                               'unit': {'description': 'Argument unit',\n",
      "                                        'type': 'string'}},\n",
      "                'required': ['city']}}\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Generate a JSON schema for a tool automatically\n",
    "# ---------------------------------------------------------\n",
    "#\n",
    "# Steps:\n",
    "#   1. Rewrite the get_current_weather function with docstring and arg types\n",
    "#   2. Use `inspect.signature` to automatically get function parameters and docstring\n",
    "#   2. For each argument, record its name, type, and description.\n",
    "#   3. Build a schema containing: name, description, and parameters.\n",
    "#   4. Test your helper on `get_current_weather` and print the result.\n",
    "#\n",
    "# Expected:\n",
    "#   A dictionary describing the tool (its name, args, and types).\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "from pprint import pprint\n",
    "import inspect\n",
    "\n",
    "def get_current_weather(city: str, unit: str = \"celsius\") -> str:\n",
    "    \"\"\"Return the current weather as a human-readable sentence.\"\"\"\n",
    "    return f\"It is 23°{unit[0].upper()} and sunny in {city}.\" # no real API call to stay offline-friendly\n",
    "\n",
    "def to_schema(fn):\n",
    "    sig = inspect.signature(fn)\n",
    "    props = {\n",
    "        name: {\n",
    "            'type': 'string' if param.annotation is str else 'number',\n",
    "            'description': f\"Argument {name}\"\n",
    "        }\n",
    "        for name, param in sig.parameters.items()\n",
    "    }\n",
    "    return {\n",
    "        'name': fn.__name__,\n",
    "        'description': (fn.__doc__ or '').strip().split('\\n')[0],\n",
    "        'parameters': {\n",
    "            'properties': props,\n",
    "            'required': [n for n, p in sig.parameters.items() if p.default is inspect._empty]\n",
    "        }\n",
    "    }\n",
    "\n",
    "tool_schema = to_schema(get_current_weather)\n",
    "pprint(tool_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1163f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOOL_CALL:{\"name\": \"get_current_weather\", \"args\": { \"properties\": { \"city\": \"Boston\" }, \"unit\": \"Celsius\"} }\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Provide the tool schema to the model instead of prompt surgery\n",
    "# ---------------------------------------------------------\n",
    "# Goal:\n",
    "#   Give the model a \"menu\" of available tools so it can choose\n",
    "#   which one to call based on the user’s question.\n",
    "#\n",
    "# Steps:\n",
    "#   1. Add an extra system message (e.g., name=\"tool_spec\")\n",
    "#      containing the JSON schema(s) of your tools.\n",
    "#   2. Include SYSTEM_PROMPT and the user question as before.\n",
    "#   3. Send the messages to the model (e.g., gemma3:1b).\n",
    "#   4. Print the model output to see if it picks the right tool.\n",
    "#\n",
    "# Expected:\n",
    "#   The model should produce a structured TOOL_CALL indicating\n",
    "#   which tool to use and with what arguments.\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "messages=[\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"system\", \"name\": \"tool_spec\", \"content\": json.dumps([tool_schema])},\n",
    "    {\"role\": \"user\", \"content\": \"Is Boston warmer today or Seattle?\"}\n",
    "]\n",
    "MODEL = \"gemma3:1b\" # doesn't produce two calls\n",
    "# MODEL = \"llama3.2:3b\" # produces two calls\n",
    "output = client.chat.completions.create(\n",
    "    model=MODEL, \n",
    "    messages=messages,\n",
    "    temperature=0.5,\n",
    ")\n",
    "print(output.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8ec86e",
   "metadata": {},
   "source": [
    "## 3- LangChain for Tool Calling\n",
    "\n",
    "So far, you built a simple tool-calling pipeline. While this helps you understand the logic, it does not scale well when working with multiple tools, complex parsing, or multi-step reasoning. We have to write manual parsers, function calling logic, and adding responses back to the prompt.\n",
    "\n",
    "LangChain simplifies this process. You only need to declare your tools, and its *Agent* abstraction handles when to call a tool, how to use it, and how to continue reasoning afterward. In this section, you will create a **ReAct** Agent (Reasoning + Acting). As shown below, the model alternates between reasoning steps and tool use wihtout any manual work.\n",
    "\n",
    "<img src=\"assets/react.png\" width=\"500\">\n",
    "\n",
    "The following links might be helpful for completing this section:\n",
    "- [Create Agents](https://docs.langchain.com/oss/python/langchain/agents)\n",
    "- [LangChain Tools](https://docs.langchain.com/oss/python/langchain/tools)\n",
    "- [Ollama](https://docs.langchain.com/oss/python/integrations/chat/ollama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c609d97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 1: Define tools for LangChain\n",
    "# ---------------------------------------------------------\n",
    "# Steps:\n",
    "#   1. Keep your existing `get_current_weather` function as before.\n",
    "#   2. Create a new function (e.g., get_weather) that calls it.\n",
    "#   3. Add the `@tool` decorator so LangChain can register it automatically.\n",
    "#\n",
    "# Notes:\n",
    "#   • The decorator converts your Python function into a standardized tool object.\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "def get_current_weather(city: str, unit: str = \"celsius\") -> str:\n",
    "    \"\"\"Return the current weather as a human-readable sentence.\"\"\"\n",
    "    return f\"It is 23°{unit[0].upper()} and sunny in {city}.\"\n",
    "\n",
    "@tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get current weather for a city and return results.\"\"\"\n",
    "    return get_current_weather(city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "daa159c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ResponseError",
     "evalue": "registry.ollama.ai/library/gemma3:1b does not support tools (status code: 400)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResponseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 26\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Call your agent using agent.invoke\u001b[39;00m\n\u001b[1;32m     23\u001b[0m messages\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     24\u001b[0m     [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDo I need an umbrella in Seattle today?\u001b[39m\u001b[38;5;124m\"\u001b[39m}]\n\u001b[1;32m     25\u001b[0m }\n\u001b[0;32m---> 26\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[0;32m~/Desktop/claude_p3/.venv-web-agent-uv/lib/python3.10/site-packages/langgraph/pregel/main.py:3071\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[0m\n\u001b[1;32m   3068\u001b[0m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m|\u001b[39m Any] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   3069\u001b[0m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 3071\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m   3072\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   3073\u001b[0m     config,\n\u001b[1;32m   3074\u001b[0m     context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[1;32m   3075\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdates\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   3076\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3077\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m stream_mode,\n\u001b[1;32m   3078\u001b[0m     print_mode\u001b[38;5;241m=\u001b[39mprint_mode,\n\u001b[1;32m   3079\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m   3080\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[1;32m   3081\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[1;32m   3082\u001b[0m     durability\u001b[38;5;241m=\u001b[39mdurability,\n\u001b[1;32m   3083\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3084\u001b[0m ):\n\u001b[1;32m   3085\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   3086\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunk) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/claude_p3/.venv-web-agent-uv/lib/python3.10/site-packages/langgraph/pregel/main.py:2646\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2644\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mmatch_cached_writes():\n\u001b[1;32m   2645\u001b[0m     loop\u001b[38;5;241m.\u001b[39moutput_writes(task\u001b[38;5;241m.\u001b[39mid, task\u001b[38;5;241m.\u001b[39mwrites, cached\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 2646\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   2647\u001b[0m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mwrites],\n\u001b[1;32m   2648\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   2649\u001b[0m     get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   2650\u001b[0m     schedule_task\u001b[38;5;241m=\u001b[39mloop\u001b[38;5;241m.\u001b[39maccept_push,\n\u001b[1;32m   2651\u001b[0m ):\n\u001b[1;32m   2652\u001b[0m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   2653\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _output(\n\u001b[1;32m   2654\u001b[0m         stream_mode, print_mode, subgraphs, stream\u001b[38;5;241m.\u001b[39mget, queue\u001b[38;5;241m.\u001b[39mEmpty\n\u001b[1;32m   2655\u001b[0m     )\n\u001b[1;32m   2656\u001b[0m loop\u001b[38;5;241m.\u001b[39mafter_tick()\n",
      "File \u001b[0;32m~/Desktop/claude_p3/.venv-web-agent-uv/lib/python3.10/site-packages/langgraph/pregel/_runner.py:167\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[1;32m    165\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 167\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/Desktop/claude_p3/.venv-web-agent-uv/lib/python3.10/site-packages/langgraph/pregel/_retry.py:42\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     40\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     44\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[0;32m~/Desktop/claude_p3/.venv-web-agent-uv/lib/python3.10/site-packages/langgraph/_internal/_runnable.py:656\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    654\u001b[0m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[0;32m--> 656\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/Desktop/claude_p3/.venv-web-agent-uv/lib/python3.10/site-packages/langgraph/_internal/_runnable.py:400\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(ret)\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 400\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/Desktop/claude_p3/.venv-web-agent-uv/lib/python3.10/site-packages/langchain/agents/factory.py:1166\u001b[0m, in \u001b[0;36mcreate_agent.<locals>.model_node\u001b[0;34m(state, runtime)\u001b[0m\n\u001b[1;32m   1153\u001b[0m request \u001b[38;5;241m=\u001b[39m ModelRequest(\n\u001b[1;32m   1154\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   1155\u001b[0m     tools\u001b[38;5;241m=\u001b[39mdefault_tools,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1161\u001b[0m     runtime\u001b[38;5;241m=\u001b[39mruntime,\n\u001b[1;32m   1162\u001b[0m )\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wrap_model_call_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;66;03m# No handlers - execute directly\u001b[39;00m\n\u001b[0;32m-> 1166\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_execute_model_sync\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1168\u001b[0m     \u001b[38;5;66;03m# Call composed handler with base handler\u001b[39;00m\n\u001b[1;32m   1169\u001b[0m     response \u001b[38;5;241m=\u001b[39m wrap_model_call_handler(request, _execute_model_sync)\n",
      "File \u001b[0;32m~/Desktop/claude_p3/.venv-web-agent-uv/lib/python3.10/site-packages/langchain/agents/factory.py:1137\u001b[0m, in \u001b[0;36mcreate_agent.<locals>._execute_model_sync\u001b[0;34m(request)\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request\u001b[38;5;241m.\u001b[39msystem_message:\n\u001b[1;32m   1135\u001b[0m     messages \u001b[38;5;241m=\u001b[39m [request\u001b[38;5;241m.\u001b[39msystem_message, \u001b[38;5;241m*\u001b[39mmessages]\n\u001b[0;32m-> 1137\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name:\n\u001b[1;32m   1139\u001b[0m     output\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m name\n",
      "File \u001b[0;32m~/Desktop/claude_p3/.venv-web-agent-uv/lib/python3.10/site-packages/langchain_core/runnables/base.py:5557\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5550\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m   5551\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   5552\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5555\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   5556\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 5557\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5558\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5559\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5560\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5561\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/claude_p3/.venv-web-agent-uv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:402\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    396\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AIMessage:\n\u001b[1;32m    397\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAIMessage\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    400\u001b[0m         cast(\n\u001b[1;32m    401\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 402\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    412\u001b[0m         )\u001b[38;5;241m.\u001b[39mmessage,\n\u001b[1;32m    413\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/claude_p3/.venv-web-agent-uv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:1121\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m   1114\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1119\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m   1120\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/claude_p3/.venv-web-agent-uv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:931\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[1;32m    929\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    930\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 931\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    937\u001b[0m         )\n\u001b[1;32m    938\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    939\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/Desktop/claude_p3/.venv-web-agent-uv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:1233\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1231\u001b[0m     result \u001b[38;5;241m=\u001b[39m generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[1;32m   1232\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1233\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1235\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1237\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/claude_p3/.venv-web-agent-uv/lib/python3.10/site-packages/langchain_ollama/chat_models.py:1030\u001b[0m, in \u001b[0;36mChatOllama._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_generate\u001b[39m(\n\u001b[1;32m   1024\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1025\u001b[0m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1028\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1029\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[0;32m-> 1030\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chat_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1033\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m final_chunk\u001b[38;5;241m.\u001b[39mgeneration_info\n\u001b[1;32m   1034\u001b[0m     chat_generation \u001b[38;5;241m=\u001b[39m ChatGeneration(\n\u001b[1;32m   1035\u001b[0m         message\u001b[38;5;241m=\u001b[39mAIMessage(\n\u001b[1;32m   1036\u001b[0m             content\u001b[38;5;241m=\u001b[39mfinal_chunk\u001b[38;5;241m.\u001b[39mtext,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1043\u001b[0m         generation_info\u001b[38;5;241m=\u001b[39mgeneration_info,\n\u001b[1;32m   1044\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/claude_p3/.venv-web-agent-uv/lib/python3.10/site-packages/langchain_ollama/chat_models.py:965\u001b[0m, in \u001b[0;36mChatOllama._chat_stream_with_aggregation\u001b[0;34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    956\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_chat_stream_with_aggregation\u001b[39m(\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    958\u001b[0m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    963\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatGenerationChunk:\n\u001b[1;32m    964\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 965\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterate_over_stream(messages, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    966\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m final_chunk \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    967\u001b[0m             final_chunk \u001b[38;5;241m=\u001b[39m chunk\n",
      "File \u001b[0;32m~/Desktop/claude_p3/.venv-web-agent-uv/lib/python3.10/site-packages/langchain_ollama/chat_models.py:1054\u001b[0m, in \u001b[0;36mChatOllama._iterate_over_stream\u001b[0;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_iterate_over_stream\u001b[39m(\n\u001b[1;32m   1048\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1049\u001b[0m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[1;32m   1050\u001b[0m     stop: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1051\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1052\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[ChatGenerationChunk]:\n\u001b[1;32m   1053\u001b[0m     reasoning \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreasoning)\n\u001b[0;32m-> 1054\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_stream(messages, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1055\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(stream_resp, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1056\u001b[0m             content \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1057\u001b[0m                 stream_resp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1058\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m stream_resp \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m stream_resp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1059\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1060\u001b[0m             )\n",
      "File \u001b[0;32m~/Desktop/claude_p3/.venv-web-agent-uv/lib/python3.10/site-packages/langchain_ollama/chat_models.py:952\u001b[0m, in \u001b[0;36mChatOllama._create_chat_stream\u001b[0;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chat_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    951\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client:\n\u001b[0;32m--> 952\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mchat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mchat_params)\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client:\n\u001b[1;32m    954\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mchat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mchat_params)\n",
      "File \u001b[0;32m~/Desktop/claude_p3/.venv-web-agent-uv/lib/python3.10/site-packages/ollama/_client.py:179\u001b[0m, in \u001b[0;36mClient._request.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    178\u001b[0m   e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m--> 179\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext, e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_lines():\n\u001b[1;32m    182\u001b[0m   part \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(line)\n",
      "\u001b[0;31mResponseError\u001b[0m: registry.ollama.ai/library/gemma3:1b does not support tools (status code: 400)"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 2: Create the Agent\n",
    "# ---------------------------------------------------------\n",
    "# Steps:\n",
    "#   1. Create a gemma3:1b LLM instance \n",
    "#   2. Create the agent using create_agent\n",
    "#   3. Test the agent with a natural question using agent.invoke\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "# Create an LLM instance\n",
    "MODEL = \"gemma3:1b\"\n",
    "llm = ChatOllama(model=MODEL, temperature=0)\n",
    "\n",
    "# Create your tool list\n",
    "tools = [get_weather]\n",
    "\n",
    "# Create your agent\n",
    "agent = create_agent(llm, tools)\n",
    "\n",
    "# Call your agent using agent.invoke\n",
    "messages={\"messages\":\n",
    "    [{\"role\": \"user\", \"content\": \"Do I need an umbrella in Seattle today?\"}]\n",
    "}\n",
    "out = agent.invoke(messages)\n",
    "print(out[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5824437",
   "metadata": {},
   "source": [
    "### What just happened?\n",
    "Your run failed because `gemma3:1b` does not support native tool calling (function calling). LangChain expects the model to return a structured tool-call object, but `gemma3:1b` can only return plain text, so the tool invocation step breaks.\n",
    "\n",
    "### Why previosuly, our manual approach worked with any model?\n",
    "\n",
    "In previous sections, we used **text-based tool calling**. We described the tool format in the system prompt. We asked the model to output `TOOL_CALL: {\"name\": ..., \"args\": ...}`. We then parsed this text with regex.\n",
    "\n",
    "This works with **any model** (even small ones like `gemma3:1b`) because we're just asking the model to follow a certain structured output format.\n",
    "\n",
    "### Why LangChain requires specific models?\n",
    "\n",
    "LangChain relies on **native tool calling** and it expects a consistent structured output format irrespective of the model. Hence, it enfornces model outputs structured tool calls in a specific format. This requires models trained specifically for function calling\n",
    "\n",
    "**Rule of thumb**: Models under 3B parameters typically lack native tool-calling capability.\n",
    "\n",
    "| Model | Size | Native Tool Support | Notes |\n",
    "|-------|------|---------------------|-------|\n",
    "| `gemma3:1b` | 1B | No | Works for manual approach only |\n",
    "| `llama3.2:1b` | 1B | No | Works for manual approach only |\n",
    "| `llama3.2:3b` | 3B | Yes | Good balance of speed and capability |\n",
    "| `gemma3` | 4B | Yes | Supports native tools |\n",
    "| `mistral` | 7B | Yes | Strong tool support |\n",
    "\n",
    "Let's fix the issue we observed in the previous cell.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9552348d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You don't need an umbrella in Seattle today.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 2 (retry): Re-create the Agent with a native tool-calling LLM\n",
    "# ---------------------------------------------------------\n",
    "# Steps:\n",
    "#   1. Create a llama3.2:3b LLM instance \n",
    "#   2. Create a system prompt to teach react-style reasoning to the LLM\n",
    "#   3. Create the agent using create_agent\n",
    "#   4. Test the agent with a natural question using agent.invoke\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "# Create an LLM instance\n",
    "MODEL = \"llama3.2:3b\"\n",
    "llm = ChatOllama(model=MODEL, temperature=0)\n",
    "\n",
    "# Create your tool list\n",
    "tools = [get_weather]\n",
    "\n",
    "# Create a ReAct-Style System Prompt\n",
    "react_system_prompt = \"\"\"\n",
    "You are a ReAct-style tool-using assistant.\n",
    "\n",
    "Loop:\n",
    "- Decide if you need a tool.\n",
    "- If yes, call one tool with a specific input.\n",
    "- Read the result and repeat until you can answer.\n",
    "\n",
    "Rules:\n",
    "- Use tools for factual, time-sensitive, or “latest/current/price/schedule/who is” questions.\n",
    "- Never invent tool outputs or sources.\n",
    "- If tools fail or are insufficient, say what you could not verify.\n",
    "\n",
    "Output:\n",
    "- Give a clear final answer. Do not reveal internal reasoning.\n",
    "\"\"\"\n",
    "\n",
    "# Create your agent\n",
    "agent = create_agent(llm, tools, system_prompt=react_system_prompt)\n",
    "\n",
    "# Call your agent using agent.invoke\n",
    "messages={\"messages\":\n",
    "    [{\"role\": \"user\", \"content\": \"Do I need an umbrella in Seattle today?\"}]\n",
    "}\n",
    "out = agent.invoke(messages)\n",
    "print(out[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yn0fe4dnbf",
   "metadata": {},
   "source": [
    "## 4- Web Search Agent\n",
    "\n",
    "Now that you know how to use LangChain with tools, let's build something useful. Instead of a toy get_weather tool, let create an agent that searches the web and answers questions using real results. In the next section, you will create a [DuckDuckGo](https://github.com/deedy5/ddgs) search tool and wire it into a ReAct agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cb0ec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 1: Write a web search tool\n",
    "# ---------------------------------------------------------\n",
    "# Steps:\n",
    "#   1. Write a helper function (e.g., search_web) that:\n",
    "#        • Takes a query string\n",
    "#        • Uses DuckDuckGo (DDGS) to fetch top results (titles + URLs)\n",
    "#        • Returns them as a formatted string\n",
    "#   2. Wrap it with the @tool decorator to make it available to LangChain.\n",
    "\n",
    "\n",
    "from ddgs import DDGS\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "def search_web(query: str, max_results: int = 10) -> str:\n",
    "    \"\"\"Return the top `max_results` web results for `query` as a single\n",
    "    formatted string. Uses DuckDuckGo's unofficial API.\"\"\"\n",
    "    results = []\n",
    "    with DDGS() as ddgs:\n",
    "        for r in ddgs.text(query, max_results=max_results):\n",
    "            results.append(f\"- {r['title']} — {r['href']}\")\n",
    "    return \"\\n\".join(results)\n",
    "\n",
    "@tool\n",
    "def web_search(query: str) -> str:\n",
    "    \"\"\"Search the web for a query and return concise results.\"\"\"\n",
    "    return search_web(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fdc4fc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 2: Initialize the web-search agent\n",
    "# ---------------------------------------------------------\n",
    "# Steps:\n",
    "#   1. Create an LLM (e.g., ChatOllama).\n",
    "#   2. Add your `web_search` tool to the tools list.\n",
    "#   3. Create the agent using create_agent.\n",
    "#\n",
    "# Expected:\n",
    "#   The agent should be ready to accept user queries\n",
    "#   and use your web search tool when needed.\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "MODEL = \"llama3.2:3b\"\n",
    "llm = ChatOllama(model=MODEL, temperature=0)\n",
    "tools = [web_search]\n",
    "\n",
    "web_agent = create_agent(llm, tools)\n",
    "\n",
    "def run_agent_with_reasoning(agent, question: str) -> str:\n",
    "    \"\"\"\n",
    "    Run the agent and display reasoning steps.\n",
    "    Returns the final answer.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    final_answer = \"\"\n",
    "    \n",
    "    for step in agent.stream({\"messages\": [(\"user\", question)]}):\n",
    "        for node_name, node_output in step.items():\n",
    "            print(f\"\\n[{node_name.upper()}]\")\n",
    "            \n",
    "            if \"messages\" in node_output:\n",
    "                for msg in node_output[\"messages\"]:\n",
    "                    if hasattr(msg, \"tool_calls\") and msg.tool_calls:\n",
    "                        for tc in msg.tool_calls:\n",
    "                            print(f\"  Tool: {tc['name']}\")\n",
    "                            print(f\"  Args: {tc['args']}\")\n",
    "                    elif hasattr(msg, \"content\") and msg.content:\n",
    "                        content = str(msg.content)\n",
    "                        # Truncate long outputs (like search results)\n",
    "                        if len(content) > 500:\n",
    "                            print(f\"  {content[:500]}...\")\n",
    "                        else:\n",
    "                            print(f\"  {content}\")\n",
    "                        final_answer = content\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1696c281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Question: What are the current events in San Francisco this week?\n",
      "============================================================\n",
      "\n",
      "[MODEL]\n",
      "  Tool: web_search\n",
      "  Args: {'query': 'San Francisco current events this week'}\n",
      "\n",
      "[TOOLS]\n",
      "  - San Francisco - Wikipedia — https://en.wikipedia.org/wiki/San_Francisco\n",
      "- ISACA San Francisco Current Events — http://www.sfisaca.org/events.htm\n",
      "- Current Local Time in San Francisco , California, USA — https://www.timeanddate.com/worldclock/usa/san-francisco\n",
      "- San Francisco Raids Ice | TikTok — https://www.tiktok.com/discover/san-francisco-raids-ice\n",
      "- Time in San Francisco , California, United States now — https://time.is/San_Francisco\n",
      "- San Francisco Current Events - YouTube — https://www.yo...\n",
      "\n",
      "[MODEL]\n",
      "  It appears that there are several current events happening in San Francisco this week. Here are a few:\n",
      "\n",
      "* Riordan High School has suspended in-person classes for one week due to a tuberculosis outbreak.\n",
      "* The San Francisco Giants are playing at home, and fans can catch their games on the official MLB.com website.\n",
      "* There are various events happening around the city, including the GDC Festival of Gaming.\n",
      "\n",
      "If you're looking for more information or specific details about these events, I recommend c...\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 3: Test your Ask-the-Web agent using agent.invoke\n",
    "# ---------------------------------------------------------\n",
    "try:\n",
    "    answer = run_agent_with_reasoning(\n",
    "        web_agent, \n",
    "        \"What are the current events in San Francisco this week?\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyzwk2etyt",
   "metadata": {},
   "source": [
    "## 5- (Optional) MCP: Model Context Protocol\n",
    "\n",
    "Up to now, every tool you used started as a Python function you wrote and registered yourself. **MCP (Model Context Protocol)** lets you skip that step. Tools come from an external *server*, and your code just connects to it. Think of it like USB for AI tools: any MCP client can plug into any MCP server and immediately use whatever tools it offers.\n",
    "\n",
    "Below, we connect to `mcp-server-fetch` (a ready-made server that can retrieve any URL) using the Python MCP SDK. We launch the server, discover its tools, and call one, all without writing a single `@tool` function. To learn more, read: https://github.com/modelcontextprotocol/servers/tree/main/src/fetch\n",
    "\n",
    "> **LangChain integration:** The `langchain-mcp-adapters` package can convert MCP tools into LangChain-compatible tools automatically, so you can drop them straight into a ReAct agent like the ones in section 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "buxkz996bq",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fetch server — 1 tool(s):\n",
      "  fetch: Fetches a URL from the internet and optionally extracts its contents as markdown\n",
      "\n",
      "filesystem server — 14 tool(s):\n",
      "  read_file: Read the complete contents of a file as text. DEPRECATED: Use read_text_file ins\n",
      "  read_text_file: Read the complete contents of a file from the file system as text. Handles vario\n",
      "  read_media_file: Read an image or audio file. Returns the base64 encoded data and MIME type. Only\n",
      "  read_multiple_files: Read the contents of multiple files simultaneously. This is more efficient than \n",
      "  write_file: Create a new file or completely overwrite an existing file with new content. Use\n",
      "  edit_file: Make line-based edits to a text file. Each edit replaces exact line sequences wi\n",
      "  create_directory: Create a new directory or ensure a directory exists. Can create multiple nested \n",
      "  list_directory: Get a detailed listing of all files and directories in a specified path. Results\n",
      "  list_directory_with_sizes: Get a detailed listing of all files and directories in a specified path, includi\n",
      "  directory_tree: Get a recursive tree view of files and directories as a JSON structure. Each ent\n",
      "  move_file: Move or rename files and directories. Can move files between directories and ren\n",
      "  search_files: Recursively search for files and directories matching a pattern. The patterns sh\n",
      "  get_file_info: Retrieve detailed metadata about a file or directory. Returns comprehensive info\n",
      "  list_allowed_directories: Returns the list of directories that this server is allowed to access. Subdirect\n"
     ]
    }
   ],
   "source": [
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "\n",
    "# Two MCP servers — different capabilities, same protocol\n",
    "fetch_params = StdioServerParameters(command=\"uvx\", args=[\"mcp-server-fetch\"])\n",
    "fs_params = StdioServerParameters(\n",
    "    command=\"npx\", args=[\"-y\", \"@modelcontextprotocol/server-filesystem\", \"/tmp\"],\n",
    ")\n",
    "\n",
    "for label, params in [(\"fetch\", fetch_params), (\"filesystem\", fs_params)]:\n",
    "    async with stdio_client(params) as (read, write):\n",
    "        async with ClientSession(read, write) as session:\n",
    "            await session.initialize()\n",
    "            result = await session.list_tools()\n",
    "            print(f\"\\n{label} server — {len(result.tools)} tool(s):\")\n",
    "            for t in result.tools:\n",
    "                print(f\"  {t.name}: {t.description[:80]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7v9so18x",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Python website provides various resources for users, including:\n",
      "\n",
      "* Download links for the latest version of Python (Python 3.14.2)\n",
      "* Documentation for the standard library, tutorials, and guides\n",
      "* A community-run job board for finding work or hiring staff with Python-related skills\n",
      "* News and updates about the Python project, including announcements and new features\n",
      "\n",
      "Additionally, the website lists various applications and tools that can be used with Python, including:\n",
      "\n",
      "* Web development frameworks such as Django, Pyramid, Bottle, Tornado, Flask, Litestar, and web2py\n",
      "* GUI development libraries like tkInter, PyGObject, PyQt, PySide, Kivy, wxPython, and DearPyGui\n",
      "* Scientific and numeric libraries like SciPy, Pandas, and IPython\n",
      "* Software development tools like Buildbot, Trac, and Roundup\n",
      "* System administration tools like Ansible, Salt, OpenStack, and xonsh\n",
      "\n",
      "Overall, the Python website is a comprehensive resource for users looking to learn more about the language, its applications, and its community.\n"
     ]
    }
   ],
   "source": [
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "\n",
    "from langchain.agents import create_agent\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "async with stdio_client(fetch_params) as (read, write):\n",
    "    async with ClientSession(read, write) as session:\n",
    "        await session.initialize()\n",
    "\n",
    "        # In section 4: tools = [web_search]\n",
    "        # With MCP:     tools come from the server\n",
    "        tools = await load_mcp_tools(session)\n",
    "\n",
    "        # Small models sometimes pass null for optional int params.\n",
    "        # Strip None values so the server's JSON-schema validation won't reject them.\n",
    "        for t in tools:\n",
    "            _orig = t.coroutine\n",
    "            async def _strip_none(_f=_orig, **kw):\n",
    "                return await _f(**{k: v for k, v in kw.items() if v is not None})\n",
    "            t.coroutine = _strip_none\n",
    "\n",
    "        # From here, same as previous section\n",
    "        llm = ChatOllama(model=\"llama3.2:3b\", temperature=0)\n",
    "        agent = create_agent(llm, tools)\n",
    "\n",
    "        # ainvoke (not invoke) because MCP tools are async\n",
    "        out = await agent.ainvoke(\n",
    "            {\"messages\": [(\"user\", \"Fetch https://python.org and summarize the page\")]}\n",
    "        )\n",
    "        print(out[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29vb8wgu964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of https://python.org/:\n",
      "**Notice:** This page displays a fallback because interactive scripts did not run. Possible causes include disabled JavaScript or failure to load scripts or stylesheets.\n",
      "\n",
      "## Download\n",
      "\n",
      "Python source code and installers are available for download for all versions!\n",
      "\n",
      "Latest: [Python 3.14.2](/downloads/release/python-3142/)\n",
      "\n",
      "## Docs\n",
      "\n",
      "Documentation for Python's standard library, along with tutorials and guides, are available online.\n",
      "\n",
      "[docs.python.org](https://docs.python.org/)\n",
      "\n",
      "## Jobs\n",
      "\n",
      "Looking for work or have a Python related position that you're trying to hire for? Our **relaunched community-run job board** is the place to go.\n",
      "\n",
      "[jobs.python.org](//jobs.python.org)\n",
      "\n",
      "## Latest News\n",
      "\n",
      "[More](https://blog.python.org/ \"More News\")\n",
      "\n",
      "* 2026-01-26\n",
      "  [Your Python. Your Voice. Join the Python Developers Survey 2026!](https://pyfound.blogspot.com/2026/01/your-python-your-voice-join-python.html)\n",
      "* 2026-01-21\n",
      "  [Departing the Python Software Foundation (Staff)](https://pyfound.blogspot.com/2026/01/ee-departing-the-psf-staff.html)\n",
      "* 2026-01-20\n",
      "  [Announcing Python Software Foundation Fellow Members for Q4 2025! 🎉](https://pyfound.blogspot.com/2026/01/announcing-python-software-foundation.html)\n",
      "* 2026-01-14\n",
      "  [Python 3.15.0 alpha 5 (yes, another alpha!)](https://pythoninsider.blogspot.com/2026/01/python-3150-alpha-5-yes-another-alpha.html)\n",
      "* 2026-01-13\n",
      "  [Python 3.15.0 alpha 4](https://pythoninsider.blogspot.com/2026/01/python-3150-alpha-4.html)\n",
      "\n",
      "## Use Python for…\n",
      "\n",
      "[More](/about/apps \"More Applications\")\n",
      "\n",
      "* **Web Development**:\n",
      "  [Django](https://www.djangoproject.com/), [Pyramid](https://trypyramid.com/), [Bottle](https://bottlepy.org/), [Tornado](https://www.tornadoweb.org/), [Flask](https://flask.palletsprojects.com/), [Litestar](https://litestar.dev/), [web2py](https://www.web2py.com/)\n",
      "* **GUI Development**:\n",
      "  [tkInter](https://wiki.python.org/moin/TkInter), [PyGObject](https://wiki.gnome.org/Projects/PyGObject), [PyQt](https://riverbankcomputing.com/software/pyqt/intro), [PySide](https://wiki.qt.io/Qt_for_Python), [Kivy](https://kivy.org/), [wxPython](https://www.wxpython.org/), [DearPyGui](https://dearpygui.readthedocs.io/en/latest/)\n",
      "* **Scientific and Numeric**:\n",
      "  [SciPy](https://scipy.org/), [Pandas](https://pandas.pydata.org/), [IPython](https://ipython.org/)\n",
      "* **Software Development**:\n",
      "  [Buildbot](https://buildbot.net/), [Trac](https://trac.edgewall.org/), [Roundup](https://www.roundup-tracker.org/)\n",
      "* **System Administration**:\n",
      "  [Ansible](https://docs.ansible.com/), [Salt](https://saltproject.io/), [OpenStack](https://www.openstack.org/), [xonsh](https://xon.sh/)\n"
     ]
    }
   ],
   "source": [
    "# You can also call MCP tools directly, without LangChain\n",
    "async with stdio_client(fetch_params) as (read, write):\n",
    "    async with ClientSession(read, write) as session:\n",
    "        await session.initialize()\n",
    "\n",
    "        result = await session.call_tool(\"fetch\", arguments={\"url\": \"https://python.org\"})\n",
    "        for block in result.content:\n",
    "            if hasattr(block, \"text\"):\n",
    "                print(block.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6647eac",
   "metadata": {},
   "source": [
    "## 6- (Optional) A Minimal UI\n",
    "\n",
    "[Chainlit](https://chainlit.io/) is a Python library designed specifically for building LLM and agent UIs. It provides:\n",
    "- Built-in streaming support\n",
    "- Message history\n",
    "- Step visualization (see tool calls as they happen)\n",
    "- No frontend code required\n",
    "\n",
    "If you are interested, follow Chainlit's documentation to implement a simple UI for your agent. The process typically involves:\n",
    "\n",
    "1. You write a Python file named `chainlit_app.py` with the agent creation logic as well as UI handlers (e.g.,`@cl.on_message`)\n",
    "2. Run the file in your terminal with `chainlit run app.py`\n",
    "3. A web UI opens automatically at `http://localhost:8000`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "hi1y4z7r2y",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting chainlit_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile chainlit_app.py\n",
    "# ---------------------------------------------------------\n",
    "# Chainlit Web Search Agent\n",
    "\n",
    "import chainlit as cl\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import AIMessage, ToolMessage\n",
    "from ddgs import DDGS\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Define the web search tool\n",
    "# ---------------------------------------------------------\n",
    "@tool\n",
    "def web_search(query: str) -> str:\n",
    "    \"\"\"Search the web for a query and return results.\"\"\"\n",
    "    results = []\n",
    "    with DDGS() as ddgs:\n",
    "        for r in ddgs.text(query, max_results=5):\n",
    "            results.append(f\"- {r['title']}: {r['href']}\")\n",
    "    return \"\\n\".join(results) if results else \"No results found.\"\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Create the agent (once at startup)\n",
    "# ---------------------------------------------------------\n",
    "MODEL = \"llama3.2:3b\"\n",
    "llm = ChatOllama(model=MODEL, temperature=0)\n",
    "agent = create_react_agent(llm, [web_search])\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Chainlit message handler\n",
    "# ---------------------------------------------------------\n",
    "@cl.on_message\n",
    "async def handle_message(message: cl.Message):\n",
    "    \"\"\"Handle user messages and stream agent responses.\"\"\"\n",
    "    \n",
    "    final_answer = \"\"\n",
    "    \n",
    "    # Stream through agent steps\n",
    "    for step in agent.stream({\"messages\": [(\"user\", message.content)]}):\n",
    "        for node_name, node_output in step.items():\n",
    "            if \"messages\" not in node_output:\n",
    "                continue\n",
    "                \n",
    "            for msg in node_output[\"messages\"]:\n",
    "                # AIMessage: either a tool call or the final answer\n",
    "                if isinstance(msg, AIMessage):\n",
    "                    if msg.tool_calls:\n",
    "                        # Agent decided to call a tool\n",
    "                        for tc in msg.tool_calls:\n",
    "                            async with cl.Step(name=f\"🔍 {tc['name']}\") as step:\n",
    "                                step.output = f\"Query: {tc['args'].get('query', tc['args'])}\"\n",
    "                    elif msg.content:\n",
    "                        # Final answer from the agent\n",
    "                        final_answer = msg.content\n",
    "                \n",
    "                # ToolMessage: results from a tool call\n",
    "                elif isinstance(msg, ToolMessage):\n",
    "                    async with cl.Step(name=\"📄 Search Results\") as step:\n",
    "                        content = msg.content\n",
    "                        step.output = content[:1000] + \"...\" if len(content) > 1000 else content\n",
    "    \n",
    "    # Send the final answer\n",
    "    if final_answer:\n",
    "        await cl.Message(content=final_answer).send()\n",
    "    else:\n",
    "        await cl.Message(content=\"I couldn't generate a response. Please try again.\").send()\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Welcome message\n",
    "# ---------------------------------------------------------\n",
    "@cl.on_chat_start\n",
    "async def start():\n",
    "    await cl.Message(\n",
    "        content=\"👋 Welcome! I'm an Ask-the-Web agent. Ask me anything and I'll search the web to find answers.\\n\\n\"\n",
    "                \"Try: **What are the current events in San Francisco this week?**\"\n",
    "    ).send()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a116bbee",
   "metadata": {},
   "source": [
    "## 🎉 Congratulations!\n",
    "\n",
    "You have built a **web-enabled agent** from scratch: manual tool calling → JSON schemas → LangChain ReAct → web search → MCP → UI.\n",
    "\n",
    "Next steps:\n",
    "* Try adding more tools, such as news or finance APIs.\n",
    "* Experiment with multiple tools, different models, and measure accuracy vs. hallucination.\n",
    "* Explore the [MCP server registry](https://github.com/modelcontextprotocol/servers) for ready-made tool servers.\n",
    "\n",
    "👏 **Great job!** Take a moment to celebrate. The techniques you implemented here power many production agents and chatbots."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-web-agent-uv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
