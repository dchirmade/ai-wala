{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f7fe8ac",
   "metadata": {},
   "source": [
    "# Project 3: **Ask‑the‑Web Agent**\n",
    "\n",
    "Welcome to Project 3! In this project, you will learn how to use tool‑calling LLMs, extend them with custom tools, and build a simplified *Perplexity‑style* agent that answers questions by searching the web."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4311a6",
   "metadata": {},
   "source": [
    "## Learning Objectives  \n",
    "* Understand why tool calling is useful and how LLMs can invoke external tools.\n",
    "* Implement a minimal loop that parses the LLM's output and executes a Python function.\n",
    "* See how *function schemas* (docstrings and type hints) let us scale to many tools.\n",
    "* Use **LangChain** to get function‑calling capability\n",
    "* Combine LLM with a web‑search tool to build a simple ask‑the‑web agent.\n",
    "* Connect to external tools using **MCP (Model Context Protocol)**, a universal standard for LLM‑tool integration.\n",
    "* Optionally build a UI using Chainlit to test your agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f16864",
   "metadata": {},
   "source": [
    "## Roadmap\n",
    "0. Environment setup\n",
    "1. Write simple tools and connect them to an LLM\n",
    "2. Standardize tool calling with JSON schemas\n",
    "3. Use LangGraph for tool calling\n",
    "4. Build a Perplexity-style web-search agent\n",
    "5. (Optional) MCP: connect to external tool servers\n",
    "6. (Optional) A minimal UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeae51d0",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "# 0- Environment setup\n",
    "\n",
    "### Step 1: Create your environment and install dependencies \n",
    "Before we start coding, you need a reproducible setup. Open a terminal in the same directory as this notebook, and use Conda or uv to install the project dependencies.\n",
    "\n",
    "#### Option 1: Conda\n",
    "\n",
    "\n",
    "```bash\n",
    "# Create and activate the conda environment\n",
    "conda env create -f environment.yml && conda activate web_agent\n",
    "\n",
    "```\n",
    "\n",
    "#### Option 2: UV (faster)\n",
    "\n",
    "If you prefer [uv](https://docs.astral.sh/uv/) over Conda:\n",
    "\n",
    "```bash\n",
    "# Install uv (skip if already installed)\n",
    "curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "\n",
    "# Create venv and install dependencies\n",
    "uv venv .venv-web-agent-uv && source .venv-web-agent-uv/bin/activate\n",
    "uv pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "### Step 2: Register this environment as a Jupyter kernel\n",
    "```bash\n",
    "python -m ipykernel install --user --name=web_agent --display-name \"web_agent\"\n",
    "```\n",
    "Now open your notebook and switch to the `web_agent` kernel (Kernel → Change Kernel).\n",
    "\n",
    "### Step 3: Set up Ollama\n",
    "\n",
    "In this project, we use **Ollama** to load and use open-weight LLMs. We start with smaller models like `gemma3:1b` and then switch to larger models like `llama3.2:3b`.\n",
    "\n",
    "Start the **Ollama** server in a terminal. This launches a local API endpoint that listens for LLM requests.\n",
    "\n",
    "```bash\n",
    "ollama serve\n",
    "```\n",
    "\n",
    "Downloads the model so you can run them locally without API calls. \n",
    "```bash\n",
    "ollama pull gemma3:1b\n",
    "ollama pull llama3.2:3b\n",
    "```\n",
    "\n",
    "You can explore other available models [here](https://ollama.com/library) and pull them to experiment with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bejkbbzrdy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is running. Installed models: ['x/flux2-klein:latest', 'qwen3:latest', 'gpt-oss:latest', 'gpt-oss:20b', 'MedAIBase/MedGemma1.5:4b', 'MedAIBase/MedGemma1.0:4b', 'gemma3:latest', 'gpt-oss:120b-cloud', 'deepseek-r1:latest', 'devstral:latest', 'llama4:latest', 'llava:latest', 'qwen2.5vl:3b', 'codellama:latest', 'qwen2.5vl:7b', 'nomic-embed-text:latest', 'qwen2.5-coder:1.5b-base', 'phi4-reasoning:latest', 'deepcoder:latest']\n"
     ]
    }
   ],
   "source": [
    "# Quick check: is Ollama running?\n",
    "# If this fails, open a terminal and run: ollama serve\n",
    "\n",
    "import httpx\n",
    "\n",
    "response = httpx.get(\"http://localhost:11434/api/tags\", timeout=5)\n",
    "models = [m[\"name\"] for m in response.json().get(\"models\", [])]\n",
    "print(f\"Ollama is running. Installed models: {models}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c27158f",
   "metadata": {},
   "source": [
    "## 1- Tool Calling\n",
    "\n",
    "LLMs are strong at answering questions, but they cannot directly access external data such as live web results, APIs, or computations. In real applications, agents rarely rely only on their internal knowledge. They need to query APIs, retrieve data, or perform calculations to stay accurate and useful. Tool calling bridges this gap by allowing the LLM to request actions from the outside world.\n",
    "\n",
    "<img src=\"assets/tools.png\" width=\"700\">\n",
    "\n",
    "As show below, We first implement a tool, then describe the tool as part of the model's prompt. When the model decides that a tool is needed, it emits a structured output. A parser will detect this output, execute the corresponding function, and feed the result back to the LLM so the conversation continues.\n",
    "\n",
    "<img src=\"assets/tool_flow.png\" width=\"700\">\n",
    "\n",
    "In this section, you will implement a simple `get_current_weather` function and teach the `gemma3:1b` model to use it when required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a536f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing get_current_weather:\n",
      "It is 23°C and sunny in San Francisco.\n",
      "It is 23°C and sunny in Paris.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 1: Implement the tool\n",
    "# ---------------------------------------------------------\n",
    "# CONCEPT: What is a Tool?\n",
    "# A tool is a Python function that an LLM can call to perform actions\n",
    "# it cannot do on its own (e.g., fetch live data, run calculations).\n",
    "# The LLM outputs a request, we parse it, execute the function, and return results.\n",
    "\n",
    "def get_current_weather(city: str) -> str:\n",
    "    \"\"\"\n",
    "    Get the current weather for a given city.\n",
    "    \n",
    "    This is a DUMMY implementation for learning purposes.\n",
    "    In production, you would call a real weather API like OpenWeatherMap.\n",
    "    \n",
    "    Args:\n",
    "        city: The name of the city to get weather for\n",
    "        \n",
    "    Returns:\n",
    "        A human-readable string describing the weather\n",
    "    \"\"\"\n",
    "    # Return dummy data for learning\n",
    "    return f\"It is 23°C and sunny in {city}.\"\n",
    "\n",
    "# Test the function\n",
    "print(\"Testing get_current_weather:\")\n",
    "print(get_current_weather(\"San Francisco\"))\n",
    "print(get_current_weather(\"Paris\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a43c3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Prompt:\n",
      "You are a helpful assistant with access to tools.\n",
      "\n",
      "When you need to use a tool, output exactly this format:\n",
      "TOOL_CALL: {\"name\": \"tool_name\", \"args\": {\"arg1\": \"value1\", \"arg2\": \"value2\"}}\n",
      "\n",
      "Available tools:\n",
      "- get_current_weather(city: str) -> str\n",
      "  Description: Gets the current weather for a specified city\n",
      "  When to use: When the user asks about weather conditions\n",
      "\n",
      "Example:\n",
      "User: \"What's the weather in Tokyo?\"\n",
      "Assistant: TOOL_CALL: {\"name\": \"get_current_weather\", \"args\": {\"city\": \"Tokyo\"}}\n",
      "\n",
      "After receiving the tool result, provide a natural response to the user.\n",
      "\n",
      "\n",
      "User Question:\n",
      "What is the weather in San Diego today?\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Step 2: Create a prompt to teach the LLM when and how to use your tool\n",
    "# ----------------------------------------------------------------------\n",
    "# CONCEPT: Prompt Engineering for Tool Calling\n",
    "# Since gemma3:latest does not have native tool calling, we teach it through\n",
    "# the prompt. We define a clear format for tool calls that we can parse.\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a helpful assistant with access to tools.\n",
    "\n",
    "When you need to use a tool, output exactly this format:\n",
    "TOOL_CALL: {\"name\": \"tool_name\", \"args\": {\"arg1\": \"value1\", \"arg2\": \"value2\"}}\n",
    "\n",
    "Available tools:\n",
    "- get_current_weather(city: str) -> str\n",
    "  Description: Gets the current weather for a specified city\n",
    "  When to use: When the user asks about weather conditions\n",
    "  \n",
    "Example:\n",
    "User: \"What's the weather in Tokyo?\"\n",
    "Assistant: TOOL_CALL: {\"name\": \"get_current_weather\", \"args\": {\"city\": \"Tokyo\"}}\n",
    "\n",
    "After receiving the tool result, provide a natural response to the user.\n",
    "\"\"\"\n",
    "\n",
    "# Example user question that should trigger the weather tool\n",
    "USER_QUESTION = \"What is the weather in San Diego today?\"\n",
    "\n",
    "print(\"System Prompt:\")\n",
    "print(SYSTEM_PROMPT)\n",
    "print(\"\\nUser Question:\")\n",
    "print(USER_QUESTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eebb062",
   "metadata": {},
   "source": [
    "Now that you have defined a tool and shown the model how to use it, the next step is to call the LLM using your prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "027cb75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Output:\n",
      "TOOL_CALL: {\"name\": \"get_current_weather\", \"args\": {\"city\": \"San Diego\"}}\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Expected: The model should output something like:\n",
      "TOOL_CALL: {\"name\": \"get_current_weather\", \"args\": {\"city\": \"San Diego\"}}\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 3: Call the LLM with your prompt\n",
    "# ---------------------------------------------------------\n",
    "# CONCEPT: Using Ollama with OpenAI-Compatible API\n",
    "# Ollama provides a local LLM server that mimics the OpenAI API.\n",
    "# We can use the OpenAI Python client to interact with local models.\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "# Create an Ollama client\n",
    "# base_url points to local Ollama server\n",
    "# api_key is not needed for local Ollama (we use placeholder)\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"ollama\"\n",
    ")\n",
    "\n",
    "# Send the prompt to the model\n",
    "# We use gemma3:latest - a small, fast 1B parameter model\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gemma3:latest\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},  # Tool instructions\n",
    "        {\"role\": \"user\", \"content\": USER_QUESTION}      # User question\n",
    "    ],\n",
    "    temperature=0.1  # Low temperature = more deterministic outputs\n",
    ")\n",
    "\n",
    "# Extract the model's response\n",
    "model_output = response.choices[0].message.content\n",
    "\n",
    "print(\"Model Output:\")\n",
    "print(model_output)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Expected: The model should output something like:\")\n",
    "print('TOOL_CALL: {\"name\": \"get_current_weather\", \"args\": {\"city\": \"San Diego\"}}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94aeb4d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting ',' delimiter: line 1 column 62 (char 61)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m tool_call_json = match.group(\u001b[32m1\u001b[39m)\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Parse the JSON to get a Python dictionary\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m tool_call = \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool_call_json\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Extract function name and arguments\u001b[39;00m\n\u001b[32m     25\u001b[39m function_name = tool_call[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dipak-workspace/anaconda3/envs/web_agent/lib/python3.11/json/__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    341\u001b[39m     s = s.decode(detect_encoding(s), \u001b[33m'\u001b[39m\u001b[33msurrogatepass\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    348\u001b[39m     \u001b[38;5;28mcls\u001b[39m = JSONDecoder\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dipak-workspace/anaconda3/envs/web_agent/lib/python3.11/json/decoder.py:337\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w=WHITESPACE.match):\n\u001b[32m    333\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[32m    334\u001b[39m \u001b[33;03m    containing a JSON document).\u001b[39;00m\n\u001b[32m    335\u001b[39m \n\u001b[32m    336\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m     obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m     end = _w(s, end).end()\n\u001b[32m    339\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m end != \u001b[38;5;28mlen\u001b[39m(s):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dipak-workspace/anaconda3/envs/web_agent/lib/python3.11/json/decoder.py:353\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n\u001b[32m    344\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[33;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[33;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    350\u001b[39m \n\u001b[32m    351\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m     obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    355\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting ',' delimiter: line 1 column 62 (char 61)"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 4: Manually parse the LLM output and call the tool\n",
    "# ---------------------------------------------------------\n",
    "# CONCEPT: Parsing Structured Output from LLMs\n",
    "# The model generates text, so we need to:\n",
    "#   1. Detect if a tool call was requested\n",
    "#   2. Extract the JSON data\n",
    "#   3. Parse it and call the corresponding function\n",
    "\n",
    "import re\n",
    "import json\n",
    "\n",
    "# Search for the TOOL_CALL pattern in the model's output\n",
    "# Pattern: TOOL_CALL: {...}\n",
    "match = re.search(r'TOOL_CALL:\\s*(\\{.*?\\})', model_output)\n",
    "\n",
    "if match:\n",
    "    # Extract the JSON string\n",
    "    tool_call_json = match.group(1)\n",
    "    \n",
    "    # Parse the JSON to get a Python dictionary\n",
    "    tool_call = json.loads(tool_call_json)\n",
    "    \n",
    "    # Extract function name and arguments\n",
    "    function_name = tool_call[\"name\"]\n",
    "    function_args = tool_call[\"args\"]\n",
    "    \n",
    "    print(f\"✓ Tool call detected!\")\n",
    "    print(f\"  Function: {function_name}\")\n",
    "    print(f\"  Arguments: {function_args}\")\n",
    "    print()\n",
    "    \n",
    "    # Execute the tool\n",
    "    if function_name == \"get_current_weather\":\n",
    "        # **function_args unpacks the dictionary into keyword arguments\n",
    "        result = get_current_weather(**function_args)\n",
    "        print(f\"Tool Result: {result}\")\n",
    "    else:\n",
    "        print(f\"Error: Unknown tool '{function_name}'\")\n",
    "else:\n",
    "    print(\"No tool call detected in the model output.\")\n",
    "    print(\"The model responded directly:\", model_output)\n",
    "\n",
    "# LEARNING NOTE: Why Manual Parsing Doesn't Scale\n",
    "# - Adding new tools requires updating if/else chains\n",
    "# - Must manually maintain tool descriptions in prompts\n",
    "# - No automatic validation of arguments\n",
    "# Next section solves this with JSON schemas!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be26661",
   "metadata": {},
   "source": [
    "# 2- Standardize tool calling\n",
    "\n",
    "So far, we handled tool calling manually by writing a function, manually teaching the LLM about it, and write a regex to parse the output. This approach does not scale if we want to add more tools. Adding more tools would mean more `if/else` blocks and manual edits to the prompt.\n",
    "\n",
    "To make the system flexible, we can standardize tool definitions by automatically reading each function's signature, converting it to a JSON schema, and passing that schema to the LLM. This way, the LLM can dynamically understand which tools exist and how to call them without requiring manual updates to prompts or conditional logic.\n",
    "\n",
    "Next, you will implement a small helper that extracts metadata from functions and builds a schema for each tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce911b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Tool Schema:\n",
      "======================================================================\n",
      "{'description': 'Get the current weather for a specified city.',\n",
      " 'name': 'get_current_weather',\n",
      " 'parameters': {'properties': {'city': {'description': 'The name of the city '\n",
      "                                                       'to check weather for',\n",
      "                                        'type': 'string'},\n",
      "                               'unit': {'description': 'Temperature unit, '\n",
      "                                                       \"either 'celsius' or \"\n",
      "                                                       \"'fahrenheit' (default: \"\n",
      "                                                       'celsius)',\n",
      "                                        'type': 'string'}},\n",
      "                'required': ['city'],\n",
      "                'type': 'object'}}\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Generate a JSON schema for a tool automatically\n",
    "# ---------------------------------------------------------\n",
    "# CONCEPT: Automatic Schema Generation\n",
    "# Instead of manually writing tool descriptions, we can use Python's\n",
    "# inspect module to automatically extract function metadata and build schemas.\n",
    "\n",
    "from pprint import pprint\n",
    "import inspect\n",
    "\n",
    "def get_current_weather(city: str, unit: str = \"celsius\") -> str:\n",
    "    \"\"\"\n",
    "    Get the current weather for a specified city.\n",
    "    \n",
    "    Args:\n",
    "        city: The name of the city to check weather for\n",
    "        unit: Temperature unit, either 'celsius' or 'fahrenheit' (default: celsius)\n",
    "        \n",
    "    Returns:\n",
    "        A string describing the current weather conditions\n",
    "    \"\"\"\n",
    "    # Dummy implementation\n",
    "    temp = 23 if unit == \"celsius\" else 73\n",
    "    return f\"It is {temp}°{unit[0].upper()} and sunny in {city}.\"\n",
    "\n",
    "def to_schema(fn):\n",
    "    \"\"\"\n",
    "    Convert a Python function into a JSON schema for tool calling.\n",
    "    \n",
    "    This uses introspection to extract:\n",
    "      - Function name\n",
    "      - Description from docstring\n",
    "      - Parameter names, types, and descriptions\n",
    "      - Required vs optional parameters\n",
    "    \"\"\"\n",
    "    # Get the function signature (parameters, types, defaults)\n",
    "    sig = inspect.signature(fn)\n",
    "    \n",
    "    # Extract the docstring\n",
    "    doc = inspect.getdoc(fn) or \"\"\n",
    "    \n",
    "    # Build the schema structure\n",
    "    schema = {\n",
    "        \"name\": fn.__name__,\n",
    "        \"description\": doc.split(\"Args:\")[0].strip(),\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {},\n",
    "            \"required\": []\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Parse each parameter\n",
    "    for param_name, param in sig.parameters.items():\n",
    "        # Map Python types to JSON schema types\n",
    "        param_type = \"string\"  # Default\n",
    "        if param.annotation != inspect.Parameter.empty:\n",
    "            type_map = {str: \"string\", int: \"integer\", float: \"number\", bool: \"boolean\"}\n",
    "            param_type = type_map.get(param.annotation, \"string\")\n",
    "        \n",
    "        # Extract parameter description from docstring\n",
    "        param_desc = \"\"\n",
    "        for line in doc.split(\"\\n\"):\n",
    "            if line.strip().startswith(f\"{param_name}:\"):\n",
    "                param_desc = line.split(\":\", 1)[1].strip()\n",
    "                break\n",
    "        \n",
    "        # Add to schema\n",
    "        schema[\"parameters\"][\"properties\"][param_name] = {\n",
    "            \"type\": param_type,\n",
    "            \"description\": param_desc\n",
    "        }\n",
    "        \n",
    "        # Mark as required if no default value\n",
    "        if param.default == inspect.Parameter.empty:\n",
    "            schema[\"parameters\"][\"required\"].append(param_name)\n",
    "    \n",
    "    return schema\n",
    "\n",
    "# Test the schema generator\n",
    "tool_schema = to_schema(get_current_weather)\n",
    "print(\"Generated Tool Schema:\")\n",
    "print(\"=\"*70)\n",
    "pprint(tool_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1163f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Output:\n",
      "TOOL_CALL: {\"name\": \"get_current_weather\", \"args\": {\"city\": \"London\", \"unit\": \"fahrenheit\"}}\n",
      "Okay, I've checked the weather in London for you. The current temperature is 59°F. Would you like to know anything else about the weather there?\n",
      "\n",
      "======================================================================\n",
      "Notice: The model should correctly identify both parameters:\n",
      "  - city: London\n",
      "  - unit: fahrenheit\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Provide the tool schema to the model\n",
    "# ---------------------------------------------------------\n",
    "# CONCEPT: Schema-Based Tool Calling\n",
    "# Instead of embedding tool descriptions in the system prompt,\n",
    "# we send schemas as a separate message. This scales to many tools.\n",
    "\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "# Create Ollama client\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"ollama\"\n",
    ")\n",
    "\n",
    "# Generate schema for our tool\n",
    "tool_schema = to_schema(get_current_weather)\n",
    "\n",
    "# Create messages with tool schema\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"You are a helpful assistant with access to tools.\n",
    "\n",
    "When you need to use a tool, output exactly this format:\n",
    "TOOL_CALL: {\"name\": \"tool_name\", \"args\": {\"arg1\": \"value1\"}}\n",
    "\n",
    "After receiving the tool result, provide a natural response to the user.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"name\": \"tool_spec\",\n",
    "        \"content\": f\"Available tools:\\n{json.dumps([tool_schema], indent=2)}\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What's the weather in London in fahrenheit?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Call the model\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gemma3:latest\",\n",
    "    messages=messages,\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "model_output = response.choices[0].message.content\n",
    "print(\"Model Output:\")\n",
    "print(model_output)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Notice: The model should correctly identify both parameters:\")\n",
    "print(\"  - city: London\")\n",
    "print(\"  - unit: fahrenheit\")\n",
    "\n",
    "# LEARNING NOTE: Scalability\n",
    "# With this approach, adding a new tool is simple:\n",
    "#   1. Write the function with docstring and type hints\n",
    "#   2. Add it to the tools list: [to_schema(tool1), to_schema(tool2), ...]\n",
    "#   3. No prompt editing needed!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8ec86e",
   "metadata": {},
   "source": [
    "## 3- LangChain for Tool Calling\n",
    "\n",
    "So far, you built a simple tool-calling pipeline. While this helps you understand the logic, it does not scale well when working with multiple tools, complex parsing, or multi-step reasoning. We have to write manual parsers, function calling logic, and adding responses back to the prompt.\n",
    "\n",
    "LangChain simplifies this process. You only need to declare your tools, and its *Agent* abstraction handles when to call a tool, how to use it, and how to continue reasoning afterward. In this section, you will create a **ReAct** Agent (Reasoning + Acting). As shown below, the model alternates between reasoning steps and tool use wihtout any manual work.\n",
    "\n",
    "<img src=\"assets/react.png\" width=\"500\">\n",
    "\n",
    "The following links might be helpful for completing this section:\n",
    "- [Create Agents](https://docs.langchain.com/oss/python/langchain/agents)\n",
    "- [LangChain Tools](https://docs.langchain.com/oss/python/langchain/tools)\n",
    "- [Ollama](https://docs.langchain.com/oss/python/integrations/chat/ollama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c609d97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool created: get_weather\n",
      "Tool description: Get the current weather for a specified city.\n",
      "\n",
      "    Args:\n",
      "        city: The name of the city to check weather for\n",
      "        unit: Temperature unit, either 'celsius' or 'fahrenheit'\n",
      "Tool schema: {'city': {'title': 'City', 'type': 'string'}, 'unit': {'default': 'celsius', 'title': 'Unit', 'type': 'string'}}\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 1: Define tools for LangChain\n",
    "# ---------------------------------------------------------\n",
    "# CONCEPT: LangChain's @tool Decorator\n",
    "# LangChain provides a @tool decorator that automatically:\n",
    "#   - Converts your function into a Tool object\n",
    "#   - Extracts the schema from docstring and type hints\n",
    "#   - Handles argument validation\n",
    "#   - Integrates with LangChain's agent framework\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def get_weather(city: str, unit: str = \"celsius\") -> str:\n",
    "    \"\"\"Get the current weather for a specified city.\n",
    "    \n",
    "    Args:\n",
    "        city: The name of the city to check weather for\n",
    "        unit: Temperature unit, either 'celsius' or 'fahrenheit'\n",
    "    \"\"\"\n",
    "    # Dummy implementation\n",
    "    temp = 23 if unit == \"celsius\" else 73\n",
    "    return f\"It is {temp}°{unit[0].upper()} and sunny in {city}.\"\n",
    "\n",
    "# The @tool decorator converts this into a LangChain Tool object\n",
    "print(\"Tool created:\", get_weather.name)\n",
    "print(\"Tool description:\", get_weather.description)\n",
    "print(\"Tool schema:\", get_weather.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "daa159c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28538/1383335945.py:18: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
      "  agent = create_react_agent(llm, [get_weather])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error occurred (this is expected):\n",
      "   ResponseError: registry.ollama.ai/library/gemma3:latest does not support tools (status code: 400)\n",
      "\n",
      "Why it failed:\n",
      "  - gemma3:latest doesn't support native tool calling\n",
      "  - LangChain expects structured tool call outputs\n",
      "  - The model returns plain text instead\n",
      "\n",
      "Solution: Use a larger model with native tool support (3B+)\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 2: Create the Agent (with small model - will fail)\n",
    "# ---------------------------------------------------------\n",
    "# CONCEPT: Why This Fails with Small Models\n",
    "# gemma3:latest is a 1B parameter model. While fast, it was NOT trained\n",
    "# for native function calling. It can't output structured JSON reliably.\n",
    "#\n",
    "# Models under 3B parameters typically lack this capability.\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# Create a small LLM (1B parameters)\n",
    "llm = ChatOllama(model=\"gemma3:latest\", temperature=0)\n",
    "\n",
    "# Try to create an agent with the weather tool\n",
    "try:\n",
    "    agent = create_react_agent(llm, [get_weather])\n",
    "    \n",
    "    # Test the agent\n",
    "    result = agent.invoke({\"messages\": [(\"user\", \"What's the weather in Paris?\")]})\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(\"❌ Error occurred (this is expected):\")\n",
    "    print(f\"   {type(e).__name__}: {str(e)}\")\n",
    "    print(\"\\nWhy it failed:\")\n",
    "    print(\"  - gemma3:latest doesn't support native tool calling\")\n",
    "    print(\"  - LangChain expects structured tool call outputs\")\n",
    "    print(\"  - The model returns plain text instead\")\n",
    "    print(\"\\nSolution: Use a larger model with native tool support (3B+)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5824437",
   "metadata": {},
   "source": [
    "### What just happened?\n",
    "Your run failed because `gemma3:1b` does not support native tool calling (function calling). LangChain expects the model to return a structured tool-call object, but `gemma3:1b` can only return plain text, so the tool invocation step breaks.\n",
    "\n",
    "### Why previosuly, our manual approach worked with any model?\n",
    "\n",
    "In previous sections, we used **text-based tool calling**. We described the tool format in the system prompt. We asked the model to output `TOOL_CALL: {\"name\": ..., \"args\": ...}`. We then parsed this text with regex.\n",
    "\n",
    "This works with **any model** (even small ones like `gemma3:1b`) because we're just asking the model to follow a certain structured output format.\n",
    "\n",
    "### Why LangChain requires specific models?\n",
    "\n",
    "LangChain relies on **native tool calling** and it expects a consistent structured output format irrespective of the model. Hence, it enfornces model outputs structured tool calls in a specific format. This requires models trained specifically for function calling\n",
    "\n",
    "**Rule of thumb**: Models under 3B parameters typically lack native tool-calling capability.\n",
    "\n",
    "| Model | Size | Native Tool Support | Notes |\n",
    "|-------|------|---------------------|-------|\n",
    "| `gemma3:1b` | 1B | No | Works for manual approach only |\n",
    "| `llama3.2:1b` | 1B | No | Works for manual approach only |\n",
    "| `llama3.2:3b` | 3B | Yes | Good balance of speed and capability |\n",
    "| `gemma3` | 4B | Yes | Supports native tools |\n",
    "| `mistral` | 7B | Yes | Strong tool support |\n",
    "\n",
    "Let's fix the issue we observed in the previous cell.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9552348d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the agent...\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "ResponseError",
     "evalue": "registry.ollama.ai/library/gemma3n:latest does not support tools (status code: 400)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResponseError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 56\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTesting the agent...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     54\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m70\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m result = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43ms the weather in Tokyo?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# Display the conversation\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAgent Conversation:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dipak-workspace/anaconda3/envs/web_agent/lib/python3.11/site-packages/langgraph/pregel/main.py:3071\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3068\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3069\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3071\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3072\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3073\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3074\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3076\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3077\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3078\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3079\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3080\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3081\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3082\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3083\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3084\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3085\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3086\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dipak-workspace/anaconda3/envs/web_agent/lib/python3.11/site-packages/langgraph/pregel/main.py:2646\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2644\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2645\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2646\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2647\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2648\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2649\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2650\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2651\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2652\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2653\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2655\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2656\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dipak-workspace/anaconda3/envs/web_agent/lib/python3.11/site-packages/langgraph/pregel/_runner.py:167\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    165\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dipak-workspace/anaconda3/envs/web_agent/lib/python3.11/site-packages/langgraph/pregel/_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dipak-workspace/anaconda3/envs/web_agent/lib/python3.11/site-packages/langgraph/_internal/_runnable.py:656\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    658\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dipak-workspace/anaconda3/envs/web_agent/lib/python3.11/site-packages/langgraph/_internal/_runnable.py:400\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    398\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dipak-workspace/anaconda3/envs/web_agent/lib/python3.11/site-packages/langchain/agents/factory.py:1166\u001b[39m, in \u001b[36mcreate_agent.<locals>.model_node\u001b[39m\u001b[34m(state, runtime)\u001b[39m\n\u001b[32m   1153\u001b[39m request = ModelRequest(\n\u001b[32m   1154\u001b[39m     model=model,\n\u001b[32m   1155\u001b[39m     tools=default_tools,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1161\u001b[39m     runtime=runtime,\n\u001b[32m   1162\u001b[39m )\n\u001b[32m   1164\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m wrap_model_call_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1165\u001b[39m     \u001b[38;5;66;03m# No handlers - execute directly\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1166\u001b[39m     response = \u001b[43m_execute_model_sync\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1168\u001b[39m     \u001b[38;5;66;03m# Call composed handler with base handler\u001b[39;00m\n\u001b[32m   1169\u001b[39m     response = wrap_model_call_handler(request, _execute_model_sync)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dipak-workspace/anaconda3/envs/web_agent/lib/python3.11/site-packages/langchain/agents/factory.py:1137\u001b[39m, in \u001b[36mcreate_agent.<locals>._execute_model_sync\u001b[39m\u001b[34m(request)\u001b[39m\n\u001b[32m   1134\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m request.system_message:\n\u001b[32m   1135\u001b[39m     messages = [request.system_message, *messages]\n\u001b[32m-> \u001b[39m\u001b[32m1137\u001b[39m output = \u001b[43mmodel_\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1138\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name:\n\u001b[32m   1139\u001b[39m     output.name = name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dipak-workspace/anaconda3/envs/web_agent/lib/python3.11/site-packages/langchain_core/runnables/base.py:5557\u001b[39m, in \u001b[36mRunnableBindingBase.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5550\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5551\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m   5552\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5555\u001b[39m     **kwargs: Any | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   5556\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5557\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbound\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5558\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5559\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5560\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5561\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dipak-workspace/anaconda3/envs/web_agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:402\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    390\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    395\u001b[39m     **kwargs: Any,\n\u001b[32m    396\u001b[39m ) -> AIMessage:\n\u001b[32m    397\u001b[39m     config = ensure_config(config)\n\u001b[32m    398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    399\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    400\u001b[39m         cast(\n\u001b[32m    401\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    412\u001b[39m         ).message,\n\u001b[32m    413\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dipak-workspace/anaconda3/envs/web_agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1121\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1112\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1114\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1118\u001b[39m     **kwargs: Any,\n\u001b[32m   1119\u001b[39m ) -> LLMResult:\n\u001b[32m   1120\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dipak-workspace/anaconda3/envs/web_agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:931\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    930\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m931\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    937\u001b[39m         )\n\u001b[32m    938\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    939\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dipak-workspace/anaconda3/envs/web_agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1233\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1231\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1232\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1233\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1237\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dipak-workspace/anaconda3/envs/web_agent/lib/python3.11/site-packages/langchain_ollama/chat_models.py:1030\u001b[39m, in \u001b[36mChatOllama._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1023\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate\u001b[39m(\n\u001b[32m   1024\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1025\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1028\u001b[39m     **kwargs: Any,\n\u001b[32m   1029\u001b[39m ) -> ChatResult:\n\u001b[32m-> \u001b[39m\u001b[32m1030\u001b[39m     final_chunk = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_chat_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1031\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1032\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1033\u001b[39m     generation_info = final_chunk.generation_info\n\u001b[32m   1034\u001b[39m     chat_generation = ChatGeneration(\n\u001b[32m   1035\u001b[39m         message=AIMessage(\n\u001b[32m   1036\u001b[39m             content=final_chunk.text,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1043\u001b[39m         generation_info=generation_info,\n\u001b[32m   1044\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dipak-workspace/anaconda3/envs/web_agent/lib/python3.11/site-packages/langchain_ollama/chat_models.py:965\u001b[39m, in \u001b[36mChatOllama._chat_stream_with_aggregation\u001b[39m\u001b[34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[39m\n\u001b[32m    956\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_chat_stream_with_aggregation\u001b[39m(\n\u001b[32m    957\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    958\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m    962\u001b[39m     **kwargs: Any,\n\u001b[32m    963\u001b[39m ) -> ChatGenerationChunk:\n\u001b[32m    964\u001b[39m     final_chunk = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iterate_over_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfinal_chunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfinal_chunk\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dipak-workspace/anaconda3/envs/web_agent/lib/python3.11/site-packages/langchain_ollama/chat_models.py:1054\u001b[39m, in \u001b[36mChatOllama._iterate_over_stream\u001b[39m\u001b[34m(self, messages, stop, **kwargs)\u001b[39m\n\u001b[32m   1047\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_iterate_over_stream\u001b[39m(\n\u001b[32m   1048\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1049\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m   1050\u001b[39m     stop: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1051\u001b[39m     **kwargs: Any,\n\u001b[32m   1052\u001b[39m ) -> Iterator[ChatGenerationChunk]:\n\u001b[32m   1053\u001b[39m     reasoning = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mreasoning\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m.reasoning)\n\u001b[32m-> \u001b[39m\u001b[32m1054\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_chat_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1055\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1056\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1057\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   1058\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   1059\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   1060\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dipak-workspace/anaconda3/envs/web_agent/lib/python3.11/site-packages/langchain_ollama/chat_models.py:952\u001b[39m, in \u001b[36mChatOllama._create_chat_stream\u001b[39m\u001b[34m(self, messages, stop, **kwargs)\u001b[39m\n\u001b[32m    950\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chat_params[\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    951\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client:\n\u001b[32m--> \u001b[39m\u001b[32m952\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.chat(**chat_params)\n\u001b[32m    953\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client:\n\u001b[32m    954\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.chat(**chat_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dipak-workspace/anaconda3/envs/web_agent/lib/python3.11/site-packages/ollama/_client.py:179\u001b[39m, in \u001b[36mClient._request.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    178\u001b[39m   e.response.read()\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e.response.text, e.response.status_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m r.iter_lines():\n\u001b[32m    182\u001b[39m   part = json.loads(line)\n",
      "\u001b[31mResponseError\u001b[39m: registry.ollama.ai/library/gemma3n:latest does not support tools (status code: 400)",
      "During task with name 'model' and id 'b4c7093e-1aa8-b110-0cf2-f7c426fe2ee0'"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 2 (retry): Re-create the Agent with a native tool-calling LLM\n",
    "# ---------------------------------------------------------\n",
    "# CONCEPT: ReAct Pattern (Reasoning + Acting)\n",
    "# ReAct agents alternate between:\n",
    "#   1. REASONING: Think about what to do next\n",
    "#   2. ACTING: Use a tool to gather information\n",
    "#\n",
    "# Example flow:\n",
    "#   User: \"What's the weather in Tokyo?\"\n",
    "#   Agent (Reasoning): I need to check the weather for Tokyo\n",
    "#   Agent (Acting): Call get_weather(city=\"Tokyo\")\n",
    "#   Tool Result: \"It is 23°C and sunny in Tokyo\"\n",
    "#   Agent (Reasoning): I have the weather information\n",
    "#   Agent (Final Answer): \"The weather in Tokyo is currently 23°C and sunny.\"\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "#from langgraph.prebuilt import create_react_agent\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "\n",
    "# Create an LLM with native tool calling support\n",
    "# gemma3:latest is a 3B parameter model trained for function calling\n",
    "llm = ChatOllama(\n",
    "    model=\"gemma3n:latest\",\n",
    "    temperature=0  # Deterministic outputs for consistency\n",
    ")\n",
    "\n",
    "# Create a system prompt to guide the agent's behavior\n",
    "system_prompt = \"\"\"You are a helpful weather assistant.\n",
    "\n",
    "When a user asks about weather, use the get_weather tool to fetch current conditions.\n",
    "Always provide friendly, conversational responses.\n",
    "\n",
    "Remember to:\n",
    "- Use the tool when you need real-time information\n",
    "- Provide clear, concise answers\n",
    "- Be helpful and polite\n",
    "\"\"\"\n",
    "\n",
    "# Create the ReAct agent\n",
    "# The agent will automatically:\n",
    "#   - Decide when to use tools\n",
    "#   - Parse tool results\n",
    "#   - Continue reasoning until it has a final answer\n",
    "agent = create_agent(\n",
    "    llm,\n",
    "    tools=[get_weather],\n",
    "    system_prompt=system_prompt\n",
    ")\n",
    "\n",
    "# Test the agent with a weather question\n",
    "print(\"Testing the agent...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "result = agent.invoke({\n",
    "    \"messages\": [(\"user\", \"What's the weather in Tokyo?\")]\n",
    "})\n",
    "\n",
    "# Display the conversation\n",
    "print(\"\\nAgent Conversation:\")\n",
    "for message in result[\"messages\"]:\n",
    "    role = message.__class__.__name__\n",
    "    content = message.content if hasattr(message, 'content') else str(message)\n",
    "    print(f\"\\n[{role}]\")\n",
    "    print(content)\n",
    "\n",
    "# EXPECTED BEHAVIOR:\n",
    "# You should see the agent:\n",
    "#   1. Receive the user question\n",
    "#   2. Decide to call get_weather tool\n",
    "#   3. Execute the tool with city=\"Tokyo\"\n",
    "#   4. Receive the result\n",
    "#   5. Formulate a natural language response\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Testing with temperature unit parameter...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "result2 = agent.invoke({\n",
    "    \"messages\": [(\"user\", \"What's the weather in London in fahrenheit?\")]\n",
    "})\n",
    "\n",
    "print(\"\\nAgent Response:\")\n",
    "print(result2[\"messages\"][-1].content)\n",
    "\n",
    "# LEARNING NOTE: What Just Happened?\n",
    "# Without writing ANY parsing code or control flow, the agent:\n",
    "#   ✓ Understood the user's intent\n",
    "#   ✓ Identified the right tool to use\n",
    "#   ✓ Extracted the correct arguments (city=\"London\", unit=\"fahrenheit\")\n",
    "#   ✓ Called the tool\n",
    "#   ✓ Formatted a natural response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yn0fe4dnbf",
   "metadata": {},
   "source": [
    "## 4- Web Search Agent\n",
    "\n",
    "Now that you know how to use LangChain with tools, let's build something useful. Instead of a toy get_weather tool, let create an agent that searches the web and answers questions using real results. In the next section, you will create a [DuckDuckGo](https://github.com/deedy5/ddgs) search tool and wire it into a ReAct agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb0ec3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing web_search tool:\n",
      "======================================================================\n",
      "1. Learn - Docs by LangChain\n",
      "   URL: https://docs.langchain.com/oss/python/learn\n",
      "   Tutorials , conceptual guides, and resources to help you get started. In the Learn section of the documentation, you'll find a collection of tutorials...\n",
      "\n",
      "2. LangChain Tutorial - GeeksforGeeks\n",
      "   URL: https://www.geeksforgeeks.org/data-science/langchain-tutorial/\n",
      "   LangChain is a framework that makes it easier to build applications using large language models (LLMs) by connecting them with data, tools and APIs. I...\n",
      "\n",
      "3. LangChain Course Curriculum - All Lessons & Tutorials\n",
      "   URL: https://langchain-tutorials.com/lessons\n",
      "   Complete LangChain course curriculum with 15+ tutorials . Learn AI development, RAG systems, vector databases, and more. From beginner to advanced - f...\n",
      "\n",
      "4. LangChain Tutorial: From Fundamentals to Advanced RAG\n",
      "   URL: https://tutorial.theaibuilders.dev/tutorials/Frameworks/langchain\n",
      "   Learn LangChain Tutorial : From Fundamentals to Advanced RAG - Interactive AI tutorial with hands-on examples, code snippets, and practical applicatio...\n",
      "\n",
      "5. LangChain OpenTutorial - GitHub\n",
      "   URL: https://github.com/LangChain-OpenTutorial/\n",
      "   Learn LangChain , a framework for building AI applications, with practical examples and global use cases. This repository offers a roadmap, a Python p...\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 1: Write a web search tool\n",
    "# ---------------------------------------------------------\n",
    "# CONCEPT: Real-World Tool - Web Search\n",
    "# Now we move from toy examples (weather) to a practical tool: web search.\n",
    "#\n",
    "# DuckDuckGo Search (DDGS) provides:\n",
    "#   - Free API with no authentication required\n",
    "#   - Privacy-focused search results\n",
    "#   - Simple Python interface\n",
    "#   - No rate limits for reasonable use\n",
    "\n",
    "from ddgs import DDGS\n",
    "from langchain_core.tools import tool\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "\n",
    "@tool\n",
    "def web_search(query: str) -> str:\n",
    "    \"\"\"Search the web for information about a query.\n",
    "    \n",
    "    This tool uses DuckDuckGo to find relevant web pages and returns\n",
    "    the titles and URLs of the top results.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query string (e.g., \"latest AI developments\")\n",
    "        \n",
    "    Returns:\n",
    "        A formatted string containing search results with titles and URLs\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a DuckDuckGo search instance\n",
    "        ddgs = DDGS()\n",
    "        \n",
    "        # Perform the search\n",
    "        # max_results=5: Get top 5 results (balance between info and speed)\n",
    "        # safesearch='moderate': Filter explicit content\n",
    "        results = ddgs.text(\n",
    "            query,\n",
    "            max_results=5,\n",
    "            safesearch='moderate'\n",
    "        )\n",
    "        \n",
    "        # Format the results into a readable string\n",
    "        if not results:\n",
    "            return f\"No results found for: {query}\"\n",
    "        \n",
    "        formatted_results = []\n",
    "        for i, result in enumerate(results, 1):\n",
    "            # Each result has: title, href (URL), body (snippet)\n",
    "            title = result.get('title', 'No title')\n",
    "            url = result.get('href', 'No URL')\n",
    "            snippet = result.get('body', '')\n",
    "            \n",
    "            formatted_results.append(\n",
    "                f\"{i}. {title}\\n\"\n",
    "                f\"   URL: {url}\\n\"\n",
    "                f\"   {snippet[:150]}...\"  # Limit snippet to 150 chars\n",
    "            )\n",
    "        \n",
    "        return \"\\n\\n\".join(formatted_results)\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Handle errors gracefully (network issues, API changes, etc.)\n",
    "        return f\"Error performing search: {str(e)}\"\n",
    "\n",
    "# Test the search tool\n",
    "print(\"Testing web_search tool:\")\n",
    "print(\"=\"*70)\n",
    "test_result = web_search.invoke({\"query\": \"LangChain tutorial\"})\n",
    "print(test_result)\n",
    "\n",
    "# LEARNING NOTE: Error Handling\n",
    "# Real-world tools should handle:\n",
    "#   - Network failures (timeout, no connection)\n",
    "#   - API changes (DuckDuckGo updates their interface)\n",
    "#   - Rate limiting (too many requests)\n",
    "#   - Invalid inputs (empty query, special characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fdc4fc87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Web search agent initialized!\n",
      "  Model: qwen3:latest \n",
      "  Tools: web_search\n",
      "  Ready to answer questions using web search\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 2: Initialize the web-search agent\n",
    "# ---------------------------------------------------------\n",
    "# CONCEPT: Building a Perplexity-Style Agent\n",
    "# Perplexity.ai is a popular AI search engine that:\n",
    "#   1. Takes a user question\n",
    "#   2. Searches the web for relevant information\n",
    "#   3. Synthesizes an answer from search results\n",
    "#   4. Cites sources\n",
    "#\n",
    "# We're building a simplified version using:\n",
    "#   - LangChain ReAct agent (reasoning + acting)\n",
    "#   - Our web_search tool\n",
    "#   - gemma3:latest (for native tool calling)\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "#from langgraph.prebuilt import create_react_agent\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "\n",
    "# Create the LLM\n",
    "llm = ChatOllama(\n",
    "    model=\"qwen3:latest\",\n",
    "    temperature=0.3  # Slightly higher for more natural responses\n",
    ")\n",
    "\n",
    "# Create a system prompt for the search agent\n",
    "search_agent_prompt = \"\"\"You are a helpful research assistant with access to web search.\n",
    "\n",
    "When a user asks a question:\n",
    "1. Determine if you need current/factual information from the web\n",
    "2. If yes, use the web_search tool to find relevant information\n",
    "3. Synthesize the search results into a clear, accurate answer\n",
    "4. Cite your sources when possible\n",
    "\n",
    "Guidelines:\n",
    "- Use web search for current events, facts, or information you're unsure about\n",
    "- Don't search for simple questions you can answer directly\n",
    "- Provide concise, well-organized answers\n",
    "- Always mention when information comes from search results\n",
    "\"\"\"\n",
    "\n",
    "# Create the agent with the web search tool\n",
    "web_agent = create_agent(\n",
    "    llm,\n",
    "    tools=[web_search],\n",
    "    system_prompt=search_agent_prompt\n",
    ")\n",
    "\n",
    "print(\"✓ Web search agent initialized!\")\n",
    "print(\"  Model: qwen3:latest \")\n",
    "print(\"  Tools: web_search\")\n",
    "print(\"  Ready to answer questions using web search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1696c281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TEST 1: Current Events Question\n",
      "======================================================================\n",
      "\n",
      "Question: What are the latest developments in AI in 2026?\n",
      "\n",
      "Agent's Response:\n",
      "Here are the key AI developments and trends predicted for 2026, based on recent analyses:\n",
      "\n",
      "1. **AI as a Collaborative Partner**  \n",
      "   AI is expected to transition from a tool to a true collaborator, enhancing teamwork, security, and infrastructure efficiency (Microsoft, 2026).\n",
      "\n",
      "2. **Shift to Pragmatism**  \n",
      "   The AI industry will move from hype to practical applications, focusing on reliable agents, smaller models, and integration into real-world workflows (TechCrunch, 2026).\n",
      "\n",
      "3. **Advancements in Model Architecture**  \n",
      "   Innovations include \"world models,\" physical AI systems, and more efficient architectures that balance performance with resource constraints (TechCrunch, IBM).\n",
      "\n",
      "4. **GenAI as an Organizational Tool**  \n",
      "   Generative AI (GenAI) will become a cornerstone for businesses, enabling new productivity tools and reshaping workflows (MIT Sloan, 2026).\n",
      "\n",
      "5. **Security and Ethical AI**  \n",
      "   Enhanced focus on AI security, privacy, and ethical frameworks will accompany technological growth, driven by regulatory and societal demands (IBM, Forbes).\n",
      "\n",
      "6. **Quantum and AI Convergence**  \n",
      "   Experts predict increased collaboration between AI and quantum computing to solve complex problems in fields like drug discovery and cryptography (IBM).\n",
      "\n",
      "For more details, refer to the full articles:  \n",
      "- [Microsoft's 7 AI Trends](https://news.microsoft.com/source/features/ai/whats-next-in-ai-7-trends-to-watch-in-2026)  \n",
      "- [Forbes' 10 Predictions](https://www.forbes.com/sites/robtoews/2025/12/22/10-ai-predictions-for-2026/)  \n",
      "- [IBM's Tech Trends](https://www.ibm.com/think/news/ai-tech-trends-predictions-2026)  \n",
      "- [MIT Sloan's Analysis](https://sloanreview.mit.edu/article/five-trends-in-ai-and-data-science-for-2026/)  \n",
      "- [TechCrunch's Outlook](https://techcrunch.com/2026/01/02/in-2026-ai-will-move-from-hype-to-pragmatism/)\n",
      "\n",
      "======================================================================\n",
      "TEST 2: Factual Question\n",
      "======================================================================\n",
      "\n",
      "Question: What is LangChain and how does it work?\n",
      "\n",
      "Agent's Response:\n",
      "LangChain is an open-source framework designed to simplify the development of applications using large language models (LLMs). It enables developers to build AI applications by integrating LLMs with data sources, tools, and other services, allowing models to interact with the real world. Here's how it works:\n",
      "\n",
      "1. **Core Functionality**:  \n",
      "   LangChain provides a pre-built agent architecture that acts as an interface between LLMs and external tools or data sources. This allows models to perform tasks like answering questions, executing code, or retrieving information by chaining together multiple steps.\n",
      "\n",
      "2. **Key Components**:  \n",
      "   - **Agents**: These are autonomous entities that use LLMs to make decisions and take actions based on input.  \n",
      "   - **Memory**: LangChain incorporates memory systems (e.g., vector stores) to let models retain context across interactions.  \n",
      "   - **Tool Integration**: It supports integration with various tools, databases, and APIs, enabling LLMs to access real-time data or perform specific functions.\n",
      "\n",
      "3. **Workflow**:  \n",
      "   Developers use LangChain to define prompts, connect LLMs to tools, and orchestrate workflows. For example, an agent might use an LLM to analyze user input, query a database, and generate a response—all while maintaining context through memory.\n",
      "\n",
      "4. **Use Cases**:  \n",
      "   Applications include chatbots, data analysis tools, and automation systems where LLMs need to interact dynamically with external data or services.\n",
      "\n",
      "For more details, refer to the [LangChain documentation](https://docs.langchain.com/) or tutorials on building agents with LLMs.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 3: Test your Ask-the-Web agent\n",
    "# ---------------------------------------------------------\n",
    "# CONCEPT: Testing the Complete Agent\n",
    "# Let's test with different types of questions to see how the agent behaves:\n",
    "#   1. Current events (requires search)\n",
    "#   2. Factual questions (might require search)\n",
    "\n",
    "# Test 1: Current events question\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST 1: Current Events Question\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "question1 = \"What are the latest developments in AI in 2026?\"\n",
    "\n",
    "result1 = web_agent.invoke({\n",
    "    \"messages\": [(\"user\", question1)]\n",
    "})\n",
    "\n",
    "print(f\"\\nQuestion: {question1}\")\n",
    "print(f\"\\nAgent's Response:\")\n",
    "print(result1[\"messages\"][-1].content)\n",
    "\n",
    "# Test 2: Factual question\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST 2: Factual Question\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "question2 = \"What is LangChain and how does it work?\"\n",
    "\n",
    "result2 = web_agent.invoke({\n",
    "    \"messages\": [(\"user\", question2)]\n",
    "})\n",
    "\n",
    "print(f\"\\nQuestion: {question2}\")\n",
    "print(f\"\\nAgent's Response:\")\n",
    "print(result2[\"messages\"][-1].content)\n",
    "\n",
    "# LEARNING NOTE: Observing Agent Behavior\n",
    "# As you run these tests, notice:\n",
    "#   1. Tool Decision Making:\n",
    "#      - Does the agent search for every question?\n",
    "#      - When does it rely on its own knowledge?\n",
    "#   2. Search Query Formulation:\n",
    "#      - How does the agent phrase search queries?\n",
    "#      - Are they different from the user's question?\n",
    "#   3. Answer Synthesis:\n",
    "#      - Does it combine multiple search results?\n",
    "#      - Does it cite sources?\n",
    "#   4. Reasoning Steps:\n",
    "#      - You can see the agent's thought process in the message history\n",
    "#      - Look at result[\"messages\"] to see all intermediate steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyzwk2etyt",
   "metadata": {},
   "source": [
    "## 5- (Optional) MCP: Model Context Protocol\n",
    "\n",
    "Up to now, every tool you used started as a Python function you wrote and registered yourself. **MCP (Model Context Protocol)** lets you skip that step. Tools come from an external *server*, and your code just connects to it. Think of it like USB for AI tools: any MCP client can plug into any MCP server and immediately use whatever tools it offers.\n",
    "\n",
    "Below, we connect to `mcp-server-fetch` (a ready-made server that can retrieve any URL) using the Python MCP SDK. We launch the server, discover its tools, and call one, all without writing a single `@tool` function. To learn more, read: https://github.com/modelcontextprotocol/servers/tree/main/src/fetch\n",
    "\n",
    "> **LangChain integration:** The `langchain-mcp-adapters` package can convert MCP tools into LangChain-compatible tools automatically, so you can drop them straight into a ReAct agent like the ones in section 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "buxkz996bq",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  + Exception Group Traceback (most recent call last):\n",
      "  |   File \"/tmp/ipykernel_28538/2114192685.py\", line 86, in <module>\n",
      "  |     await test_mcp_fetch()\n",
      "  |   File \"/tmp/ipykernel_28538/2114192685.py\", line 48, in test_mcp_fetch\n",
      "  |     async with stdio_client(server_params) as (read, write):\n",
      "  |   File \"/home/dipak/dipak-workspace/anaconda3/envs/web_agent/lib/python3.11/contextlib.py\", line 231, in __aexit__\n",
      "  |     await self.gen.athrow(typ, value, traceback)\n",
      "  |   File \"/home/dipak/dipak-workspace/anaconda3/envs/web_agent/lib/python3.11/site-packages/mcp/client/stdio/__init__.py\", line 182, in stdio_client\n",
      "  |     async with (\n",
      "  |   File \"/home/dipak/dipak-workspace/anaconda3/envs/web_agent/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 783, in __aexit__\n",
      "  |     raise BaseExceptionGroup(\n",
      "  | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n",
      "  +-+---------------- 1 ----------------\n",
      "    | Exception Group Traceback (most recent call last):\n",
      "    |   File \"/home/dipak/dipak-workspace/anaconda3/envs/web_agent/lib/python3.11/site-packages/mcp/client/stdio/__init__.py\", line 189, in stdio_client\n",
      "    |     yield read_stream, write_stream\n",
      "    |   File \"/tmp/ipykernel_28538/2114192685.py\", line 49, in test_mcp_fetch\n",
      "    |     async with ClientSession(read, write) as session:\n",
      "    |   File \"/home/dipak/dipak-workspace/anaconda3/envs/web_agent/lib/python3.11/site-packages/mcp/shared/session.py\", line 238, in __aexit__\n",
      "    |     return await self._task_group.__aexit__(exc_type, exc_val, exc_tb)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/dipak/dipak-workspace/anaconda3/envs/web_agent/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 783, in __aexit__\n",
      "    |     raise BaseExceptionGroup(\n",
      "    | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n",
      "    +-+---------------- 1 ----------------\n",
      "      | Traceback (most recent call last):\n",
      "      |   File \"/tmp/ipykernel_28538/2114192685.py\", line 52, in test_mcp_fetch\n",
      "      |     await session.initialize()\n",
      "      |   File \"/home/dipak/dipak-workspace/anaconda3/envs/web_agent/lib/python3.11/site-packages/mcp/client/session.py\", line 171, in initialize\n",
      "      |     result = await self.send_request(\n",
      "      |              ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "      |   File \"/home/dipak/dipak-workspace/anaconda3/envs/web_agent/lib/python3.11/site-packages/mcp/shared/session.py\", line 306, in send_request\n",
      "      |     raise McpError(response_or_error.error)\n",
      "      | mcp.shared.exceptions.McpError: Connection closed\n",
      "      +------------------------------------\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dipak/dipak-workspace/anaconda3/envs/web_agent/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3699, in run_code\n",
      "    await eval(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_28538/2114192685.py\", line 89, in <module>\n",
      "    asyncio.run(test_mcp_fetch())\n",
      "  File \"/home/dipak/dipak-workspace/anaconda3/envs/web_agent/lib/python3.11/asyncio/runners.py\", line 186, in run\n",
      "    raise RuntimeError(\n",
      "RuntimeError: asyncio.run() cannot be called from a running event loop\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# MCP: Model Context Protocol\n",
    "# ---------------------------------------------------------\n",
    "# CONCEPT: What is MCP?\n",
    "# MCP is a universal standard for connecting LLMs to external tools and data.\n",
    "#\n",
    "# Think of it like USB for AI:\n",
    "#   - USB: Any device can plug into any computer\n",
    "#   - MCP: Any LLM can use any tool server\n",
    "#\n",
    "# Benefits:\n",
    "#   1. Standardization: One protocol for all tools\n",
    "#   2. Reusability: Write a tool once, use it anywhere\n",
    "#   3. Security: Tools run in separate processes\n",
    "#   4. Ecosystem: Growing library of ready-made tool servers\n",
    "#\n",
    "# Example servers:\n",
    "#   - mcp-server-fetch: Fetch content from URLs\n",
    "#   - mcp-server-filesystem: Read/write files\n",
    "#   - mcp-server-git: Git operations\n",
    "#   - mcp-server-postgres: Database queries\n",
    "\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "import asyncio\n",
    "\n",
    "async def test_mcp_fetch():\n",
    "    \"\"\"\n",
    "    Connect to mcp-server-fetch and use it to retrieve web content.\n",
    "    \n",
    "    This demonstrates:\n",
    "      1. Launching an MCP server\n",
    "      2. Discovering available tools\n",
    "      3. Calling a tool through MCP\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Define server parameters\n",
    "    # StdioServerParameters launches the server as a subprocess\n",
    "    server_params = StdioServerParameters(\n",
    "        command=\"npx\",  # Use npx to run the server\n",
    "        args=[\n",
    "            \"-y\",  # Auto-install if needed\n",
    "            \"@modelcontextprotocol/server-fetch\"  # The MCP server package\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Step 2: Connect to the server\n",
    "    async with stdio_client(server_params) as (read, write):\n",
    "        async with ClientSession(read, write) as session:\n",
    "            \n",
    "            # Step 3: Initialize the session\n",
    "            await session.initialize()\n",
    "            \n",
    "            print(\"✓ Connected to mcp-server-fetch\")\n",
    "            print()\n",
    "            \n",
    "            # Step 4: Discover available tools\n",
    "            tools = await session.list_tools()\n",
    "            \n",
    "            print(f\"Available tools: {len(tools.tools)}\")\n",
    "            for tool in tools.tools:\n",
    "                print(f\"  - {tool.name}: {tool.description}\")\n",
    "            print()\n",
    "            \n",
    "            # Step 5: Call the fetch tool\n",
    "            url_to_fetch = \"https://www.python.org\"\n",
    "            \n",
    "            print(f\"Fetching content from: {url_to_fetch}\")\n",
    "            print(\"=\"*70)\n",
    "            \n",
    "            result = await session.call_tool(\n",
    "                \"fetch\",\n",
    "                arguments={\"url\": url_to_fetch}\n",
    "            )\n",
    "            \n",
    "            # Display the result (truncated for readability)\n",
    "            content = str(result.content[0].text)\n",
    "            print(content[:500] + \"...\")\n",
    "            print()\n",
    "            print(f\"✓ Successfully fetched {len(content)} characters\")\n",
    "\n",
    "# Run the async function\n",
    "# Note: In Jupyter, you can use await directly if using ipykernel 6+\n",
    "try:\n",
    "    # Try direct await (works in modern Jupyter)\n",
    "    await test_mcp_fetch()\n",
    "except:\n",
    "    # Fallback to asyncio.run() (works in regular Python)\n",
    "    asyncio.run(test_mcp_fetch())\n",
    "\n",
    "# LEARNING NOTE: Why MCP Matters\n",
    "# Without MCP, every tool integration is custom:\n",
    "#   - Different APIs for different tools\n",
    "#   - Different authentication methods\n",
    "#   - Different error handling\n",
    "#\n",
    "# With MCP:\n",
    "#   - Uniform interface for all tools\n",
    "#   - Standard discovery mechanism\n",
    "#   - Consistent error handling\n",
    "#   - Interoperable ecosystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7v9so18x",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# MCP + LangChain Integration\n",
    "# ---------------------------------------------------------\n",
    "# CONCEPT: Bridging MCP and LangChain\n",
    "# The langchain-mcp-adapters package bridges MCP and LangChain:\n",
    "#   - Converts MCP tools to LangChain tools\n",
    "#   - Handles async/sync conversion\n",
    "#   - Manages server lifecycle\n",
    "#\n",
    "# This lets you use MCP servers directly in LangChain agents!\n",
    "\n",
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "async def create_mcp_agent():\n",
    "    \"\"\"\n",
    "    Create a LangChain agent that uses MCP tools.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Load MCP tools\n",
    "    # This connects to the server and converts tools to LangChain format\n",
    "    print(\"Loading MCP tools...\")\n",
    "    \n",
    "    mcp_tools = await load_mcp_tools(\n",
    "        \"npx\",\n",
    "        \"-y\",\n",
    "        \"@modelcontextprotocol/server-fetch\"\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ Loaded {len(mcp_tools)} MCP tools\")\n",
    "    for tool in mcp_tools:\n",
    "        print(f\"  - {tool.name}\")\n",
    "    print()\n",
    "    \n",
    "    # Step 2: Create LLM\n",
    "    llm = ChatOllama(\n",
    "        model=\"gemma3:latest\",\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # Step 3: Create agent with MCP tools\n",
    "    agent = create_react_agent(\n",
    "        llm,\n",
    "        tools=mcp_tools,\n",
    "        state_modifier=\"\"\"You are a helpful assistant with access to web fetching tools.\n",
    "\n",
    "When asked to retrieve web content, use the fetch tool to get the page content.\n",
    "Provide clear summaries of the content you retrieve.\"\"\"\n",
    "    )\n",
    "    \n",
    "    print(\"✓ Agent created with MCP tools\")\n",
    "    print()\n",
    "    \n",
    "    # Step 4: Test the agent\n",
    "    test_query = \"Fetch the content of https://www.python.org and tell me what it's about\"\n",
    "    \n",
    "    print(f\"Query: {test_query}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    result = await agent.ainvoke({\n",
    "        \"messages\": [(\"user\", test_query)]\n",
    "    })\n",
    "    \n",
    "    print(\"\\nAgent Response:\")\n",
    "    print(result[\"messages\"][-1].content)\n",
    "    \n",
    "    return agent\n",
    "\n",
    "# Run the async function\n",
    "try:\n",
    "    await create_mcp_agent()\n",
    "except:\n",
    "    asyncio.run(create_mcp_agent())\n",
    "\n",
    "# LEARNING NOTE: MCP vs Custom Tools\n",
    "# Compare this to Section 4 where we wrote a custom web_search tool:\n",
    "#\n",
    "# Custom Tool (Section 4):\n",
    "#   ✓ Full control over implementation\n",
    "#   ✓ No external dependencies\n",
    "#   ✗ Must write and maintain code\n",
    "#   ✗ Limited to what you implement\n",
    "#\n",
    "# MCP Tool (Section 5):\n",
    "#   ✓ Ready-made, tested tools\n",
    "#   ✓ Community-maintained\n",
    "#   ✓ Standardized interface\n",
    "#   ✗ Less control over behavior\n",
    "#   ✗ Requires MCP server\n",
    "#\n",
    "# Best practice: Use MCP for common tasks, custom tools for specialized needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6647eac",
   "metadata": {},
   "source": [
    "## 6- (Optional) A Minimal UI\n",
    "\n",
    "[Chainlit](https://chainlit.io/) is a Python library designed specifically for building LLM and agent UIs. It provides:\n",
    "- Built-in streaming support\n",
    "- Message history\n",
    "- Step visualization (see tool calls as they happen)\n",
    "- No frontend code required\n",
    "\n",
    "If you are interested, follow Chainlit's documentation to implement a simple UI for your agent. The process typically involves:\n",
    "\n",
    "1. You write a Python file named `chainlit_app.py` with the agent creation logic as well as UI handlers (e.g.,`@cl.on_message`)\n",
    "2. Run the file in your terminal with `chainlit run app.py`\n",
    "3. A web UI opens automatically at `http://localhost:8000`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hi1y4z7r2y",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile chainlit_app.py\n",
    "# ---------------------------------------------------------\n",
    "# Chainlit Web Search Agent\n",
    "# ---------------------------------------------------------\n",
    "# CONCEPT: What is Chainlit?\n",
    "# Chainlit is a Python framework specifically designed for building LLM UIs.\n",
    "#\n",
    "# Key Features:\n",
    "#   1. Zero frontend code needed - pure Python\n",
    "#   2. Built-in streaming support - see responses in real-time\n",
    "#   3. Step visualization - watch the agent think and use tools\n",
    "#   4. Message history - automatic conversation management\n",
    "#   5. File uploads - users can upload documents\n",
    "#   6. Authentication - optional user management\n",
    "#\n",
    "# To run this app:\n",
    "#   1. Save this cell (it creates chainlit_app.py)\n",
    "#   2. Run: chainlit run chainlit_app.py\n",
    "#   3. Open browser to: http://localhost:8000\n",
    "\n",
    "import chainlit as cl\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import AIMessage, ToolMessage\n",
    "from ddgs import DDGS\n",
    "\n",
    "\n",
    "# Define the web search tool\n",
    "@tool\n",
    "def web_search(query: str) -> str:\n",
    "    \"\"\"Search the web for information about a query.\"\"\"\n",
    "    try:\n",
    "        ddgs = DDGS()\n",
    "        results = ddgs.text(query, max_results=5, safesearch='moderate')\n",
    "        \n",
    "        if not results:\n",
    "            return f\"No results found for: {query}\"\n",
    "        \n",
    "        formatted_results = []\n",
    "        for i, result in enumerate(results, 1):\n",
    "            title = result.get('title', 'No title')\n",
    "            url = result.get('href', 'No URL')\n",
    "            snippet = result.get('body', '')\n",
    "            \n",
    "            formatted_results.append(\n",
    "                f\"{i}. {title}\\n\"\n",
    "                f\"   URL: {url}\\n\"\n",
    "                f\"   {snippet[:150]}...\"\n",
    "            )\n",
    "        \n",
    "        return \"\\n\\n\".join(formatted_results)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error performing search: {str(e)}\"\n",
    "\n",
    "\n",
    "# Create the agent (once at startup)\n",
    "llm = ChatOllama(model=\"gemma3:latest\", temperature=0.3)\n",
    "\n",
    "agent = create_react_agent(\n",
    "    llm,\n",
    "    tools=[web_search],\n",
    "    state_modifier=\"\"\"You are a helpful research assistant with access to web search.\n",
    "\n",
    "When a user asks a question:\n",
    "1. Determine if you need current/factual information from the web\n",
    "2. If yes, use the web_search tool to find relevant information\n",
    "3. Synthesize the search results into a clear, accurate answer\n",
    "4. Cite your sources when possible\n",
    "\n",
    "Guidelines:\n",
    "- Use web search for current events, facts, or information you're unsure about\n",
    "- Don't search for simple questions you can answer directly\n",
    "- Provide concise, well-organized answers\n",
    "- Always mention when information comes from search results\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "# Chainlit message handler\n",
    "@cl.on_message\n",
    "async def handle_message(message: cl.Message):\n",
    "    \"\"\"Handle user messages and stream agent responses.\"\"\"\n",
    "    \n",
    "    # Create a message placeholder for streaming\n",
    "    response_message = cl.Message(content=\"\")\n",
    "    await response_message.send()\n",
    "    \n",
    "    # Track the current step for visualization\n",
    "    current_step = None\n",
    "    \n",
    "    # Invoke the agent with streaming\n",
    "    async for event in agent.astream_events(\n",
    "        {\"messages\": [(\"user\", message.content)]},\n",
    "        version=\"v1\"\n",
    "    ):\n",
    "        kind = event[\"event\"]\n",
    "        \n",
    "        # Agent is thinking/reasoning\n",
    "        if kind == \"on_chat_model_stream\":\n",
    "            chunk = event[\"data\"][\"chunk\"]\n",
    "            if hasattr(chunk, \"content\"):\n",
    "                await response_message.stream_token(chunk.content)\n",
    "        \n",
    "        # Agent is calling a tool\n",
    "        elif kind == \"on_tool_start\":\n",
    "            tool_name = event[\"name\"]\n",
    "            tool_input = event[\"data\"].get(\"input\", {})\n",
    "            \n",
    "            # Create a step to show tool usage\n",
    "            current_step = cl.Step(\n",
    "                name=f\"🔧 Using {tool_name}\",\n",
    "                type=\"tool\"\n",
    "            )\n",
    "            current_step.input = str(tool_input)\n",
    "            await current_step.send()\n",
    "        \n",
    "        # Tool execution completed\n",
    "        elif kind == \"on_tool_end\":\n",
    "            if current_step:\n",
    "                tool_output = event[\"data\"].get(\"output\", \"\")\n",
    "                current_step.output = str(tool_output)[:500] + \"...\"  # Truncate long outputs\n",
    "                await current_step.update()\n",
    "    \n",
    "    # Finalize the response\n",
    "    await response_message.update()\n",
    "\n",
    "\n",
    "# Welcome message\n",
    "@cl.on_chat_start\n",
    "async def start():\n",
    "    \"\"\"Display a welcome message when the chat starts.\"\"\"\n",
    "    \n",
    "    welcome_message = \"\"\"# 🌐 Ask-the-Web Agent\n",
    "\n",
    "Welcome! I'm an AI assistant with access to web search.\n",
    "\n",
    "## What I Can Do:\n",
    "- 🔍 Search the web for current information\n",
    "- 📰 Answer questions about recent events\n",
    "- 🎓 Research topics and provide summaries\n",
    "- 🔗 Cite sources for my answers\n",
    "\n",
    "## Example Questions:\n",
    "- \"What are the latest developments in AI?\"\n",
    "- \"Explain how LangChain works\"\n",
    "- \"Compare Python and JavaScript for web development\"\n",
    "- \"What's happening in the tech industry today?\"\n",
    "\n",
    "**Try asking me anything!** ��\n",
    "\"\"\"\n",
    "    \n",
    "    await cl.Message(content=welcome_message).send()\n",
    "\n",
    "\n",
    "# LEARNING NOTE: Running the Chainlit App\n",
    "# To run this application:\n",
    "#   1. Make sure Ollama is running: ollama serve\n",
    "#   2. Make sure gemma3:latest is installed: ollama pull gemma3:latest\n",
    "#   3. Run the Chainlit app: chainlit run chainlit_app.py\n",
    "#   4. Open your browser to: http://localhost:8000\n",
    "#   5. Start chatting!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a116bbee",
   "metadata": {},
   "source": [
    "## 🎉 Congratulations!\n",
    "\n",
    "You have built a **web-enabled agent** from scratch: manual tool calling → JSON schemas → LangChain ReAct → web search → MCP → UI.\n",
    "\n",
    "Next steps:\n",
    "* Try adding more tools, such as news or finance APIs.\n",
    "* Experiment with multiple tools, different models, and measure accuracy vs. hallucination.\n",
    "* Explore the [MCP server registry](https://github.com/modelcontextprotocol/servers) for ready-made tool servers.\n",
    "\n",
    "👏 **Great job!** Take a moment to celebrate. The techniques you implemented here power many production agents and chatbots."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
