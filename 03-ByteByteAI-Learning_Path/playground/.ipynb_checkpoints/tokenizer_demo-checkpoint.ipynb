{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9faf1ec",
   "metadata": {},
   "source": [
    "# Tokenizer Demo\n",
    "This notebook demonstrates tokenization using two popular libraries:\n",
    "- **Hugging Face Transformers** – for BERT‑style tokenizers\n",
    "- **tiktoken** – OpenAI's fast tokeniser used by GPT models\n",
    "\n",
    "## Installation\n",
    "You need to install the following Python packages before running the examples:\n",
    "```bash\n",
    "pip install transformers tiktoken\n",
    "```\n",
    "The command above works on most platforms (Linux, macOS, Windows).\n",
    "\n",
    "### Detailed tiktoken installation notes\n",
    "* **Python version**: tiktoken requires Python >= 3.8.\n",
    "* **Binary wheels**: For Linux/macOS, pip will download pre‑compiled wheels (manylinux). No compiler is needed.\n",
    "* **Building from source**: If a wheel is not available for your platform, pip will attempt to build from source. This requires a C compiler (e.g., `gcc` on Linux, `clang` on macOS) and `rustc` because tiktoken includes Rust extensions. Install Rust via `curl https://sh.rustup.rs -sSf | sh` if needed.\n",
    "* **Conda users**: You can also install via conda‑forge: `conda install -c conda-forge tiktoken`.\n",
    "* **Optional dependencies**: No additional system libraries are required for the basic encoder. If you plan to use the `tiktoken` tokenizer with OpenAI's `gpt‑3.5‑turbo` or `gpt‑4` models, ensure you have internet access for the model‑specific encodings (they are bundled).\n",
    "* **Verification**: After installation, you can run `python -c \"import tiktoken, sys; print(tiktoken.__version__)\"` to confirm it works.\n",
    "\n",
    "You can also install the packages directly from this notebook using the magic command below (run the cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "149aaeff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.57.6-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.12.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: filelock in /home/dipak/dipak-workspace/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/dipak/dipak-workspace/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/dipak/dipak-workspace/anaconda3/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/dipak/dipak-workspace/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/dipak/dipak-workspace/anaconda3/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /home/dipak/dipak-workspace/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Using cached tokenizers-0.22.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/dipak/dipak-workspace/anaconda3/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/dipak/dipak-workspace/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/dipak/dipak-workspace/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.11.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/dipak/dipak-workspace/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/dipak/dipak-workspace/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/dipak/dipak-workspace/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dipak/dipak-workspace/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n",
      "Using cached transformers-4.57.6-py3-none-any.whl (12.0 MB)\n",
      "Downloading tiktoken-0.12.0-cp312-cp312-manylinux_2_28_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "Using cached safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
      "Using cached tokenizers-0.22.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Installing collected packages: safetensors, hf-xet, tiktoken, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed hf-xet-1.2.0 huggingface-hub-0.36.0 safetensors-0.7.0 tiktoken-0.12.0 tokenizers-0.22.2 transformers-4.57.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers tiktoken\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580bc5e8",
   "metadata": {},
   "source": [
    "## Example 1: Hugging Face Transformers tokenizer\n",
    "We load the `bert-base-uncased` tokenizer and tokenize a short sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6922a801",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80903efe82a9420caddddf6c235d0e89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a6c14f1962a42ff94db9299f6624d23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5caaf1320c41444db1c7952e2d9a452a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23bfc316b1014c5a9edc1543362790c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers tokens: ['hello', ',', 'world', '!', 'this', 'is', 'a', 'token', '##izer', 'demo', '.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "text = \"Hello, world! This is a tokenizer demo.\"\n",
    "print('Transformers tokens:', tokenizer.tokenize(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2f1b8a",
   "metadata": {},
   "source": [
    "## Example 2: tiktoken\n",
    "tiktoken provides fast tokenisation for OpenAI models.\n",
    "We use the `cl100k_base` encoding (used by gpt‑3.5‑turbo and gpt‑4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea691837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken tokens: [9906, 11, 1917, 0, 1115, 374, 264, 47058, 17074, 13]\n",
      "Number of tokens: 10\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('cl100k_base')\n",
    "print('tiktoken tokens:', enc.encode(text))\n",
    "# Show token count\n",
    "print('Number of tokens:', len(enc.encode(text)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19f6a78-7eac-4fa8-915f-2fba1c1d8c1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
