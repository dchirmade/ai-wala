{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe0fb30d",
   "metadata": {},
   "source": [
    "# Project 4: **Build a Deep Research System**\n",
    "Welcome to project 4! For this project, we shift our focus from tool use and agents to *reasoning* models. You will practice state‑of‑the‑art inference‑time scaling methods such as *Chain‑of‑Thought* prompting and *Tree‑of‑Thoughts*, and briefly explore high-level concepts of training reasoning models using techniques like **STaR**.\n",
    "\n",
    "\n",
    "Finally, you will put everything together to build a *deep research agent* that can browse the web, reason over what it finds, and give structured answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54845369",
   "metadata": {},
   "source": [
    "## Learning Objectives  \n",
    "* Apply common inference‑time scaling methods: **zero‑shot / few‑shot CoT, self‑consistency, sequential decoding, tree‑of‑thoughts**  \n",
    "* Gain intuition for **training** reasoning‑capable models following **STaR** approach \n",
    "* Build a minimal **deep‑research agent** that combines step‑by‑step reasoning with live web search   \n",
    "* Practice extending deep-search to a multi-agent system "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a40a86",
   "metadata": {},
   "source": [
    "## Roadmap  \n",
    "0. Environment setup  \n",
    "1. Inference‑time scaling  \n",
    "  1.1 Few‑shot.   \n",
    "  1.2 Zero‑shot CoT.   \n",
    "  1.3 Self‑consistency.   \n",
    "  1.4 Sequential revisions.     \n",
    "  1.5 Tree‑of‑Thought (ToT)\n",
    "2. Training reasoning models and inspecting deepseek-r1 \n",
    "3. Deep-research agent  \n",
    "4. (Optional) Multi-agent deep-research"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17c2218",
   "metadata": {},
   "source": [
    "# 0- Environment setup\n",
    "\n",
    "### Step 1: Create your environment and install dependencies \n",
    "Before we start coding, you need a reproducible setup. Open a terminal in the same directory as this notebook, and use Conda or uv to install the project dependencies.\n",
    "\n",
    "#### Option 1: Conda\n",
    "```bash\n",
    "# Create and activate the conda environment\n",
    "conda env create -f environment.yaml && conda activate deep_research\n",
    "```\n",
    "\n",
    "#### Option 2: uv (Fast alternative)\n",
    "If you prefer [uv](https://docs.astral.sh/uv/) over Conda:\n",
    "\n",
    "```bash\n",
    "# Install uv (skip if already installed)\n",
    "curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "\n",
    "# Create a virtual environment and install dependencies\n",
    "uv venv .venv-deep-research && source .venv-deep-research/bin/activate\n",
    "uv pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "### Step 2: Register this environment as a Jupyter kernel\n",
    "```bash\n",
    "python -m ipykernel install --user --name=deep_research --display-name \"deep_research\"\n",
    "```\n",
    "Now open your notebook and switch to the `deep_research` kernel (Kernel → Change Kernel).\n",
    "\n",
    "### Step 3: Setup and run Ollama serve\n",
    "\n",
    "In this project we use the `llama3.2:3b`, `qwen2.5:3b-instruct` and `deepseek-r1:1.5b` models. You can try other smaller or larger reasoning LLMs such as `phi4-mini` to compare performance. Explore available models here: https://ollama.com/library.\n",
    "\n",
    "Open terminal and run ollama:\n",
    "```bash\n",
    "ollama serve\n",
    "```\n",
    "Then open another terminal and pull required models: \n",
    "```bash\n",
    "ollama pull llama3.2:3b\n",
    "ollama pull deepseek-r1:1.5b\n",
    "ollama pull qwen2.5:3b-instruct\n",
    "# Additional small reasoning models to compare\n",
    "# ollama pull phi4-mini\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e8d1b7",
   "metadata": {},
   "source": [
    "---  \n",
    "# 1‑ Inference‑time scaling\n",
    "\n",
    "Inference-time scaling refers to techniques that make an existing model reason better without retraining it. Instead of changing the model’s weights, we achieve reasoning capability by adjusting how we prompt, sample, or aggregate LLM's outputs.\n",
    "\n",
    "In this section, we’ll explore several inference-time strategies that improve reasoning quality using a non-reasoning base model. You will experiment with and compare methods such as:\n",
    "\n",
    "- Few-shot Chain-of-Thought (CoT)\n",
    "- Zero-shot CoT\n",
    "- Self-consistency\n",
    "- Sequential revision\n",
    "- Tree-of-Thoughts (ToT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081d499a",
   "metadata": {},
   "source": [
    "## 1.1: Few-Shot CoT\n",
    "\n",
    "Few-shot prompting provides examples before asking a new question. The model learns from the pattern and applies it to new inputs.\n",
    "\n",
    "We'll explore this with two models to understand how few-shot interacts with model capabilities:\n",
    "\n",
    "1. **GPT-2** (no instruction tuning): Doesn't reason by default. We'll see if few-shot examples can elicit reasoning.\n",
    "2. **Llama 3.2** (instruction-tuned): Already reasons naturally. We'll use few-shot to control the output format.\n",
    "\n",
    "### GPT-2: Can few-shot examples elicit reasoning?\n",
    "\n",
    "GPT-2 is a base language model that just predicts the next token. It wasn't trained to follow instructions or reason step-by-step. Let's see what happens with and without few-shot examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "173d73f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54a0c8adf00f40ff909bead193432b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11d786d77d16467a991f7b750824c914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee2e3106ed794d2ab1ea77981ae66eb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel LOAD REPORT from: openai-community/gpt2\n",
      "Key                  | Status     |  | \n",
      "---------------------+------------+--+-\n",
      "h.{0...11}.attn.bias | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d3ab15b41ff4f4bbe67116280f8f9b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83caba482d1146b7a87fcd44a34c65da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0d40bcfb48a49378d50080311ea481f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d22107d4cde0450284d23f498365f133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09f3e9ca49f7443faad5e066247579f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing `generation_config` together with generation-related arguments=({'max_new_tokens', 'temperature', 'do_sample'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=100) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=100) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GPT-2 WITHOUT few-shot examples ===\n",
      "Q: A rectangle has a perimeter of 36 cm. If the length is twice the width, what is the area?\n",
      "A: The square in the top right corner is the area of the rectangle.\n",
      "Q: But the width is two times the width?\n",
      "A: No, the width is not two times the width. It's just that the same area is only twice the width. Also, if two people cross each other and try to eat the same food, the surface of the rectangle will have a thickness of a fraction of the width, so that on the other hand the rectangle is twice as wide and the surface\n",
      "\n",
      "=== GPT-2 WITH few-shot examples ===\n",
      "Q: A rectangle has a perimeter of 36 cm. If the length is twice the width, what is the area?\n",
      "A: Step 1: Add corners. 36 = 36 cm x 36 = 36.3 x 36.3 = 36.39\n",
      "Therefore, the answer is 36:39 x 36 = 36.39 x 36 = 36.39 x 36 = 36.39 x 36 = 36.39 x 36 = 36.39 x 36 = 36.39 x 36 = 36.39 x 36 = 36.39 x 36 = 36.39 x 36 = 36.39 x 36 = 36.39 x\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "# MPS setup for Apple Silicon (use CPU if not on Mac)\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "if device.type == \"mps\":\n",
    "    torch.mps.empty_cache()\n",
    "\n",
    "generator = pipeline(task=\"text-generation\", model=\"openai-community/gpt2\", dtype=torch.float16, device=device)\n",
    "\n",
    "question = \"A rectangle has a perimeter of 36 cm. If the length is twice the width, what is the area?\"\n",
    "\n",
    "# --- Without few-shot examples ---\n",
    "output_zero = generator(f\"Q: {question}\\nA:\", max_new_tokens=100, do_sample=True, temperature=0.8)[0][\"generated_text\"]\n",
    "\n",
    "print(\"=== GPT-2 WITHOUT few-shot examples ===\")\n",
    "print(output_zero)\n",
    "print()\n",
    "\n",
    "# --- With few-shot examples ---\n",
    "few_shot = \"\"\"Q: A store sells apples for $2 each. If I buy 3 apples and pay with a $10 bill, how much change do I get?\n",
    "A: Step 1: Calculate total cost. 3 x $2 = $6.\n",
    "Step 2: Calculate change. $10 - $6 = $4.\n",
    "Therefore, the answer is $4.\n",
    "\n",
    "Q: A train leaves at 9:15 AM and the journey takes 2 hours 30 minutes. What time does it arrive?\n",
    "A: Step 1: Add hours. 9:15 + 2:00 = 11:15.\n",
    "Step 2: Add minutes. 11:15 + 0:30 = 11:45.\n",
    "Therefore, the answer is 11:45 AM.\n",
    "\"\"\"\n",
    "\n",
    "prompt = few_shot + f\"Q: {question}\\nA:\"\n",
    "output_few = generator(prompt, max_new_tokens=100, do_sample=True, temperature=0.8)[0][\"generated_text\"]\n",
    "\n",
    "print(\"=== GPT-2 WITH few-shot examples ===\")\n",
    "print(output_few[len(few_shot):])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698e29d9",
   "metadata": {},
   "source": [
    "### Llama 3.2: Using few-shot to control output format\n",
    "\n",
    "Unlike GPT-2, Llama 3.2 is instruction-tuned and already produces reasoning traces by default. So what's the point of few-shot examples?\n",
    "\n",
    "**The power of few-shot with instruction-tuned models is controlling the output format.** We can make the model follow a specific structure like `[GIVEN]/[FIND]/[SOLVE]/[ANSWER]` that it wouldn't use naturally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8af711f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== WITHOUT few-shot examples ===\n",
      "To find the area, we need to first find the dimensions of the rectangle.\n",
      "\n",
      "Let's call the width \"w\" and the length \"2w\" since it's twice the width. The perimeter of a rectangle is given by:\n",
      "\n",
      "Perimeter = 2(length + width)\n",
      "36 = 2(2w + w)\n",
      "\n",
      "Combine like terms:\n",
      "36 = 6w\n",
      "\n",
      "Divide both sides by 6:\n",
      "w = 6\n",
      "\n",
      "So, the width is 6 cm and the length is 2w = 12 cm.\n",
      "\n",
      "Now that we have the dimensions, we can find the area of the rectangle:\n",
      "\n",
      "Area = Length x Width\n",
      "= 12 x 6\n",
      "= 72\n",
      "\n",
      "The area of the rectangle is 72 square centimeters.\n",
      "\n",
      "=== WITH few-shot examples ===\n",
      "To solve this problem, we need to find the dimensions of the rectangle first.\n",
      "\n",
      "Let's denote the width as W and the length as L. We are given that the length is twice the width, so:\n",
      "\n",
      "L = 2W\n",
      "\n",
      "We also know that the perimeter of a rectangle is given by:\n",
      "\n",
      "Perimeter = 2L + 2W\n",
      "\n",
      "Substituting L = 2W into the equation above, we get:\n",
      "\n",
      "36 = 2(2W) + 2W\n",
      "36 = 4W + 2W\n",
      "36 = 6W\n",
      "\n",
      "Now, divide both sides by 6 to find W:\n",
      "\n",
      "W = 36/6\n",
      "W = 6 cm\n",
      "\n",
      "Since L = 2W, we can now find the length:\n",
      "\n",
      "L = 2(6)\n",
      "L = 12 cm\n",
      "\n",
      "The area of a rectangle is given by:\n",
      "\n",
      "Area = Length × Width\n",
      "= 12 × 6\n",
      "= 72 square centimeters\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"ollama\", base_url=\"http://localhost:11434/v1\")\n",
    "MODEL = \"llama3.2:3b\"\n",
    "\n",
    "question = \"A rectangle has a perimeter of 36 cm. If the length is twice the width, what is the area?\"\n",
    "\n",
    "# --- Without few-shot examples (model's default format) ---\n",
    "response_zero = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[{\"role\": \"user\", \"content\": question}],\n",
    "    temperature=0.7\n",
    ")\n",
    "print(\"=== WITHOUT few-shot examples ===\")\n",
    "print(response_zero.choices[0].message.content)\n",
    "print()\n",
    "\n",
    "# --- With few-shot examples (enforcing a specific format) ---\n",
    "few_shot_examples = \"\"\"Q: A store sells apples for $2 each. If I buy 3 apples and pay with a $10 bill, how much change do I get?\n",
    "A: [GIVEN] apples cost $2, buying 3, paying with $10\n",
    "[FIND] change received\n",
    "[SOLVE] total = 3 x $2 = $6; change = $10 - $6 = $4\n",
    "[ANSWER] $4\n",
    "\n",
    "Q: A train leaves at 9:15 AM and the journey takes 2 hours 30 minutes. What time does it arrive?\n",
    "A: [GIVEN] departure 9:15 AM, duration 2h 30m\n",
    "[FIND] arrival time\n",
    "[SOLVE] 9:15 + 2:30 = 11:45\n",
    "[ANSWER] 11:45 AM\n",
    "\"\"\"\n",
    "\n",
    "prompt = few_shot_examples + f\"Q: {question}\\nA:\"\n",
    "response_few = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=0.7\n",
    ")\n",
    "print(\"=== WITH few-shot examples ===\")\n",
    "print(response_few.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adee0e7",
   "metadata": {},
   "source": [
    "### 1.2: Zero‑Shot Chain‑of‑Thought\n",
    "Zero-shot CoT encourages the model to reason without examples by adding a short cue such as “Let’s think step by step.” This simple phrase often activates the model’s latent reasoning ability even when no demonstrations are provided. It serves as a baseline to compare with few-shot and other inference-time scaling methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c444eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To understand why neural networks are used to build Large Language Models (LLMs), let's break down the process step by step:\n",
      "\n",
      "1. **Understanding the Problem**: The primary goal of building an LLM is to create a model that can generate human-like text, answer questions, or perform other natural language processing tasks.\n",
      "\n",
      "2. **Traditional Approaches**: Before neural networks, traditional approaches to NLP involved rule-based systems and statistical models. These methods were limited in their ability to handle complex linguistic structures and nuances of human language.\n",
      "\n",
      "3. **The Rise of Neural Networks**: In the 1980s and 1990s, researchers began exploring the use of neural networks for NLP tasks. The key innovation was the development of recurrent neural networks (RNNs) and long short-term memory (LSTM) networks, which allowed models to capture sequential dependencies in language.\n",
      "\n",
      "4. **Why Neural Networks?**: Neural networks are particularly well-suited for LLMs because they can:\n",
      "   - Learn complex patterns in language data\n",
      "   - Handle sequential dependencies between words or tokens\n",
      "   - Capture contextual relationships and nuances of human language\n",
      "\n",
      "5. **Key Architectures**: The most common architectures used in LLMs are:\n",
      "   - Recurrent Neural Networks (RNNs)\n",
      "   - Long Short-Term Memory (LSTM) networks\n",
      "   - Transformers, which have become the de facto standard for modern LLMs\n",
      "\n",
      "6. **Why Transformers?**: Transformers revolutionized the field of NLP by introducing self-attention mechanisms, which allow models to weigh the importance of different tokens in a sentence relative to each other. This leads to better performance and more efficient training.\n",
      "\n",
      "7. **Training Large Models**: The availability of large amounts of labeled data (e.g., text corpora) has enabled researchers to train massive neural networks that can capture complex patterns in language. These models are often trained using techniques like masked language modeling, next sentence prediction, or other supervised learning objectives.\n",
      "\n",
      "8. **Advantages of Neural Networks**: The use of neural networks for LLMs offers several advantages:\n",
      "   - Ability to learn from large amounts of data\n",
      "   - Flexibility and adaptability in handling different linguistic tasks\n",
      "   - Potential for significant improvements over traditional NLP approaches\n",
      "\n",
      "In summary, the use of neural networks for building LLMs is driven by their ability to capture complex patterns in language, handle sequential dependencies, and learn from large amounts of data. The development of architectures like RNNs, LSTMs, and transformers has enabled researchers to create models that can generate human-like text or perform other NLP tasks with high accuracy.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Step 1: Write the question and a zero-shot CoT cue (e.g., \"Let's think step by step.\")\n",
    "# Step 2: Build a single prompt string that includes brief role guidance plus the question\n",
    "# Step 3: Call your Ollama or OpenAI client to get a response from llama3.2:3b  # e.g., client.chat.completions.create(...)\n",
    "# Step 4: Print the chain and the final answer\n",
    "\n",
    "client = OpenAI(api_key = \"ollama\", base_url = \"http://localhost:11434/v1\")\n",
    "\n",
    "question = \"Why do we use neural network to build LLMs?\"\n",
    "\n",
    "prompt = f\"\"\"You are a knowledgeable tutor. Answer the question. \n",
    "Question: {question}\n",
    "Let's think step by step.\"\"\"\n",
    "\n",
    "MODEL = \"llama3.2:3b\"\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[{\"role\":\"user\",\"content\": prompt}],\n",
    "    temperature=0\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686708da",
   "metadata": {},
   "source": [
    "### 1.3 Self‑Consistency\n",
    "Self-consistency enhances reasoning accuracy by sampling multiple independent reasoning paths for the same question instead of relying on a single deterministic answer. Each run may follow a slightly different logical chain, and the diversity helps correct individual mistakes. After generating several reasoning traces, you then aggregate the final answers using majority voting.\n",
    "\n",
    "This approach is especially useful when tasks involve multi-step reasoning or arithmetic, where single-path outputs may be incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2fb325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sample 1 (Answer: $100) ===\n",
      "To find the original price of the item before the discount, we can start with the price after the discount and work our way backwards.\n",
      "\n",
      "Step 1: The item costs $80 after the discount and has a 20% off sale. Let's use this information to set up an equation.\n",
      "Let x be the original price of the item.\n",
      "Since there is a 20% discount, the discount amount can be represented as 0.2x (20% of x).\n",
      "The sale price after the discount can be calculated by subtracting the discount amount from the original price:\n",
      "\n",
      "...\n",
      "\n",
      "=== Sample 2 (Answer: None) ===\n",
      "To find the original price of the item before the discount, we need to calculate the amount of the discount and add it back to the final price.\n",
      "\n",
      "Step 1: Calculate the original price without considering the discount.\n",
      "Item costs $80 after a 20% off sale.\n",
      "\n",
      "Step 2: Apply the percentage change due to the discount:\n",
      "Let x be the value multiplied by the number 0.20 (or 20%) and add that value back into the final result of $80, set equal = 0.8x+ is  $80\n",
      "\n",
      "Step 3: Solve for x to find original price.\n",
      "First,...\n",
      "\n",
      "=== Sample 3 (Answer: 100) ===\n",
      "To find the original price of the item before the discount, we can follow these steps:\n",
      "\n",
      "Step 1: Let's denote the original price as x.\n",
      "\n",
      "Step 2: During the 20% off sale, 20% of the original price is discounted from the original price.\n",
      "\n",
      "Step 3: The amountDiscounted = 20% of x = (20/100) * x = 0.2x\n",
      "\n",
      "Step 4: Since the item costs $80 after the discount, we know that the original price minus the discounted amount equals the final cost:\n",
      "\n",
      "x - 0.2x = 80\n",
      "\n",
      "Step 5: Combine like terms to simplify the equation...\n",
      "\n",
      "=== Sample 4 (Answer: $100) ===\n",
      "To find the original price of the item before the discount, we can use the following steps:\n",
      "\n",
      "1. Let x be the original price of the item.\n",
      "2. The store offers a 20% off sale, which means that the item is sold at 80% of its original price (100% - 20% = 80%).\n",
      "3. Therefore, the equation representing the situation is: 0.8x = $80 (where 0.8 represents 80% and x is the original price)\n",
      "4. To solve for x, we can divide both sides of the equation by 0.8: x = $80 / 0.8\n",
      "5. Calculating this value gives us x =...\n",
      "\n",
      "=== Sample 5 (Answer: 100) ===\n",
      "To find the original price of the item before the discount, we can use the following steps:\n",
      "\n",
      "Step 1: Understand that the discounted price is 80% of the original price (since 100% - 20% = 80%).\n",
      "\n",
      "Step 2: Set up an equation to represent the relationship between the original price and the discounted price. Let x be the original price:\n",
      "0.8x = discounted price\n",
      "\n",
      "We know that the discounted price is $80, so we can set up the equation as follows:\n",
      "\n",
      "0.8x = 80\n",
      "\n",
      "Step 3: Solve for x by dividing both sides of ...\n",
      "\n",
      "==================================================\n",
      "Votes: Counter({'$100': 2, '100': 2, None: 1})\n",
      "Chosen answer: $100\n",
      "Correct answer: $100\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import re, collections\n",
    "\n",
    "client = OpenAI(api_key=\"ollama\", base_url=\"http://localhost:11434/v1\")\n",
    "MODEL = \"llama3.2:3b\"\n",
    "\n",
    "\n",
    "def cot_answer(question, temperature=1.2):\n",
    "    prompt = f\"\"\"Answer the following question with step-by-step reasoning.\n",
    "End your answer with exactly: \"Therefore, the answer is [X]\" where [X] is just the final answer.\n",
    "\n",
    "Question: {question}\n",
    "Let's think step by step.\"\"\"\n",
    "    \n",
    "    r = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature\n",
    "    )\n",
    "\n",
    "    content = r.choices[0].message.content\n",
    "    match = re.search(r\"the answer is[:\\s]*([^\\n\\.]+)\", content, re.IGNORECASE)\n",
    "    return content, match.group(1).strip() if match else None\n",
    "\n",
    "\n",
    "def self_consistent(question, n=5):\n",
    "    traces = []\n",
    "    answers = []\n",
    "    for i in range(n):\n",
    "        reasoning, ans = cot_answer(question)\n",
    "        traces.append((i + 1, reasoning, ans))\n",
    "        answers.append(ans)\n",
    "    \n",
    "    counter = collections.Counter(answers)\n",
    "    winner, _ = counter.most_common(1)[0]\n",
    "    return winner, counter, traces\n",
    "\n",
    "\n",
    "question = \"A store has a 20% off sale. If an item costs $80 after the discount, what was the original price?\"\n",
    "\n",
    "winner, counter, traces = self_consistent(question, n=5)\n",
    "\n",
    "# Print each reasoning trace\n",
    "for i, reasoning, ans in traces:\n",
    "    print(f\"=== Sample {i} (Answer: {ans}) ===\")\n",
    "    print(reasoning[:500] + \"...\" if len(reasoning) > 500 else reasoning)\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Votes:\", counter)\n",
    "print(\"Chosen answer:\", winner)\n",
    "print(\"Correct answer: $100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bea715",
   "metadata": {},
   "source": [
    "### 1.4: Sequential Revision\n",
    "\n",
    "Sequential revision iteratively improves an answer by generating a first draft, critiquing it, and producing revised drafts that condition on prior answers. Each round should be short and focused, so improvements accumulate without drifting from the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e5859d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"ollama\", base_url=\"http://localhost:11434/v1\")\n",
    "MODEL = \"llama3.2:3b\"\n",
    "\n",
    "\n",
    "def sequential_revision(question: str, max_steps: int = 3) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant. Keep your answers clear and correct.\"},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "   \n",
    "    draft = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=messages,\n",
    "        temperature=0.7,\n",
    "    ).choices[0].message.content.strip()\n",
    "    print(f\"Draft 1: {draft}\")\n",
    "\n",
    "    # Iterative revision\n",
    "    for idx in range(1, max_steps):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Improve answers by making them clearer and more accurate.\"},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "            {\"role\": \"assistant\", \"content\": draft},\n",
    "            {\"role\": \"user\", \"content\": \"Please revise your answer. Make it clearer, more accurate, and better written. Only include the new answer.\"}\n",
    "        ]\n",
    "        draft = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=messages,\n",
    "            temperature=0.7,\n",
    "        ).choices[0].message.content.strip()\n",
    "        print(f\"Draft {idx+1}: {draft}\")\n",
    "\n",
    "    return draft\n",
    "\n",
    "\n",
    "output = sequential_revision(\"If a rectangle is twice as long as it is wide and the perimeter is 30 cm, what is the area?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9319ee8",
   "metadata": {},
   "source": [
    "## 1.5 Tree-of-Thoughts\n",
    "\n",
    "Tree-of-Thoughts (ToT) reframes reasoning as a search problem. Instead of generating one linear chain of thoughts, the model:\n",
    "1. Generates multiple candidate \"thoughts\" at each step\n",
    "2. Evaluates how promising each thought is\n",
    "3. Expands only the best candidates (beam search)\n",
    "4. Backtracks if needed\n",
    "\n",
    "This mirrors how humans solve hard problems: brainstorm options, evaluate them, pursue the best, and backtrack when stuck."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cpugbki39kv",
   "metadata": {},
   "source": [
    "### Example 1: Word Ladder (Algorithmic ToT)\n",
    "\n",
    "This example shows ToT as pure beam search without LLM calls. Each \"thought\" is a candidate word that differs by one letter. We score by edit distance to goal and keep the best candidates.\n",
    "\n",
    "This demonstrates the **core algorithm** behind ToT: expand, score, prune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d047801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "['hit', 'hot', 'dot', 'dog', 'cog']\n"
     ]
    }
   ],
   "source": [
    "###### Word Ladder Puzzle ##########\n",
    "\n",
    "def neighbors(word, vocabulary):\n",
    "    for i, c1 in enumerate(word):\n",
    "        for c2 in 'abcdefghijklmnopqrstuvwxyz':\n",
    "            if c1 != c2:\n",
    "                candidate = word[:i] + c2 + word[i+1:]\n",
    "                if candidate in vocabulary:\n",
    "                    yield candidate\n",
    "\n",
    "\n",
    "def tree_of_thought(start, goal, vocab, max_depth=5, beam_width=4):\n",
    "    frontier = [[start]]\n",
    "    for depth in range(max_depth):\n",
    "        candidates = []\n",
    "        for path in frontier:\n",
    "            for nxt in neighbors(path[-1], vocab):\n",
    "                if nxt in path:  # avoid loops\n",
    "                    continue\n",
    "                candidates.append(path + [nxt])\n",
    "        # score: negative edit distance to goal\n",
    "        scored = sorted(candidates, key=lambda p: sum(a!=b for a,b in zip(p[-1], goal)))\n",
    "        frontier = scored[:beam_width]\n",
    "        if any(p[-1] == goal for p in frontier):\n",
    "            return [p for p in frontier if p[-1]==goal][0]\n",
    "    return None\n",
    "\n",
    "\n",
    "vocab = {\"hit\",\"dot\",\"cog\",\"log\",\"dog\",\"lot\",\"lit\",\"hot\"}\n",
    "print(tree_of_thought(\"hit\", \"cog\", vocab))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k84w1nz2u6p",
   "metadata": {},
   "source": [
    "### Example 2: Generic ToT for Open-Ended Problems\n",
    "\n",
    "For open-ended problems without verifiable answers, we can still apply ToT by having the LLM both propose and evaluate thoughts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89067302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best solution (score 8):\n",
      "Here are two potential next steps:\n",
      "\n",
      "1. **Define the Workshop Theme and Objectives**: Develop a clear theme and set of objectives for the weekend science workshop. This will help guide the planning process and ensure that all activities and experiments align with the goals of engaging and educating the participants (12-year-olds). Some possible themes could include:\n",
      " * \"Environmental Science\" (focusing on sustainability, conservation, and ecology)\n",
      " * \"Physics and Engineering\" (exploring simple machines, circuits, and robotics)\n",
      " * \"Chemistry and Materials\" (introducing basic chemistry concepts through hands-on experiments)\n",
      "\n",
      "By defining a clear theme and set of objectives, we can start to generate ideas for activities, experiments, and resources that will make the workshop engaging and effective.\n",
      "\n",
      "2. **Identify a Venue and Logistics**: Secure a suitable venue for the weekend science workshop, taking into account factors such as:\n",
      " * Availability (dates and times)\n",
      " * Space requirements (number of participants, equipment and materials needed)\n",
      " * Accessibility and safety (is the venue easily accessible, is it safe for children to be around certain materials or situations?)\n",
      " * Cost and budget constraints\n",
      " * Equipment and resource availability\n",
      "\n",
      "Once the venue is secured, we can start planning logistics such as:\n",
      " * Transportation (if necessary)\n",
      " * Food and drink arrangements\n",
      " * Staffing and volunteer needs\n",
      " * Activities and schedules\n",
      "Here are two additional potential next steps:\n",
      "\n",
      "3. **Develop Curricula and Activity Plans**: Create detailed curricula and activity plans for the weekend science workshop, aligning with the defined theme and objectives. This will help ensure that all activities and experiments are well-structured, engaging, and meet the learning goals.\n",
      "\n",
      "* Create a detailed outline of each day's schedule, including:\n",
      " + Morning icebreaker or introductory activity\n",
      " + Main experiment or hands-on activity\n",
      " + Lunch break and time for socialization\n",
      " + Afternoon wrap-up or review activity\n",
      "* Develop lesson plans and materials for each activity, including necessary equipment, resources, and safety protocols\n",
      "* Consider incorporating games, simulations, or competitions to make learning fun and interactive\n",
      "\n",
      "4. **Establish Partnerships and Resource Sourcing**: Identify potential partners, suppliers, or organizations that can support the weekend science workshop, providing resources, expertise, or services. This could include:\n",
      "\n",
      "* Museum or science center partnerships for access to equipment, expertise, or facilities\n",
      "* Local businesses or industry experts for guest lectures, tours, or hands-on learning experiences\n",
      "* Online resources or educational platforms for additional materials and support\n",
      "* Donations of goods or services from local organizations or individuals\n",
      "\n",
      "By establishing partnerships and sourcing resources, we can enhance the workshop experience, increase its impact, and reduce costs.\n"
     ]
    }
   ],
   "source": [
    "###### Generic ToT Search ##########\n",
    "\n",
    "import re\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"ollama\", base_url=\"http://localhost:11434/v1\")\n",
    "MODEL = \"llama3.2:3b\"\n",
    "\n",
    "def propose_thoughts(question, state, k=2):\n",
    "    prompt = f\"\"\"You are exploring solutions.\n",
    "Problem: {question}\n",
    "Current partial solution: {state}\n",
    "\n",
    "Propose {k} different next thoughts, numbered 1 to {k}.\"\"\"\n",
    "    \n",
    "    r = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.9,\n",
    "    )\n",
    "    # Split response into separate thoughts\n",
    "    return [r.choices[0].message.content.strip()]\n",
    "\n",
    "\n",
    "def score_state(question, state):\n",
    "    prompt = f\"\"\"Problem: {question}\n",
    "Rate from 1-10 how promising this partial solution is: {state}\n",
    "Reply with just a number.\"\"\"\n",
    "    \n",
    "    r = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    nums = re.findall(r\"\\d+\", r.choices[0].message.content)\n",
    "    return int(nums[0]) if nums else 5\n",
    "\n",
    "\n",
    "def tree_of_thoughts(question, depth=2, width=2):\n",
    "    frontier = [(\"\", 0)]\n",
    "    for _ in range(depth):\n",
    "        new_frontier = []\n",
    "        for state, _ in frontier:\n",
    "            for thought in propose_thoughts(question, state, k=width):\n",
    "                new_state = (state + \"\\n\" + thought).strip()\n",
    "                score = score_state(question, new_state)\n",
    "                new_frontier.append((new_state, score))\n",
    "        new_frontier.sort(key=lambda x: x[1], reverse=True)\n",
    "        frontier = new_frontier[:width]\n",
    "    best_state, best_score = frontier[0]\n",
    "    return best_state, best_score\n",
    "\n",
    "\n",
    "question = \"Design a plan for a weekend science workshop for 12-year-olds.\"\n",
    "solution, score = tree_of_thoughts(question)\n",
    "\n",
    "print(f\"Best solution (score {score}):\\n{solution}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dc38f6",
   "metadata": {},
   "source": [
    "---  \n",
    "# 3‑ Training Models for Reasoning\n",
    "\n",
    "### 3.1: CoT Training\n",
    "Chain-of-Thought (CoT) training conditions the model on explicit rationales during fine-tuning. Instead of teaching the model to output only the final answer, we train on (question, rationale, answer) so the model learns to internalize multi-step reasoning patterns. A practical recipe is STaR (Self-Taught Reasoner), which uses a stronger teacher model to bootstrap rationales that a smaller student can learn from.\n",
    "\n",
    "For tasks that require multi-hop reasoning, models fine-tuned on rationales often achieve higher accuracy and are more stable at inference time than models trained on direct answers only. \n",
    "\n",
    "Training a full language model is beyond the scope of this notebook, but here is the high-level workflow followed by a short pseudocode:\n",
    "- Collect questions: Prepare a dataset of questions and correct answers.\n",
    "- Generate rationales: Use a strong LLM to produce step-by-step reasoning ending with the correct answer.\n",
    "- Filter and clean: Discard incorrect or low-quality rationales.\n",
    "- Prepare training data: Format triples (question, rationale, answer) for supervised fine-tuning.\n",
    "- Fine-tune: Fine-tune the LLM on rationales.\n",
    "- Iterate: Refine prompts, improve data quality, and retrain for stronger reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb7cfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudocode (STaR loop)\n",
    "# for round in 1 ... iters:\n",
    "    # STEP 1: self-generate reasoning (teacher creates rationale + answer)\n",
    "    # STEP 2: keep only correct, high-quality traces\n",
    "    # STEP 3: fine-tune student on (question, rationale, answer) data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b53c70",
   "metadata": {},
   "source": [
    "### 3.2: ORM vs PRM + RL\n",
    "Training a Reward Model (RM) allows large language models to be improved through reinforcement learning (RL). Instead of fine-tuning directly on examples, we train a separate model that can score or rank model outputs, and use those scores as feedback signals to refine the policy model.\n",
    "\n",
    "Two main reward modeling approaches are ORM (predicts a scalar reward for the final answer) and PRM (evaluates the reasoning steps instead of just the outcome)\n",
    "\n",
    "\n",
    "\n",
    "| Approach | Typical loss | When to use |\n",
    "|-----------|-------------|-------------|\n",
    "|*Outcome Reward Model* | Predict scalar reward | Easy to collect training data using verifiers |\n",
    "|*Process Reward Model* | Predict rewards per step | Difficult to collect training data but more accurate |\n",
    "| *RLHF* | Use RM as reward in **RL** fine‑tuning | Aligns policy with human signals | Aligns model policy with human or synthetic preferences\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595635aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for round = 1 ... iters:\n",
    "    # STEP 1:  Generate reasoning\n",
    "        # sample a minibatch of questions\n",
    "        # policy roll‑out (actions + log‑probs)\n",
    "    # STEP 2:  Score the trajectory\n",
    "        # ORM: scalar reward for the final answer / PRM: scalar reward for the thought process\n",
    "    # STEP 3:  Reinforce the policy (PPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2e7757",
   "metadata": {},
   "source": [
    "### 3.3 Inspect a reasoning model\n",
    "\n",
    "Now that we've discussed how reasoning models are trained, let's see one in action. We'll use **DeepSeek-R1**, a reasoning model that produces explicit *thinking tokens* before giving its final answer. The model wraps its internal chain-of-thought inside `<think>...</think>` tags, followed by a clean final response.\n",
    "\n",
    "In the cell below we send a question to DeepSeek-R1 and parse the output to separate:\n",
    "- **Thinking tokens** — the model's internal reasoning process (hidden from the end user in production).\n",
    "- **Final answer** — the polished response the user actually sees.\n",
    "\n",
    "We use `deepseek-r1:1.5b` here for speed. You can switch to `deepseek-r1:8b` for higher-quality reasoning, but it will take longer to run. Pull whichever variant you want to try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfe0db7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "THINKING TOKENS (internal reasoning)\n",
      "============================================================\n",
      "Okay, so I have this problem about a rectangle with a perimeter of 36 cm. It says that the length is twice the width, and I need to find the area. Hmm, let me think step by step how to approach this.\n",
      "\n",
      "First off, I remember that the perimeter of a rectangle is calculated using the formula: P = 2*(length + width). Since they've given the perimeter as 36 cm, I can plug that into the equation.\n",
      "\n",
      "But before I do that, maybe I should define variables for length and width to make it clearer. Let me call the width 'w'. Then, since the length is twice the width, the length would be '2w'. That makes sense because they said length is twice as long as the width.\n",
      "\n",
      "So now, substituting these into the perimeter formula: 36 = 2*(length + width) which becomes 36 = 2*(2w + w). Let me write that out:\n",
      "\n",
      "36 = 2*(2w + w)\n",
      "\n",
      "Simplify inside the parentheses first. 2w plus w is 3w, so now it's:\n",
      "\n",
      "36 = 2*3w\n",
      "\n",
      "Which simplifies to:\n",
      "\n",
      "36 = 6w\n",
      "\n",
      "Hmm, okay, so if I divide both sides by 6, that should give me the value of 'w'. Let me calculate that.\n",
      "\n",
      "Dividing 36 by 6 gives w = 6. So the width is 6 cm. Then, since the length is twice that, it's 2*6 = 12 cm. Got it: width is 6 cm and length is 12 cm.\n",
      "\n",
      "Now, to find the area of a rectangle, I remember the formula is A = length * width. So plugging in the values we have:\n",
      "\n",
      "A = 12 cm * 6 cm\n",
      "\n",
      "Multiplying those together gives 72 cm². Hmm, let me double-check that multiplication: 10*6 is 60 and 2*6 is 12, so adding them up gives 72. Yep, that seems right.\n",
      "\n",
      "Let me just verify everything again to make sure I didn't make any mistakes. The perimeter was given as 36 cm, which we plugged into the formula correctly. We set width as 'w' and length as 2w, then solved for 'w', got 6, so length is 12. Multiplying them gives area of 72 cm².\n",
      "\n",
      "Wait a second, just to be thorough, let's make sure that plugging back into the perimeter formula works. So if width is 6 and length is 12:\n",
      "\n",
      "Perimeter = 2*(length + width) = 2*(12 + 6) = 2*18 = 36 cm.\n",
      "\n",
      "Yes, that matches the given perimeter. So all checks out!\n",
      "\n",
      "I think I've got this right then. The area of the rectangle is 72 square centimeters.\n",
      "\n",
      "**Final Answer**\n",
      "The area of the rectangle is \\boxed{72} square centimeters.\n",
      "\n",
      "============================================================\n",
      "FINAL ANSWER (returned to the user)\n",
      "============================================================\n",
      "Given a rectangle with a perimeter of 36 cm, where the length is twice the width, we need to find the area.\n",
      "\n",
      "First, define the variables:\n",
      "- Let \\( w \\) be the width.\n",
      "- The length will then be \\( 2w \\).\n",
      "\n",
      "Using the perimeter formula for a rectangle:\n",
      "\\[ P = 2 \\times (\\text{length} + \\text{width}) \\]\n",
      "Substitute the given values and expressions:\n",
      "\\[ 36 = 2 \\times (2w + w) \\]\n",
      "Simplify inside the parentheses:\n",
      "\\[ 36 = 2 \\times 3w \\]\n",
      "\\[ 36 = 6w \\]\n",
      "Solve for \\( w \\):\n",
      "\\[ w = \\frac{36}{6} = 6 \\text{ cm} \\]\n",
      "\n",
      "Then, calculate the length:\n",
      "\\[ \\text{Length} = 2w = 2 \\times 6 = 12 \\text{ cm} \\]\n",
      "\n",
      "To find the area, use the formula \\( A = \\text{length} \\times \\text{width} \\):\n",
      "\\[ A = 12 \\times 6 = 72 \\text{ cm}^2 \\]\n",
      "\n",
      "The final answer is:\n",
      "\\[\n",
      "\\boxed{72}\n",
      "\\]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"ollama\", base_url=\"http://localhost:11434/v1\")\n",
    "model = \"deepseek-r1:1.5b\"\n",
    "\n",
    "question = \"A rectangle has a perimeter of 36 cm. If the length is twice the width, what is the area?\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[{\"role\": \"user\", \"content\": question}],\n",
    "    temperature=0.6,\n",
    ")\n",
    "\n",
    "raw_output = response.choices[0].message.content\n",
    "\n",
    "# DeepSeek-R1 wraps its reasoning in <think>...</think> tags\n",
    "think_match = re.search(r\"<think>(.*?)</think>\", raw_output, re.DOTALL)\n",
    "thinking = think_match.group(1).strip() if think_match else \"(no thinking tokens found)\"\n",
    "final_answer = re.sub(r\"<think>.*?</think>\", \"\", raw_output, flags=re.DOTALL).strip()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"THINKING TOKENS (internal reasoning)\")\n",
    "print(\"=\" * 60)\n",
    "print(thinking)\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL ANSWER (returned to the user)\")\n",
    "print(\"=\" * 60)\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545a81a6",
   "metadata": {},
   "source": [
    "---  \n",
    "# 4‑ A Deep Research Agent\n",
    "\n",
    "A deep-research agent pairs a reasoning model with external tools for web search and retrieval. We will follow the ReAct pattern: the model writes short thoughts, decides when to call tools, reads observations, and continues reasoning until it can answer or reaches a step limit.\n",
    "\n",
    "We now combine a **search tool** with an LLM in a multi-step setup. We follow the *ReAct* pattern (reason → tool → observation):\n",
    "\n",
    "1. The model reasons and decides to use tools\n",
    "2. The agent searches and feeds condensed snippets back as context\n",
    "3. Iterate until the model answers or hits a step limit\n",
    "\n",
    "We use `create_agent` from `langchain.agents`, which builds a ReAct-style agent graph. Note: the agent model must support **tool calling** (e.g., `llama3.2:3b`). Models like `deepseek-r1` are reasoning models that do not support native tool calling and cannot be used directly as the agent LLM. We can stick to the `llama3.2:3b` or `qwen2.5:3b-instruct` for this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd1e1648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddgs import DDGS\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def ddg_search(query: str, k: int = 5) -> str:\n",
    "    \"\"\"Basic DuckDuckGo web search that returns a concatenated text snippet.\"\"\"\n",
    "    with DDGS() as ddgs:\n",
    "        results = [hit[\"body\"] for hit in ddgs.text(query, max_results=k)]\n",
    "    return \"\\n\".join(results)\n",
    "\n",
    "\n",
    "search_tool = ddg_search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418a0d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the information provided by DuckDuckGo search:\n",
      "\n",
      "1. There's an upcoming course titled \"MachineLearning for Bioinformatics & Systems Biology\" that might be relevant to learning about machine learning resources in 2025.\n",
      "\n",
      "Additionally, there are several topics related to machine learning techniques and their applications:\n",
      "- Digital marketers use machine learning to analyze user behavior on websites.\n",
      "- Cross-validation is a technique used in deep learning models like Keras, PyTorch, and MxNet. It helps evaluate the performance of these models by splitting data into training and validation sets multiple times.\n",
      "\n",
      "For resources related to machine learning in 2025 or beyond, you might want to look for more specific courses or events that are scheduled for that year. The course mentioned could be a good starting point if it aligns with your interests and goals.\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "MODEL = \"qwen2.5:3b-instruct\"\n",
    "\n",
    "question = \"What are the best resources to learn machine learning in 2025?\"\n",
    "\n",
    "# Step 1: Initialize the LLM via ChatOllama (must support tool calling)\n",
    "llm = ChatOllama(model=MODEL, temperature=0.2)\n",
    "\n",
    "# Step 2: Build a tool-calling agent with DuckDuckGo search\n",
    "agent = create_agent(llm, tools=[search_tool])\n",
    "\n",
    "# Step 3: Ask a query and let the agent search + reason to produce an answer\n",
    "result = agent.invoke({\"messages\": [(\"user\", question)]})\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b1c3f7",
   "metadata": {},
   "source": [
    "# 5- (Optional) Multi-Agent Deep Research\n",
    "\n",
    "Instead of a single agent, we can design multiple collaborating agents that work in parallel:\n",
    "\n",
    "1. **Planner**: Analyzes the query and breaks it into sub-questions\n",
    "2. **Researchers**: Run in parallel, each searching and summarizing findings for one sub-question  \n",
    "3. **Synthesizer**: Combines all research into a coherent final report\n",
    "\n",
    "This setup improves coverage and speed by parallelizing the research phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d59abf94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planning research for: What are the best resources to learn machine learning in 2025?\n",
      "\n",
      "Sub-questions (5):\n",
      "  - What are the top online courses for machine learning in 2025?\n",
      "  - What are some popular books on machine learning that were published in 2024 or later?\n",
      "  - What are some of the most influential researchers and their current projects in machine learning?\n",
      "  - How do different machine learning frameworks (e.g. TensorFlow, PyTorch) compare to each other in terms of ease of use and performance for beginners?\n",
      "  - What role do online communities (e.g. Kaggle, Reddit's r/MachineLearning) play in supporting the growth of machine learning practitioners in 2025?\n",
      "\n",
      "Researching in parallel...\n",
      "Collected 5 research summaries.\n",
      "\n",
      "Synthesizing final report...\n",
      "\n",
      "============================================================\n",
      "FINAL REPORT\n",
      "============================================================\n",
      "# Best Resources to Learn Machine Learning in 2025\n",
      "=====================================================\n",
      "\n",
      "As machine learning continues to evolve and become increasingly important across various industries, it's essential for individuals to stay up-to-date with the latest developments and best practices. This report provides an overview of the top resources for learning machine learning in 2025.\n",
      "\n",
      "## Top Online Courses for Machine Learning in 2025\n",
      "---------------------------------------------\n",
      "\n",
      "Several online courses are available that cater to different skill levels and interests, focusing on various aspects of machine learning such as algorithms, natural language processing, computer vision, neural networks, and certification programs. Some top resources include:\n",
      "\n",
      "*   **Free courses from top AI education platforms**: These courses provide a comprehensive introduction to machine learning concepts and techniques.\n",
      "*   **Courses with Google experts**: Interactive labs offer hands-on experience and guidance from industry experts in machine learning.\n",
      "*   **Generative AI courses by level**: Practical application-focused courses help learners develop skills for real-world projects.\n",
      "*   **Certification programs in machine learning**: Validate skills and knowledge with recognized certifications.\n",
      "\n",
      "## Popular Books on Machine Learning\n",
      "---------------------------------\n",
      "\n",
      "While no books were found published in 2024 or later, some popular titles from earlier years are worth mentioning:\n",
      "\n",
      "*   \"Deep Learning\" by Ian Goodfellow, Yoshua Bengio, and Aaron Courville (2016)\n",
      "*   \"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\" by Aurélien Géron (2017)\n",
      "*   \"Machine Learning Yearning: A Step-by-Step Guide to Building and Training Machine Learning Models\" by Jürgen Schmidhuber (2020)\n",
      "\n",
      "For recent developments in machine learning, consider checking online resources such as arXiv, ResearchGate, or Academia.edu for recent publications.\n",
      "\n",
      "## Influential Researchers and Their Current Projects\n",
      "---------------------------------------------\n",
      "\n",
      "Unfortunately, the provided search results do not contain specific information about influential researchers and their current projects. However, researching AI leaders with overlapping contributions may provide insights into their work.\n",
      "\n",
      "## Comparison of Machine Learning Frameworks (TensorFlow, PyTorch)\n",
      "-----------------------------------------------------------\n",
      "\n",
      "When it comes to ease of use and performance for beginners, both TensorFlow and PyTorch have their strengths and weaknesses:\n",
      "\n",
      "*   **PyTorch**: Offers a more user-friendly interface compared to TensorFlow. While not as performant as TensorFlow in some cases, its ease of use makes it an attractive option for those who value simplicity.\n",
      "*   **TensorFlow**: Has a steeper learning curve but superior performance capabilities.\n",
      "\n",
      "Keras is also worth mentioning due to its user-friendly interface, making it a viable alternative for beginners.\n",
      "\n",
      "## Online Communities Supporting Machine Learning Growth\n",
      "--------------------------------------------------------\n",
      "\n",
      "Online communities such as Kaggle and Reddit's r/MachineLearning play a significant role in supporting the growth of machine learning practitioners:\n",
      "\n",
      "*   **Sharing knowledge**: Platforms provide spaces for sharing expertise, stress testing ideas, and staying updated on the latest techniques.\n",
      "*   **Access to resources**: Community-published models, data, and code are valuable assets for practitioners looking to improve their skills or tackle new projects.\n",
      "\n",
      "By leveraging these top resources, individuals can stay up-to-date with the latest developments in machine learning and develop the necessary skills to become proficient by 2025.\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from openai import OpenAI\n",
    "from ddgs import DDGS\n",
    "\n",
    "client = OpenAI(api_key=\"ollama\", base_url=\"http://localhost:11434/v1\")\n",
    "MODEL = \"llama3.2:3b\"\n",
    "\n",
    "\n",
    "def plan_research(query: str) -> list[str]:\n",
    "    \"\"\"Planner agent: breaks query into sub-questions and decides scale (1, 3, or up to 5 sub-queries).\"\"\"\n",
    "    prompt = f\"\"\"You are a research planner. Given a query, break it into 1-5 focused sub-questions.\n",
    "- Simple factual queries: 1 sub-question\n",
    "- Moderate topics: 3 sub-questions  \n",
    "- Complex topics needing multiple angles: 5 sub-questions\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Return ONLY the sub-questions, one per line, no numbering or bullets.\"\"\"\n",
    "\n",
    "    r = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.3\n",
    "    )\n",
    "    lines = [line.strip() for line in r.choices[0].message.content.strip().split(\"\\n\") if line.strip()]\n",
    "    return lines[:5]  # cap at 5\n",
    "\n",
    "\n",
    "def search_and_summarize(sub_question: str) -> dict:\n",
    "    \"\"\"Researcher agent: searches web and summarizes findings for one sub-question.\"\"\"\n",
    "    # Search\n",
    "    with DDGS() as ddgs:\n",
    "        results = [hit[\"body\"] for hit in ddgs.text(sub_question, max_results=3)]\n",
    "    snippets = \"\\n\".join(results)\n",
    "    \n",
    "    # Summarize\n",
    "    prompt = f\"\"\"Based on these search results, write a concise summary answering: {sub_question}\n",
    "\n",
    "Search results:\n",
    "{snippets}\n",
    "\n",
    "Summary:\"\"\"\n",
    "\n",
    "    r = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.3\n",
    "    )\n",
    "    return {\"question\": sub_question, \"summary\": r.choices[0].message.content.strip()}\n",
    "\n",
    "\n",
    "def synthesize_report(query: str, findings: list[dict]) -> str:\n",
    "    \"\"\"Synthesizer agent: combines all findings into a final report.\"\"\"\n",
    "    findings_text = \"\\n\\n\".join([f\"### {f['question']}\\n{f['summary']}\" for f in findings])\n",
    "    \n",
    "    prompt = f\"\"\"You are a research synthesizer. Combine these findings into a coherent report.\n",
    "\n",
    "Original query: {query}\n",
    "\n",
    "Research findings:\n",
    "{findings_text}\n",
    "\n",
    "Write a well-structured report that answers the original query. Use markdown formatting.\"\"\"\n",
    "\n",
    "    r = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.4\n",
    "    )\n",
    "    return r.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "def deep_research(query: str) -> str:\n",
    "    \"\"\"Run the full multi-agent deep research pipeline.\"\"\"\n",
    "    print(f\"Planning research for: {query}\\n\")\n",
    "    \n",
    "    # Step 1: Plan\n",
    "    sub_questions = plan_research(query)\n",
    "    print(f\"Sub-questions ({len(sub_questions)}):\")\n",
    "    for sq in sub_questions:\n",
    "        print(f\"  - {sq}\")\n",
    "    \n",
    "    # Step 2: Research in parallel\n",
    "    print(\"\\nResearching in parallel...\")\n",
    "    with ThreadPoolExecutor(max_workers=len(sub_questions)) as executor:\n",
    "        findings = list(executor.map(search_and_summarize, sub_questions))\n",
    "    print(f\"Collected {len(findings)} research summaries.\")\n",
    "    \n",
    "    # Step 3: Synthesize\n",
    "    print(\"\\nSynthesizing final report...\\n\")\n",
    "    report = synthesize_report(query, findings)\n",
    "    return report\n",
    "\n",
    "\n",
    "# Run the multi-agent research\n",
    "query = \"What are the best resources to learn machine learning in 2025?\"\n",
    "report = deep_research(query)\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL REPORT\")\n",
    "print(\"=\" * 60)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9507d0a4",
   "metadata": {},
   "source": [
    "## 🎉 Congratulations!\n",
    "\n",
    "You have:\n",
    "* Practiced various inference-time reasoning methods (CoT, self-consistency, sequential revision, ToT)\n",
    "* Gained intuition about training reasoning models (STaR, ORM/PRM)\n",
    "* Built a **deep-research agent** with tool calling and ReAct-style reasoning\n",
    "* Implemented a **multi-agent system** with parallel research and report synthesis\n",
    "\n",
    "\n",
    "👏 **Great job!** Take a moment to celebrate. The techniques you implemented here power many production agents and chatbots."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
