{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe0fb30d",
   "metadata": {},
   "source": [
    "# Project 4: **Build a Deep Research System**\n",
    "Welcome to project 4! For this project, we shift our focus from tool use and agents to *reasoning* models. You will practice state‑of‑the‑art inference‑time scaling methods such as *Chain‑of‑Thought* prompting and *Tree‑of‑Thoughts*, and briefly explore high-level concepts of training reasoning models using techniques like **STaR**.\n",
    "\n",
    "\n",
    "Finally, you will put everything together to build a *deep research agent* that can browse the web, reason over what it finds, and give structured answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54845369",
   "metadata": {},
   "source": [
    "## Learning Objectives  \n",
    "* Apply common inference‑time scaling methods: **zero‑shot / few‑shot CoT, self‑consistency, sequential revision, tree‑of‑thoughts**  \n",
    "* Gain intuition for **training** reasoning‑capable models following **STaR** approach \n",
    "* Build a minimal **deep‑research agent** that combines step‑by‑step reasoning with live web search   \n",
    "* Practice extending deep-search to a multi-agent system "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a40a86",
   "metadata": {},
   "source": [
    "## Roadmap  \n",
    "0. Environment setup  \n",
    "1. Inference‑time scaling  \n",
    "  1.1 Few‑shot.   \n",
    "  1.2 Zero‑shot CoT.   \n",
    "  1.3 Self‑consistency.   \n",
    "  1.4 Sequential revisions.     \n",
    "  1.5 Tree‑of‑Thought (ToT)\n",
    "2. Training reasoning models and inspecting deepseek-r1 \n",
    "3. Deep-research agent  \n",
    "4. (Optional) Multi-agent deep-research"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17c2218",
   "metadata": {},
   "source": [
    "# 0- Environment setup\n",
    "\n",
    "### Step 1: Create your environment and install dependencies \n",
    "Before we start coding, you need a reproducible setup. Open a terminal in the same directory as this notebook, and use Conda or uv to install the project dependencies.\n",
    "\n",
    "#### Option 1: Conda\n",
    "```bash\n",
    "# Create and activate the conda environment\n",
    "conda env create -f environment.yaml && conda activate deep_research\n",
    "```\n",
    "\n",
    "#### Option 2: uv (Fast alternative)\n",
    "If you prefer [uv](https://docs.astral.sh/uv/) over Conda:\n",
    "\n",
    "```bash\n",
    "# Install uv (skip if already installed)\n",
    "curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "\n",
    "# Create a virtual environment and install dependencies\n",
    "uv venv .venv-deep-research && source .venv-deep-research/bin/activate\n",
    "uv pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "### Step 2: Register this environment as a Jupyter kernel\n",
    "```bash\n",
    "python -m ipykernel install --user --name=deep_research --display-name \"deep_research\"\n",
    "```\n",
    "Now open your notebook and switch to the `deep_research` kernel (Kernel → Change Kernel).\n",
    "\n",
    "### Step 3: Setup and run Ollama serve\n",
    "\n",
    "In this project we use the `llama3.2:3b`, `qwen2.5:3b-instruct` and `deepseek-r1:1.5b` models. You can try other smaller or larger reasoning LLMs such as `phi4-mini` to compare performance. Explore available models here: https://ollama.com/library.\n",
    "\n",
    "Open terminal and run ollama:\n",
    "```bash\n",
    "ollama serve\n",
    "```\n",
    "Then open another terminal and pull required models: \n",
    "```bash\n",
    "ollama pull llama3.2:3b\n",
    "ollama pull deepseek-r1:1.5b\n",
    "ollama pull qwen2.5:3b-instruct\n",
    "# Additional small reasoning models to compare\n",
    "# ollama pull phi4-mini\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e8d1b7",
   "metadata": {},
   "source": [
    "---  \n",
    "# 1‑ Inference‑time scaling\n",
    "\n",
    "Inference-time scaling refers to techniques that make an existing model reason better without retraining it. Instead of changing the model’s weights, we achieve reasoning capability by adjusting how we prompt, sample, or aggregate LLM's outputs.\n",
    "\n",
    "In this section, we’ll explore several inference-time strategies that improve reasoning quality using a non-reasoning base model. You will experiment with and compare methods such as:\n",
    "\n",
    "- Few-shot Chain-of-Thought (CoT)\n",
    "- Zero-shot CoT\n",
    "- Self-consistency\n",
    "- Sequential revision\n",
    "- Tree-of-Thoughts (ToT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081d499a",
   "metadata": {},
   "source": [
    "### 1.1: Few-Shot CoT\n",
    "\n",
    "Few-shot prompting provides examples before asking a new question. The model learns from the pattern and applies it to new inputs.\n",
    "\n",
    "We'll explore this with two models to understand how few-shot interacts with model capabilities:\n",
    "\n",
    "1. **GPT-2** (no instruction tuning): Doesn't reason by default. We'll see if few-shot examples can elicit reasoning.\n",
    "2. **Llama 3.2** (instruction-tuned): Already reasons naturally. We'll use few-shot to control the output format.\n",
    "\n",
    "#### GPT-2: Can few-shot examples elicit reasoning?\n",
    "\n",
    "GPT-2 is a base language model that just predicts the next token. It wasn't trained to follow instructions or reason step-by-step. Let's see what happens with and without few-shot examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173d73f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sect 1.1: GPT-2 Few-Shot reasoning\n",
    "import os, torch; from transformers import pipeline\n",
    "gen = pipeline('text-generation', model='gpt2')\n",
    "q = 'rectangle perimeter 36, length=2*width, area?'\n",
    "p = f'Q: triangle base 10, height 5, area? R: (10*5)/2=25. A: 25. \\nQ: {q}\\nR:'\n",
    "print(gen(p, max_new_tokens=50)[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698e29d9",
   "metadata": {},
   "source": [
    "#### Llama 3.2: Using few-shot to control output format\n",
    "\n",
    "Unlike GPT-2, Llama 3.2 is instruction-tuned and already produces reasoning traces by default. So what's the point of few-shot examples?\n",
    "\n",
    "**The power of few-shot with instruction-tuned models is controlling the output format.** We can make the model follow a specific structure like `[GIVEN]/[FIND]/[SOLVE]/[ANSWER]` that it wouldn't use naturally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8af711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sect 1.1: Llama 3.2 Few-Shot Control\n",
    "from openai import OpenAI; c = OpenAI(api_key='ollama', base_url='http://localhost:11434/v1')\n",
    "q = 'rectangle perimeter 36, length=2*width, area?'\n",
    "p = f'[GIVEN]: Square 4cm. [SOLVE]: 4*4=16. [ANSWER]: 16. \\n[GIVEN]: {q} \\n[SOLVE]:'\n",
    "print(c.chat.completions.create(model='llama3.2:3b', messages=[{'role':'user','content':p}]).choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adee0e7",
   "metadata": {},
   "source": [
    "### 1.2: Zero‑Shot Chain‑of‑Thought\n",
    "Zero-shot CoT encourages the model to reason without examples by adding a short cue such as “Let’s think step by step.” This simple phrase often activates the model’s latent reasoning ability even when no demonstrations are provided. It serves as a baseline to compare with few-shot and other inference-time scaling methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c444eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Section 1.2: Zero-Shot Chain-of-Thought ---\n",
    "# Adding 'Let's think step by step' activates the model's reasoning without examples.\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key='ollama', base_url='http://localhost:11434/v1')\n",
    "prompt = \"If a car travels 120 miles in 2h, how far in 5h? Let's think step by step.\"\n",
    "print(client.chat.completions.create(model='llama3.2:3b', messages=[{'role': 'user', 'content': prompt}]).choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686708da",
   "metadata": {},
   "source": [
    "### 1.3 Self‑Consistency\n",
    "Self-consistency enhances reasoning accuracy by sampling multiple independent reasoning paths for the same question instead of relying on a single deterministic answer. Each run may follow a slightly different logical chain, and the diversity helps correct individual mistakes. After generating several reasoning traces, you then aggregate the final answers using majority voting.\n",
    "\n",
    "This approach is especially useful when tasks involve multi-step reasoning or arithmetic, where single-path outputs may be incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2fb325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Section 1.3: Self-Consistency ---\n",
    "# Improving reliability by sampling multiple paths and using majority voting.\n",
    "from openai import OpenAI\n",
    "import re, collections\n",
    "client = OpenAI(api_key='ollama', base_url='http://localhost:11434/v1')\n",
    "def get_ans(q):\n",
    "    p = f\"{q}\\nThink step by step. End with 'Final Answer: <val>'\"\n",
    "    r = client.chat.completions.create(model='llama3.2:3b', messages=[{'role': 'user', 'content': p}], temperature=1.2).choices[0].message.content\n",
    "    m = re.search(r'Final Answer:\\s*(.*)', r, re.I)\n",
    "    return m.group(1).strip() if m else r[-10:]\n",
    "\n",
    "answers = [get_ans('Square root of 144?') for _ in range(5)]\n",
    "print('Votes:', collections.Counter(answers))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bea715",
   "metadata": {},
   "source": [
    "### 1.4: Sequential Revision\n",
    "\n",
    "Sequential revision iteratively improves an answer by generating a first draft, critiquing it, and producing revised drafts that condition on prior answers. Each round should be short and focused, so improvements accumulate without drifting from the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07e5859d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Draft 1: Quantum entanglement is a fundamental concept in quantum mechanics that describes the interconnectedness of two or more particles in such a way that their properties are correlated, regardless of the distance between them.\n",
      "\n",
      "In classical physics, it was thought that particles were independent and unaffected by each other's presence. However, in the early 20th century, physicists discovered that under certain conditions, particles could become \"entangled\" in a way that led to fascinating and unexpected effects.\n",
      "\n",
      "Entanglement occurs when two or more particles interact with each other in such a way that their quantum states become linked. This means that if something happens to one particle, it instantly affects the state of the connected particles, regardless of the distance between them.\n",
      "\n",
      "Here are some key features of entanglement:\n",
      "\n",
      "1. **Correlation**: Entangled particles exhibit correlated behavior, meaning that when the state of one particle is measured, it instantly affects the state of the other entangled particles.\n",
      "2. **Non-locality**: Entangled particles can be separated by large distances, and yet still be connected in such a way that instantaneous effects occur between them.\n",
      "3. **Quantum non-determinism**: Measuring the state of an entangled particle can lead to multiple possible outcomes, making it impossible to predict which outcome will occur.\n",
      "\n",
      "Some examples of entanglement include:\n",
      "\n",
      "1. **EPR paradox**: Albert Einstein, Boris Podolsky, and Nathan Rosen proposed a thought experiment in 1935 that challenged the concept of quantum non-locality. They described two particles entangled in such a way that measuring one particle instantly affected the state of the other particle.\n",
      "2. **Quantum teleportation**: In 1997, researchers demonstrated the possibility of teleporting information from one particle to another without physical transport of the particles themselves. This relies on the phenomenon of entanglement.\n",
      "3. **Entangled photons**: Researchers have successfully created and measured entangled photon pairs, which can be used for quantum communication, spectroscopy, and other applications.\n",
      "\n",
      "The importance of entanglement lies in its potential to enable:\n",
      "\n",
      "1. **Quantum computing**: Entangled particles are essential for creating quantum computing architectures that can solve complex problems exponentially faster than classical computers.\n",
      "2. **Quantum cryptography**: Entangled particles can be used to create secure communication channels, known as quantum keys, which are theoretically unbreakable.\n",
      "3. **Fundamental understanding of nature**: Studying entanglement and its implications continues to reveal new insights into the fundamental laws of physics.\n",
      "\n",
      "In summary, quantum entanglement is a fascinating phenomenon that showcases the interconnectedness of particles at the fundamental level. Its implications have far-reaching consequences in our understanding of the universe and the development of cutting-edge technologies.\n",
      "Draft 2: Here's a critique of the original explanation:\n",
      "\n",
      "**Strengths:**\n",
      "\n",
      "* The explanation clearly conveys the basic concept of quantum entanglement.\n",
      "* It provides some examples to illustrate its significance and applications.\n",
      "\n",
      "**Weaknesses:**\n",
      "\n",
      "* The language is somewhat simplistic, which may not be suitable for a more technical audience.\n",
      "* There's a lack of depth in explaining the underlying physics and implications of entanglement.\n",
      "* Some sections feel disconnected and could benefit from additional transitions or explanatory sentences.\n",
      "\n",
      "Here's an improved version:\n",
      "\n",
      "Quantum Entanglement: A Mysterious Phenomenon at the Heart of Quantum Mechanics\n",
      "\n",
      "In the realm of quantum mechanics, entanglement represents a fundamental puzzle waiting to be unraveled. This enigmatic phenomenon describes the interconnectedness of two or more particles in such a way that their properties are correlated, regardless of the distance between them.\n",
      "\n",
      "**The Core Concept:**\n",
      "\n",
      "Entanglement arises when two or more particles interact with each other in a way that generates a shared quantum state. This entangled state is characterized by correlations between the particles' wave functions, which describe the probability of observing specific measurement outcomes. If something happens to one particle, it instantly affects the state of the connected particles, regardless of the distance between them.\n",
      "\n",
      "**The Interplay Between Space and Time:**\n",
      "\n",
      "Entanglement highlights the peculiar nature of quantum mechanics, where space and time become intertwined. The No-Cloning Theorem states that a measurement performed on an entangled particle cannot reproduce its original wave function; instead, it creates an irreversible change in the system. This effect demonstrates the non-locality of entanglement, violating our classical intuition about causality.\n",
      "\n",
      "**Quantum Non-Determinism and Correlation:**\n",
      "\n",
      "Measuring the state of an entangled particle often yields many possible outcomes, which cannot be predicted without additional information. This phenomenon of quantum non-determinism is a direct result of entanglement's inherent correlations, forcing us to revise our understanding of classical notions such as determinism.\n",
      "\n",
      "**Applications and Significance:**\n",
      "\n",
      "Entanglement has far-reaching implications in various fields:\n",
      "\n",
      "1.  **Quantum Computing**: Entangled particles serve as the building blocks for quantum computing architectures that can solve complex problems exponentially faster than classical computers.\n",
      "2.  **Quantum Cryptography**: Entangled photons are used to create secure communication channels based on the principles of entanglement and no-cloning theorem.\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "Quantum entanglement embodies the captivating duality between space and time in quantum mechanics, challenging our understanding of fundamental laws governing reality. By delving deeper into this concept, researchers continue to uncover its secrets, pushing the boundaries of technology and inspiring further breakthroughs.\n",
      "\n",
      "I made several changes:\n",
      "\n",
      "1.  **More technical language:** I employed more precise terminology and concepts to improve the clarity and depth of the explanation.\n",
      "2.  **Added connections and transitions:** Sentences were reorganized to create a smooth narrative flow between ideas, reducing confusion and increasing readability.\n",
      "3.  **Incorporated examples with explanations:** Emphasis was placed on showcasing different aspects of quantum entanglement, such as its connection to the No-Cloning Theorem, alongside additional information to clarify their significance.\n",
      "\n",
      "These enhancements aim to make the subject more accessible while maintaining technical accuracy.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Here's a critique of the original explanation:\\n\\n**Strengths:**\\n\\n* The explanation clearly conveys the basic concept of quantum entanglement.\\n* It provides some examples to illustrate its significance and applications.\\n\\n**Weaknesses:**\\n\\n* The language is somewhat simplistic, which may not be suitable for a more technical audience.\\n* There's a lack of depth in explaining the underlying physics and implications of entanglement.\\n* Some sections feel disconnected and could benefit from additional transitions or explanatory sentences.\\n\\nHere's an improved version:\\n\\nQuantum Entanglement: A Mysterious Phenomenon at the Heart of Quantum Mechanics\\n\\nIn the realm of quantum mechanics, entanglement represents a fundamental puzzle waiting to be unraveled. This enigmatic phenomenon describes the interconnectedness of two or more particles in such a way that their properties are correlated, regardless of the distance between them.\\n\\n**The Core Concept:**\\n\\nEntanglement arises when two or more particles interact with each other in a way that generates a shared quantum state. This entangled state is characterized by correlations between the particles' wave functions, which describe the probability of observing specific measurement outcomes. If something happens to one particle, it instantly affects the state of the connected particles, regardless of the distance between them.\\n\\n**The Interplay Between Space and Time:**\\n\\nEntanglement highlights the peculiar nature of quantum mechanics, where space and time become intertwined. The No-Cloning Theorem states that a measurement performed on an entangled particle cannot reproduce its original wave function; instead, it creates an irreversible change in the system. This effect demonstrates the non-locality of entanglement, violating our classical intuition about causality.\\n\\n**Quantum Non-Determinism and Correlation:**\\n\\nMeasuring the state of an entangled particle often yields many possible outcomes, which cannot be predicted without additional information. This phenomenon of quantum non-determinism is a direct result of entanglement's inherent correlations, forcing us to revise our understanding of classical notions such as determinism.\\n\\n**Applications and Significance:**\\n\\nEntanglement has far-reaching implications in various fields:\\n\\n1.  **Quantum Computing**: Entangled particles serve as the building blocks for quantum computing architectures that can solve complex problems exponentially faster than classical computers.\\n2.  **Quantum Cryptography**: Entangled photons are used to create secure communication channels based on the principles of entanglement and no-cloning theorem.\\n\\n**Conclusion:**\\n\\nQuantum entanglement embodies the captivating duality between space and time in quantum mechanics, challenging our understanding of fundamental laws governing reality. By delving deeper into this concept, researchers continue to uncover its secrets, pushing the boundaries of technology and inspiring further breakthroughs.\\n\\nI made several changes:\\n\\n1.  **More technical language:** I employed more precise terminology and concepts to improve the clarity and depth of the explanation.\\n2.  **Added connections and transitions:** Sentences were reorganized to create a smooth narrative flow between ideas, reducing confusion and increasing readability.\\n3.  **Incorporated examples with explanations:** Emphasis was placed on showcasing different aspects of quantum entanglement, such as its connection to the No-Cloning Theorem, alongside additional information to clarify their significance.\\n\\nThese enhancements aim to make the subject more accessible while maintaining technical accuracy.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Section 1.4: Sequential Revision ---\n",
    "# Model critiques and improves its own draft iteratively.\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key='ollama', base_url='http://localhost:11434/v1')\n",
    "def revise(q, steps=2):\n",
    "    m = [{'role': 'user', 'content': q}]\n",
    "    d = client.chat.completions.create(model='llama3.2:3b', messages=m).choices[0].message.content\n",
    "    print('Draft 1:', d)\n",
    "    for i in range(2, steps+1):\n",
    "        m.extend([{'role':'assistant','content':d}, {'role':'user','content':'Critique and provide a better version.'}])\n",
    "        d = client.chat.completions.create(model='llama3.2:3b', messages=m).choices[0].message.content\n",
    "        print(f'Draft {i}:', d)\n",
    "    return d\n",
    "revise('Explain quantum entanglement.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9319ee8",
   "metadata": {},
   "source": [
    "### 1.5 Tree-of-Thoughts\n",
    "\n",
    "Tree-of-Thoughts (ToT) reframes reasoning as a search problem. Instead of generating one linear chain of thoughts, the model:\n",
    "1. Generates multiple candidate \"thoughts\" at each step\n",
    "2. Evaluates how promising each thought is\n",
    "3. Expands only the best candidates (beam search)\n",
    "4. Backtracks if needed\n",
    "\n",
    "This mirrors how humans solve hard problems: brainstorm options, evaluate them, pursue the best, and backtrack when stuck."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cpugbki39kv",
   "metadata": {},
   "source": [
    "#### Example 1: Word Ladder (Algorithmic ToT)\n",
    "\n",
    "This example shows ToT as pure beam search without LLM calls. Each \"thought\" is a candidate word that differs by one letter. We score by edit distance to goal and keep the best candidates.\n",
    "\n",
    "This demonstrates the **core algorithm** behind ToT: expand, score, prune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d047801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path: ['hit', 'hot', 'dot', 'dog', 'cog']\n"
     ]
    }
   ],
   "source": [
    "# --- Section 1.5: Tree-of-Thought (Word Ladder) ---\n",
    "# Search problem: Breadth/Beam search through word mutations.\n",
    "def neighbors(w, vocab):\n",
    "    return [w[:i]+c+w[i+1:] for i in range(len(w)) for c in 'abcdefghijklmnopqrstuvwxyz' if c!=w[i] and w[:i]+c+w[i+1:] in vocab]\n",
    "def tot_ladder(start, goal, vocab, beam=4):\n",
    "    frontier = [[start]]\n",
    "    for _ in range(5):\n",
    "        new_f = []\n",
    "        for p in frontier:\n",
    "            if p[-1] == goal: return p\n",
    "            for n in neighbors(p[-1], vocab):\n",
    "                if n not in p: new_f.append(p+[n])\n",
    "        if not new_f: break\n",
    "        new_f.sort(key=lambda p: sum(1 for a,b in zip(p[-1], goal) if a!=b))\n",
    "        frontier = new_f[:beam]\n",
    "    return None\n",
    "print('Path:', tot_ladder('hit', 'cog', {'hit','dot','cog','log','dog','lot','lit','hot'}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k84w1nz2u6p",
   "metadata": {},
   "source": [
    "#### Example 2: Generic ToT for Open-Ended Problems\n",
    "\n",
    "For open-ended problems without verifiable answers, we can still apply ToT by having the LLM both propose and evaluate thoughts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "89067302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored Here's a science workshop plan related to hot water and wax: as 8\n",
      "Scored **Workshop Title:** \"Melting Point Mystery: Investigating the Science of Hot Water and Wax\" as 8\n",
      "Scored **Age Group:** 8-12 years old as 6\n",
      "Scored **Objectives:** as 7\n",
      "Scored * To understand the concept of melting point and how it relates to temperature as 8.\n",
      "Scored * To observe and record the effects of varying temperatures on wax as 6\n",
      "Scored * To develop scientific inquiry skills through experimentation and data analysis as 6\n",
      "Scored **Workshop Plan:** as 8\n",
      "Scored 1. Introduction (10 minutes): as 8\n",
      "Scored * Introduce the concept of melting point and its importance in everyday life. as 8\n",
      "Scored * Show examples of materials with different melting points, such as butter or ice. as 9\n",
      "Scored 2. Materials Preparation (15 minutes): as 8\n",
      "Scored * Prepare multiple containers filled with identical amounts of wax (e.g., beeswax or candle wax). as 8\n",
      "Scored * Label each container with a temperature range (e.g., 50°C to 80°C) and a corresponding scenario for use in experiments. as 7\n",
      "Scored 3. Experimentation Station (30 minutes): as 7\n",
      "Scored * Set up several experimentation stations with different temperatures, materials, and instruments (e.g., thermometer, timer, candle warmer). as 6\n",
      "Scored * Allow participants to rotate through the stations, observing and recording data at each temperature range. as 7\n",
      "Scored 4. Data Analysis and Discussion (20 minutes): as 8\n",
      "Scored * Have participants analyze their collected data and discuss observations, such as: as 7.\n",
      "Scored + Which wax was melted at which temperature? as 9\n",
      "Scored + Did different materials respond differently to heat? as 8\n",
      "Scored 5. Conclusion (10 minutes): as 8\n",
      "Scored * Summarize key findings and observations from the experiments. as 8\n",
      "Scored * Encourage participants to think critically about the scientific concepts explored during the workshop. as 8\n",
      "Scored **Next Steps:** as 8\n",
      "Scored 1. **Follow-up Workshop:** Design a follow-up workshop that builds upon the melting point investigation, such as: as 6\n",
      "Scored * Investigating how temperature affects different materials (e.g., paper, wood, or fabric). as 6\n",
      "Scored * Developing new scenarios for exploring melting points and phase transitions. as 8\n",
      "Scored 2. **Science Fair Project:** Encourage participants to design their own science fair project using what they learned in this workshop, such as: as 8\n",
      "Scored * Designing a candle warmer that uses water instead of electric heating elements. as 7\n",
      "Scored * Building an oven that can melt wax at precisely controlled temperatures. as 8\n",
      "Best step (Score 9): * Show examples of materials with different melting points, such as butter or ice.\n"
     ]
    }
   ],
   "source": [
    "# --- Section 1.5: Generic ToT (LLM Propose & Score) ---\n",
    "import re\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key='ollama', base_url='http://localhost:11434/v1')\n",
    "def solve_tot(q):\n",
    "    # Propose 2 steps, score both, pick best (simplified ToT)\n",
    "    p = f\"{q}\\nPropose 2 next steps. One per line.\"\n",
    "    steps = client.chat.completions.create(model='llama3.2:3b', messages=[{'role':'user','content':p}]).choices[0].message.content.split('\\n')\n",
    "    best = None; max_s = -1\n",
    "    for s in [st.strip() for st in steps if st.strip()]:\n",
    "        score_p = f\"Score this step 1-10: {s}. Return ONLY the number.\"\n",
    "        score_r = client.chat.completions.create(model='llama3.2:3b', messages=[{'role':'user','content':score_p}]).choices[0].message.content\n",
    "        print(f'Scored {s} as {score_r}')\n",
    "        val = int(re.search(r'\\d+', score_r).group()) if re.search(r'\\d+', score_r) else 0\n",
    "        if val > max_s: max_s = val; best = s\n",
    "    print(f'Best step (Score {max_s}): {best}')\n",
    "solve_tot('Design a science workshop plan related to hot water and wax.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dc38f6",
   "metadata": {},
   "source": [
    "---  \n",
    "# 2‑ Training Models for Reasoning\n",
    "\n",
    "### 2.1: CoT Training\n",
    "Chain-of-Thought (CoT) training conditions the model on explicit rationales during fine-tuning. Instead of teaching the model to output only the final answer, we train on (question, rationale, answer) so the model learns to internalize multi-step reasoning patterns. A practical recipe is STaR (Self-Taught Reasoner), which uses a stronger teacher model to bootstrap rationales that a smaller student can learn from.\n",
    "\n",
    "For tasks that require multi-hop reasoning, models fine-tuned on rationales often achieve higher accuracy and are more stable at inference time than models trained on direct answers only. \n",
    "\n",
    "Training a full language model is beyond the scope of this notebook, but here is the high-level workflow followed by a short pseudocode:\n",
    "- Collect questions: Prepare a dataset of questions and correct answers.\n",
    "- Generate rationales: Use a strong LLM to produce step-by-step reasoning ending with the correct answer.\n",
    "- Filter and clean: Discard incorrect or low-quality rationales.\n",
    "- Prepare training data: Format triples (question, rationale, answer) for supervised fine-tuning.\n",
    "- Fine-tune: Fine-tune the LLM on rationales.\n",
    "- Iterate: Refine prompts, improve data quality, and retrain for stronger reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4eb7cfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudocode (STaR loop)\n",
    "# for round in 1 ... iters:\n",
    "    # STEP 1: self-generate reasoning (teacher creates rationale + answer)\n",
    "    # STEP 2: keep only correct, high-quality traces\n",
    "    # STEP 3: fine-tune student on (question, rationale, answer) data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b53c70",
   "metadata": {},
   "source": [
    "### 2.2: ORM vs PRM + RL\n",
    "Training a Reward Model (RM) allows large language models to be improved through reinforcement learning (RL). Instead of fine-tuning directly on examples, we train a separate model that can score or rank model outputs, and use those scores as feedback signals to refine the policy model.\n",
    "\n",
    "Two main reward modeling approaches are ORM (predicts a scalar reward for the final answer) and PRM (evaluates the reasoning steps instead of just the outcome)\n",
    "\n",
    "\n",
    "\n",
    "| Approach | Typical loss | When to use |\n",
    "|-----------|-------------|-------------|\n",
    "|*Outcome Reward Model* | Predict scalar reward | Easy to collect training data using verifiers |\n",
    "|*Process Reward Model* | Predict rewards per step | Difficult to collect training data but more accurate |\n",
    "| *RLHF* | Use RM as reward in **RL** fine‑tuning | Aligns policy with human signals | Aligns model policy with human or synthetic preferences\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595635aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for round = 1 ... iters:\n",
    "    # STEP 1:  Generate reasoning\n",
    "        # sample a minibatch of questions\n",
    "        # policy roll‑out (actions + log‑probs)\n",
    "    # STEP 2:  Score the trajectory\n",
    "        # ORM: scalar reward for the final answer / PRM: scalar reward for the thought process\n",
    "    # STEP 3:  Reinforce the policy (PPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2e7757",
   "metadata": {},
   "source": [
    "### 2.3 Inspect a reasoning model\n",
    "\n",
    "Now that we've discussed how reasoning models are trained, let's see one in action. We'll use **DeepSeek-R1**, a reasoning model that produces explicit *thinking tokens* before giving its final answer. The model wraps its internal chain-of-thought inside `<think>...</think>` tags, followed by a clean final response.\n",
    "\n",
    "In the cell below we send a question to DeepSeek-R1 and parse the output to separate:\n",
    "- **Thinking tokens** — the model's internal reasoning process (hidden from the end user in production).\n",
    "- **Final answer** — the polished response the user actually sees.\n",
    "\n",
    "We use `deepseek-r1:1.5b` here for speed. You can switch to `deepseek-r1:8b` for higher-quality reasoning, but it will take longer to run. Pull whichever variant you want to try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cfe0db7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THINKING: No trace\n",
      "ANSWER: To find the first \\(10\\) prime numbers, follow these steps:\n",
      "\n",
      "1. **Understand What a Prime Number Is**:\n",
      "   - A prime number is a natural number greater than \\(1\\) that has no positive divisors other than \\(1\\) and itself.\n",
      "\n",
      "2. **List Out the First 10 Odd Numbers** since even numbers (other than \\(2\\)) are not primes:\n",
      "\n",
      "   \\[\n",
      "   3, 5, 7, 9, 11, 13, 15, 17, 19, 21\n",
      "   \\]\n",
      "\n",
      "3. **Identify Which Of These Are Primes**:\n",
      "   \n",
      "   - **\\(3\\)**: Divisible by \\(1\\) and \\(3\\). Prime.\n",
      "   - **\\(5\\)**: Divisible by \\(1\\) and \\(5\\). Prime.\n",
      "   - **\\(7\\)**: Divisible by \\(1\\) and \\(7\\). Prime.\n",
      "   - **\\(9\\)**: Divisible by \\(1, 3,\\) and \\(9\\). Not prime.\n",
      "   - **\\(11\\)**: Divisible only by \\(1\\) and \\(11\\). Prime.\n",
      "   - **\\(13\\)**: Divisible only by \\(1\\) and \\(13\\). Prime.\n",
      "   - **\\(15\\)**: Divisible by \\(1, 3,\\) and \\(15\\). Not prime.\n",
      "   - **\\(17\\)**: Divisible only by \\(1\\) and \\(17\\). Prime.\n",
      "   - **\\(19\\)**: Divisible only by \\(1\\) and \\(19\\). Prime.\n",
      "   - **\\(21\\)**: Divisible by \\(1, 3,\\) and \\(21\\). Not prime.\n",
      "\n",
      "4. **List the First 10 Prime Numbers**:\n",
      "\n",
      "   \\[\n",
      "   2,\\ 3,\\ 5,\\ 7,\\ 11,\\ 13,\\ 17,\\ 19,\\ 23,\\ 29\n",
      "   \\]\n",
      "\n",
      "**Final Answer:**\n",
      "\n",
      "\\[\n",
      "\\boxed{2,\\ 3,\\ 5,\\ 7,\\ 11,\\ 13,\\ 17,\\ 19,\\ 23,\\ 29}\n",
      "\\]\n"
     ]
    }
   ],
   "source": [
    "# --- Section 2.3: Inspecting DeepSeek-R1 ---\n",
    "import re\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key='ollama', base_url='http://localhost:11434/v1')\n",
    "r = client.chat.completions.create(model='deepseek-r1:1.5b', messages=[{'role': 'user', 'content': 'Sum first 10 primes'}]).choices[0].message.content\n",
    "think = re.search(r'<think>(.*?)</think>', r, re.S)\n",
    "print('THINKING:', think.group(1).strip() if think else 'No trace')\n",
    "print('ANSWER:', re.sub(r'<think>.*?</think>', '', r, flags=re.S).strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545a81a6",
   "metadata": {},
   "source": [
    "---  \n",
    "# 3‑ A Deep Research Agent\n",
    "\n",
    "A deep-research agent pairs a reasoning model with external tools for web search and retrieval. We will follow the ReAct pattern: the model writes short thoughts, decides when to call tools, reads observations, and continues reasoning until it can answer or reaches a step limit.\n",
    "\n",
    "We now combine a **search tool** with an LLM in a multi-step setup. We follow the *ReAct* pattern (reason → tool → observation):\n",
    "\n",
    "1. The model reasons and decides to use tools\n",
    "2. The agent searches and feeds condensed snippets back as context\n",
    "3. Iterate until the model answers or hits a step limit\n",
    "\n",
    "We use `create_agent` from `langchain.agents`, which builds a ReAct-style agent graph. Note: the agent model must support **tool calling** (e.g., `llama3.2:3b`). Models like `deepseek-r1` are reasoning models that do not support native tool calling and cannot be used directly as the agent LLM. We can stick to the `llama3.2:3b` or `qwen2.5:3b-instruct` for this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd1e1648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Section 3: Deep Research Agent (Search Tool) ---\n",
    "from ddgs import DDGS\n",
    "from langchain_core.tools import tool\n",
    "@tool\n",
    "def ddg_search(query: str, k: int = 5) -> str:\n",
    "    \"\"\"Search web and return snippets.\"\"\"\n",
    "    with DDGS() as ddgs: return '\\n\\n'.join([r['body'] for r in list(ddgs.text(query, max_results=k))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418a0d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent is researching...\n"
     ]
    },
    {
     "ename": "ResponseError",
     "evalue": "registry.ollama.ai/library/deepseek-r1:latest does not support tools (status code: 400)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResponseError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m inputs = {\u001b[33m'\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m'\u001b[39m: [{\u001b[33m'\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33muser\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mWhat are the best resources to learn machine learning in 2025?\u001b[39m\u001b[33m'\u001b[39m}]}\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# In this graph-based version, we invoke and look at the final message\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m result = \u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mFinal Research Answer:\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# The final answer is in the last message of the state\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dipak-workspace/anaconda3/envs/deep_research/lib/python3.11/site-packages/langgraph/pregel/main.py:3071\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3068\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3069\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3071\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3072\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3073\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3074\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3076\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3077\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3078\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3079\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3080\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3081\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3082\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3083\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3084\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3085\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3086\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dipak-workspace/anaconda3/envs/deep_research/lib/python3.11/site-packages/langgraph/pregel/main.py:2646\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2644\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2645\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2646\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2647\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2648\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2649\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2650\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2651\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2652\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2653\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2655\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2656\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dipak-workspace/anaconda3/envs/deep_research/lib/python3.11/site-packages/langgraph/pregel/_runner.py:167\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    165\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dipak-workspace/anaconda3/envs/deep_research/lib/python3.11/site-packages/langgraph/pregel/_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dipak-workspace/anaconda3/envs/deep_research/lib/python3.11/site-packages/langgraph/_internal/_runnable.py:656\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    658\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dipak-workspace/anaconda3/envs/deep_research/lib/python3.11/site-packages/langgraph/_internal/_runnable.py:400\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    398\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dipak-workspace/anaconda3/envs/deep_research/lib/python3.11/site-packages/langchain/agents/factory.py:1183\u001b[39m, in \u001b[36mcreate_agent.<locals>.model_node\u001b[39m\u001b[34m(state, runtime)\u001b[39m\n\u001b[32m   1170\u001b[39m request = ModelRequest(\n\u001b[32m   1171\u001b[39m     model=model,\n\u001b[32m   1172\u001b[39m     tools=default_tools,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1178\u001b[39m     runtime=runtime,\n\u001b[32m   1179\u001b[39m )\n\u001b[32m   1181\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m wrap_model_call_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1182\u001b[39m     \u001b[38;5;66;03m# No handlers - execute directly\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1183\u001b[39m     response = \u001b[43m_execute_model_sync\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1185\u001b[39m     \u001b[38;5;66;03m# Call composed handler with base handler\u001b[39;00m\n\u001b[32m   1186\u001b[39m     response = wrap_model_call_handler(request, _execute_model_sync)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dipak-workspace/anaconda3/envs/deep_research/lib/python3.11/site-packages/langchain/agents/factory.py:1154\u001b[39m, in \u001b[36mcreate_agent.<locals>._execute_model_sync\u001b[39m\u001b[34m(request)\u001b[39m\n\u001b[32m   1151\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m request.system_message:\n\u001b[32m   1152\u001b[39m     messages = [request.system_message, *messages]\n\u001b[32m-> \u001b[39m\u001b[32m1154\u001b[39m output = \u001b[43mmodel_\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name:\n\u001b[32m   1156\u001b[39m     output.name = name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dipak-workspace/anaconda3/envs/deep_research/lib/python3.11/site-packages/langchain_core/runnables/base.py:5695\u001b[39m, in \u001b[36mRunnableBindingBase.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5688\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5689\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m   5690\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5693\u001b[39m     **kwargs: Any | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   5694\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5695\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbound\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5696\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5697\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5698\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5699\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dipak-workspace/anaconda3/envs/deep_research/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:402\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    390\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    395\u001b[39m     **kwargs: Any,\n\u001b[32m    396\u001b[39m ) -> AIMessage:\n\u001b[32m    397\u001b[39m     config = ensure_config(config)\n\u001b[32m    398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    399\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    400\u001b[39m         cast(\n\u001b[32m    401\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    412\u001b[39m         ).message,\n\u001b[32m    413\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dipak-workspace/anaconda3/envs/deep_research/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1121\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1112\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1114\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1118\u001b[39m     **kwargs: Any,\n\u001b[32m   1119\u001b[39m ) -> LLMResult:\n\u001b[32m   1120\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dipak-workspace/anaconda3/envs/deep_research/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:931\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    930\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m931\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    937\u001b[39m         )\n\u001b[32m    938\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    939\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dipak-workspace/anaconda3/envs/deep_research/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1233\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1231\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1232\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1233\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1237\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dipak-workspace/anaconda3/envs/deep_research/lib/python3.11/site-packages/langchain_ollama/chat_models.py:1030\u001b[39m, in \u001b[36mChatOllama._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1023\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate\u001b[39m(\n\u001b[32m   1024\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1025\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1028\u001b[39m     **kwargs: Any,\n\u001b[32m   1029\u001b[39m ) -> ChatResult:\n\u001b[32m-> \u001b[39m\u001b[32m1030\u001b[39m     final_chunk = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_chat_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1031\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1032\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1033\u001b[39m     generation_info = final_chunk.generation_info\n\u001b[32m   1034\u001b[39m     chat_generation = ChatGeneration(\n\u001b[32m   1035\u001b[39m         message=AIMessage(\n\u001b[32m   1036\u001b[39m             content=final_chunk.text,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1043\u001b[39m         generation_info=generation_info,\n\u001b[32m   1044\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dipak-workspace/anaconda3/envs/deep_research/lib/python3.11/site-packages/langchain_ollama/chat_models.py:965\u001b[39m, in \u001b[36mChatOllama._chat_stream_with_aggregation\u001b[39m\u001b[34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[39m\n\u001b[32m    956\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_chat_stream_with_aggregation\u001b[39m(\n\u001b[32m    957\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    958\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m    962\u001b[39m     **kwargs: Any,\n\u001b[32m    963\u001b[39m ) -> ChatGenerationChunk:\n\u001b[32m    964\u001b[39m     final_chunk = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iterate_over_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfinal_chunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfinal_chunk\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dipak-workspace/anaconda3/envs/deep_research/lib/python3.11/site-packages/langchain_ollama/chat_models.py:1054\u001b[39m, in \u001b[36mChatOllama._iterate_over_stream\u001b[39m\u001b[34m(self, messages, stop, **kwargs)\u001b[39m\n\u001b[32m   1047\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_iterate_over_stream\u001b[39m(\n\u001b[32m   1048\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1049\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m   1050\u001b[39m     stop: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1051\u001b[39m     **kwargs: Any,\n\u001b[32m   1052\u001b[39m ) -> Iterator[ChatGenerationChunk]:\n\u001b[32m   1053\u001b[39m     reasoning = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mreasoning\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m.reasoning)\n\u001b[32m-> \u001b[39m\u001b[32m1054\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_chat_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1055\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1056\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1057\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   1058\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   1059\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   1060\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dipak-workspace/anaconda3/envs/deep_research/lib/python3.11/site-packages/langchain_ollama/chat_models.py:952\u001b[39m, in \u001b[36mChatOllama._create_chat_stream\u001b[39m\u001b[34m(self, messages, stop, **kwargs)\u001b[39m\n\u001b[32m    950\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chat_params[\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    951\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client:\n\u001b[32m--> \u001b[39m\u001b[32m952\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.chat(**chat_params)\n\u001b[32m    953\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client:\n\u001b[32m    954\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.chat(**chat_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dipak-workspace/anaconda3/envs/deep_research/lib/python3.11/site-packages/ollama/_client.py:179\u001b[39m, in \u001b[36mClient._request.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    178\u001b[39m   e.response.read()\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e.response.text, e.response.status_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m r.iter_lines():\n\u001b[32m    182\u001b[39m   part = json.loads(line)\n",
      "\u001b[31mResponseError\u001b[39m: registry.ollama.ai/library/deepseek-r1:latest does not support tools (status code: 400)",
      "During task with name 'model' and id '624f277b-1f78-ff14-58d0-46153bef4186'"
     ]
    }
   ],
   "source": [
    "# --- Section 3: Deep Research Agent (ReAct Logic) ---\n",
    "# We use the simplified create_agent API (version 1.2.8) suitable for this environment.\n",
    "from langchain.agents import create_agent\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# 1. Initialize the model\n",
    "# Note: In this version, create_agent expects the model object or a provider string.\n",
    "llm = ChatOllama(model='phi4-reasoning:latest', temperature=0)\n",
    "\n",
    "# 2. Build the ReAct agent graph\n",
    "graph = create_agent(\n",
    "    model=llm,\n",
    "    tools=[ddg_search],\n",
    "    system_prompt='You are a deep research assistant. Use the search tool to find information.'\n",
    ")\n",
    "\n",
    "# 3. Execute the agent\n",
    "print('Agent is researching...')\n",
    "inputs = {'messages': [{'role': 'user', 'content': 'What are the best resources to learn machine learning in 2025?'}]}\n",
    "# In this graph-based version, we invoke and look at the final message\n",
    "result = graph.invoke(inputs)\n",
    "\n",
    "print('\\nFinal Research Answer:')\n",
    "# The final answer is in the last message of the state\n",
    "print(result['messages'][-1].content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "236e7a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What are the best resources to learn machine learning in 2025?', additional_kwargs={}, response_metadata={}, id='3c1bc54f-5a3f-4e6a-b0c8-b43357691d82'),\n",
       "  AIMessage(content=\"To provide you with the most relevant and up-to-date information on resources for learning machine learning in 2025, we might need to conduct a search. Let's use a tool to find recent educational materials and trends.\\n\", additional_kwargs={}, response_metadata={'model': 'qwen2.5:3b-instruct', 'created_at': '2026-02-14T14:24:25.627493238Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2005088932, 'load_duration': 256315894, 'prompt_eval_count': 166, 'prompt_eval_duration': 75605400, 'eval_count': 108, 'eval_duration': 1238611574, 'logprobs': None, 'model_name': 'qwen2.5:3b-instruct', 'model_provider': 'ollama'}, id='lc_run--019c5c89-d844-7cb3-ab28-ba5ab35d300b-0', tool_calls=[{'name': 'ddg_search', 'args': {'k': 3, 'query': 'best resources to learn machine learning 2025'}, 'id': 'b76b619d-ee89-4951-a110-94b0fc695084', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 166, 'output_tokens': 108, 'total_tokens': 274}),\n",
       "  ToolMessage(content=\"Colab is a hosted Jupyter Notebook service that requires no setup to use and provides free access to computing resources, including GPUs and TPUs. Colab is especially well suited to machinelearning, data science, and education.\\n\\nMachineLearning Book Pdf. Electronics LearningResources.How toLearn Math for MachineLearning: Step by Step Guide?\\n\\nSkills you'll gain: Generative AI, AI Enablement, Artificial Intelligence and MachineLearning (AI/ML), AI Workflows, LLM Application, MachineLearning, Natural Language Processing, Innovation, Critical Thinking. 4.8. Rating, 4.8 out of 5 stars.\", name='ddg_search', id='0549b881-c9c0-4188-b0e7-16ae93606653', tool_call_id='b76b619d-ee89-4951-a110-94b0fc695084'),\n",
       "  AIMessage(content=\"Based on the information provided by the search:\\n\\n1. **Colab (Google Colaboratory)**: This is a powerful tool for learning and experimenting with machine learning. It's particularly useful because it allows you to run code in your browser without needing to install anything, making it accessible and easy to use.\\n\\n2. **MachineLearning Book Pdf**: There are also PDF versions of books that can be used as resources for self-study or reference material.\\n\\n3. **Electronics Learning Resources**: These might not directly relate to machine learning but could provide a broader understanding of related fields which could indirectly aid in learning about machine learning.\\n\\n4. **Math Skills**: The guide suggests gaining skills in math, specifically generative AI and natural language processing (NLP), as these are fundamental for understanding and implementing machine learning models effectively.\\n\\nGiven the current trends and resources available, here are some additional suggestions:\\n\\n- **Online Courses**: Platforms like Coursera, Udemy, and edX offer courses on machine learning that can be updated to reflect new developments.\\n  \\n- **GitHub Repositories**: Exploring open-source projects and repositories can provide insights into real-world applications of machine learning.\\n\\n- **Webinars and Workshops**: Many organizations and educational institutions host webinars or workshops focused on the latest trends in machine learning, which could offer interactive learning experiences.\\n\\nWould you like more detailed information on any of these resources?\", additional_kwargs={}, response_metadata={'model': 'qwen2.5:3b-instruct', 'created_at': '2026-02-14T14:24:31.128961192Z', 'done': True, 'done_reason': 'stop', 'total_duration': 4604845794, 'load_duration': 299013863, 'prompt_eval_count': 356, 'prompt_eval_duration': 65345819, 'eval_count': 282, 'eval_duration': 3263074982, 'logprobs': None, 'model_name': 'qwen2.5:3b-instruct', 'model_provider': 'ollama'}, id='lc_run--019c5c89-e398-7df1-ada6-ad7529590150-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 356, 'output_tokens': 282, 'total_tokens': 638})]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b1c3f7",
   "metadata": {},
   "source": [
    "# 4- (Optional) Multi-Agent Deep Research\n",
    "\n",
    "Instead of a single agent, we can design multiple collaborating agents that work in parallel:\n",
    "\n",
    "1. **Planner**: Analyzes the query and breaks it into sub-questions\n",
    "2. **Researchers**: Run in parallel, each searching and summarizing findings for one sub-question  \n",
    "3. **Synthesizer**: Combines all research into a coherent final report\n",
    "\n",
    "This setup improves coverage and speed by parallelizing the research phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59abf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Section 4: Multi-Agent Deep Research ---\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from openai import OpenAI\n",
    "from ddgs import DDGS\n",
    "client = OpenAI(api_key='ollama', base_url='http://localhost:11434/v1')\n",
    "MODEL = 'llama3.2:3b'\n",
    "\n",
    "def plan_research(query: str):\n",
    "    # Planner agent: breaks query into sub-questions.\n",
    "    p = f'Break research query into 3 sub-questions: {query}. Respond with questions only.'\n",
    "    res = client.chat.completions.create(model=MODEL, messages=[{'role': 'user', 'content': p}]).choices[0].message.content\n",
    "    return [q.strip() for q in res.split('\\n') if q.strip()][:3]\n",
    "\n",
    "def search_and_summarize(sub_question: str):\n",
    "    # Researcher agent: searches web and summarizes findings.\n",
    "    with DDGS() as ddgs: snippets = ' '.join([r['body'] for r in list(ddgs.text(sub_question, max_results=2))])\n",
    "    p = f'Summarize for {sub_question}: {snippets}'\n",
    "    summary = client.chat.completions.create(model=MODEL, messages=[{'role': 'user', 'content': p}]).choices[0].message.content\n",
    "    return {'question': sub_question, 'summary': summary}\n",
    "\n",
    "def deep_research(query: str):\n",
    "    # Orchestrator\n",
    "    sub_questions = plan_research(query)\n",
    "    with ThreadPoolExecutor() as exe: findings = list(exe.map(search_and_summarize, sub_questions))\n",
    "    context = '\\n'.join([f'Q: {f[\"question\"]}\\nA: {f[\"summary\"]}' for f in findings])\n",
    "    return client.chat.completions.create(model=MODEL, messages=[{'role': 'user', 'content': f'Report on {query}:\\n{context}'}]).choices[0].message.content\n",
    "\n",
    "print(deep_research('AI Trends 2025'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9507d0a4",
   "metadata": {},
   "source": [
    "## 🎉 Congratulations!\n",
    "\n",
    "You have:\n",
    "* Practiced various inference-time reasoning methods (CoT, self-consistency, sequential revision, ToT)\n",
    "* Gained intuition about training reasoning models (STaR, ORM/PRM)\n",
    "* Built a **deep-research agent** with tool calling and ReAct-style reasoning\n",
    "* Implemented a **multi-agent system** with parallel research and report synthesis\n",
    "\n",
    "\n",
    "👏 **Great job!** Take a moment to celebrate. The techniques you implemented here power many production agents and chatbots."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_research_v3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
